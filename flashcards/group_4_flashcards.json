{
  "group": 4,
  "totalCards": 271,
  "sections": [
    {
      "section": "Detailed Study Guide",
      "subsections": [
        {
          "subsection": "General",
          "cardCount": 1,
          "cards": [
            {
              "id": "4-Detailed-Study-Guide--Detailed-Study-Guide",
              "title": "Detailed Study Guide",
              "front": "Detailed Study Guide",
              "back": "1. [Memory Characteristics and Classification](#memory-characteristics-and-classification)\n2. [The Memory Hierarchy Concept](#the-memory-hierarchy-concept)\n3. [Locality of Reference](#locality-of-reference)\n4. [Cache Memory Fundamentals](#cache-memory-fundamentals)\n5. [Cache Mapping Techniques](#cache-mapping-techniques)\n6. [Cache Replacement Policies](#cache-replacement-policies)\n7. [Write Policies](#write-policies)\n8. [Cache Performance Analysis](#cache-performance-analysis)\n9. [Multi-Level Caches](#multi-level-caches)\n10. [Internal Memory: DRAM and SRAM](#internal-memory-dram-and-sram)\n11. [Error Detection and Correction](#error-detection-and-correction)\n12. [Advanced DRAM Technologies](#advanced-dram-technologies)\n13. [Key Concepts Summary](#key-concepts-summary)\n14. [Practice Problems and Examples](#practice-problems-and-examples)",
              "type": "list",
              "section": "Detailed Study Guide",
              "subsection": ""
            }
          ]
        }
      ]
    },
    {
      "section": "Memory Characteristics and Classification",
      "subsections": [
        {
          "subsection": "Key Characteristics of Memory Systems",
          "cardCount": 33,
          "cards": [
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-1--Location",
              "title": "1. Location",
              "front": "1. Location",
              "back": "",
              "type": "concept",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-CPU",
              "title": "CPU",
              "front": "CPU",
              "back": "1. **Registers:** Fastest, smallest, most expensive\n2. Located directly in processor\n3. Used for temporary storage during execution",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Internal",
              "title": "Internal",
              "front": "Internal",
              "back": "1. **Main Memory (RAM):** Primary storage\n2. **Cache Memory:** Fast buffer between CPU and main memory\n3. Accessible directly by processor",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-External",
              "title": "External",
              "front": "External",
              "back": "1. **Secondary Storage:** Disk drives, SSDs, tape\n2. Accessible via I/O controllers\n3. Persistent, non-volatile",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-2--Capacity",
              "title": "2. Capacity",
              "front": "2. Capacity",
              "back": "",
              "type": "concept",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Word-Size",
              "title": "Word Size",
              "front": "Word Size",
              "back": "1. Natural unit of organization\n2. Common sizes: 8, 16, 32, 64 bits\n3. Determines how much data can be processed at once",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Number-of-Words-Bytes",
              "title": "Number of Words/Bytes",
              "front": "Number of Words/Bytes",
              "back": "1. Total storage capacity\n2. External memory typically expressed in bytes (KB, MB, GB, TB)\n3. Internal memory may be expressed in words or bytes",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Relationship",
              "title": "Relationship",
              "front": "Relationship",
              "back": "1. Address length (A bits) → 2^A addressable units\n2. Example: 20-bit address → 2^20 = 1,048,576 locations",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-3--Unit-of-Transfer",
              "title": "3. Unit of Transfer",
              "front": "3. Unit of Transfer",
              "back": "",
              "type": "concept",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Internal-Memory",
              "title": "Internal Memory",
              "front": "Internal Memory",
              "back": "1. Usually governed by bus data width\n2. May equal word length, but often larger\n3. Example: 32-bit processor with 64-bit data bus",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-External-Memory",
              "title": "External Memory",
              "front": "External Memory",
              "back": "1. Usually a **block** (much larger than a word)\n2. Example: Disk sectors (512 bytes, 4 KB, etc.)",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Addressable-Unit",
              "title": "Addressable Unit",
              "front": "Addressable Unit",
              "back": "1. Smallest location that can be uniquely addressed\n2. Typically: byte (8 bits) or word (16/32/64 bits)",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-4--Access-Methods",
              "title": "4. Access Methods",
              "front": "4. Access Methods",
              "back": "",
              "type": "concept",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Sequential-Access",
              "title": "Sequential Access",
              "front": "Sequential Access",
              "back": "1. Memory organized into records\n2. Must start at beginning and read through in order\n3. Access time variable, depends on location\n4. **Example:** Magnetic tape",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Direct-Access",
              "title": "Direct Access",
              "front": "Direct Access",
              "back": "1. Individual blocks have unique addresses\n2. Access by jumping to vicinity plus sequential search\n3. Access time depends on location and previous location\n4. **Example:** Magnetic disk",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Random-Access",
              "title": "Random Access",
              "front": "Random Access",
              "back": "1. Individual addresses identify locations exactly\n2. Access time independent of location or previous access\n3. **Example:** RAM, cache",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Associative-Access",
              "title": "Associative Access",
              "front": "Associative Access",
              "back": "1. Word retrieved based on portion of contents (not address)\n2. Access time independent of location or previous access\n3. **Example:** Cache (when searching by tag)",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-5--Performance",
              "title": "5. Performance",
              "front": "5. Performance",
              "back": "",
              "type": "concept",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Access-Time--Latency-",
              "title": "Access Time (Latency)",
              "front": "Access Time (Latency)",
              "back": "1. Time between presenting address and getting valid data\n2. Critical for performance\n3. Measured in nanoseconds (ns) or clock cycles",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Memory-Cycle-Time",
              "title": "Memory Cycle Time",
              "front": "Memory Cycle Time",
              "back": "1. Time required for memory to \"recover\" before next access\n2. Access time + recovery time\n3. Concerned with system bus, not processor\n4. May be longer than access time",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Transfer-Rate",
              "title": "Transfer Rate",
              "front": "Transfer Rate",
              "back": "1. Rate at which data can be transferred into/out of memory\n2. Measured in bits/second or bytes/second\n3. For random-access memory: 1/(cycle time)\n4. Also called **bandwidth**",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Example",
              "title": "Example",
              "front": "Example",
              "back": "Access time: 50 ns Cycle time: 100 ns Transfer rate: 1/100ns = 10 MB/s (for 1-byte transfers)",
              "type": "definition",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-6--Physical-Types",
              "title": "6. Physical Types",
              "front": "6. Physical Types",
              "back": "",
              "type": "concept",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Semiconductor-Memory",
              "title": "Semiconductor Memory",
              "front": "Semiconductor Memory",
              "back": "1. RAM (Random Access Memory)\n2. ROM (Read-Only Memory)\n3. Flash memory",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Magnetic-Surface-Memory",
              "title": "Magnetic Surface Memory",
              "front": "Magnetic Surface Memory",
              "back": "1. Hard disk drives\n2. Magnetic tape",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Optical",
              "title": "Optical",
              "front": "Optical",
              "back": "1. CD, DVD, Blu-ray",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-7--Physical-Characteristics",
              "title": "7. Physical Characteristics",
              "front": "7. Physical Characteristics",
              "back": "",
              "type": "concept",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Volatile-vs--Nonvolatile",
              "title": "Volatile vs. Nonvolatile",
              "front": "Volatile vs. Nonvolatile",
              "back": "",
              "type": "definition",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Volatile-Memory",
              "title": "Volatile Memory",
              "front": "Volatile Memory",
              "back": "1. Information lost when power is switched off\n2. Requires continuous power to retain data\n3. **Examples:** DRAM, SRAM",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Nonvolatile-Memory",
              "title": "Nonvolatile Memory",
              "front": "Nonvolatile Memory",
              "back": "1. Information remains without deterioration\n2. No electrical power needed to retain information\n3. **Examples:** ROM, Flash, Magnetic disk",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Erasable-vs--Nonerasable",
              "title": "Erasable vs. Nonerasable",
              "front": "Erasable vs. Nonerasable",
              "back": "",
              "type": "definition",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Nonerasable-Memory",
              "title": "Nonerasable Memory",
              "front": "Nonerasable Memory",
              "back": "1. Cannot be altered (except by destroying storage unit)\n2. **Example:** ROM (Read-Only Memory)",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            },
            {
              "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Erasable-Memory",
              "title": "Erasable Memory",
              "front": "Erasable Memory",
              "back": "1. Can be written and rewritten\n2. **Examples:** RAM, EEPROM, Flash",
              "type": "list",
              "section": "Memory Characteristics and Classification",
              "subsection": "Key Characteristics of Memory Systems"
            }
          ]
        }
      ]
    },
    {
      "section": "The Memory Hierarchy Concept",
      "subsections": [
        {
          "subsection": "The Memory Dilemma",
          "cardCount": 3,
          "cards": [
            {
              "id": "4-The-Memory-Hierarchy-Concept-The-Memory-Dilemma-Design-Constraints",
              "title": "Design Constraints",
              "front": "Design Constraints",
              "back": "1. **How much?** (Capacity)\n2. **How fast?** (Access time)\n3. **How expensive?** (Cost per bit)",
              "type": "list",
              "section": "The Memory Hierarchy Concept",
              "subsection": "The Memory Dilemma"
            },
            {
              "id": "4-The-Memory-Hierarchy-Concept-The-Memory-Dilemma-The-Trade-off",
              "title": "The Trade-off",
              "front": "The Trade-off",
              "back": "1. **Faster access time** → **Greater cost per bit**\n2. **Greater capacity** → **Smaller cost per bit**\n3. **Greater capacity** → **Slower access time**",
              "type": "list",
              "section": "The Memory Hierarchy Concept",
              "subsection": "The Memory Dilemma"
            },
            {
              "id": "4-The-Memory-Hierarchy-Concept-The-Memory-Dilemma-The-Problem",
              "title": "The Problem",
              "front": "The Problem",
              "back": "1. We want: Large capacity, fast access, low cost\n2. But: Can't have all three simultaneously!",
              "type": "list",
              "section": "The Memory Hierarchy Concept",
              "subsection": "The Memory Dilemma"
            }
          ]
        },
        {
          "subsection": "The Solution: Memory Hierarchy",
          "cardCount": 2,
          "cards": [
            {
              "id": "4-The-Memory-Hierarchy-Concept-The-Solution--Memory-Hierarchy-Concept",
              "title": "Concept",
              "front": "Concept",
              "back": "Use multiple levels of memory with different speeds and sizes.",
              "type": "definition",
              "section": "The Memory Hierarchy Concept",
              "subsection": "The Solution: Memory Hierarchy"
            },
            {
              "id": "4-The-Memory-Hierarchy-Concept-The-Solution--Memory-Hierarchy-Principle",
              "title": "Principle",
              "front": "Principle",
              "back": "Store frequently accessed data in fast, expensive memory; store bulk data in slow, cheap memory.",
              "type": "definition",
              "section": "The Memory Hierarchy Concept",
              "subsection": "The Solution: Memory Hierarchy"
            }
          ]
        },
        {
          "subsection": "Memory Hierarchy Levels",
          "cardCount": 1,
          "cards": [
            {
              "id": "4-The-Memory-Hierarchy-Concept-Memory-Hierarchy-Levels-From-Fastest-Smallest-Most-Expensive-to-Slowest-Largest-Cheapest",
              "title": "From Fastest/Smallest/Most Expensive to Slowest/Largest/Cheapest",
              "front": "From Fastest/Smallest/Most Expensive to Slowest/Largest/Cheapest",
              "back": "Level 1: CPU Registers Level 2: Cache Memory (L1, L2, L3) Level 3: Main Memory (DRAM) Level 4: Secondary Storage (Disk, SSD) Level 5: Tertiary Storage (Tape, Optical)",
              "type": "definition",
              "section": "The Memory Hierarchy Concept",
              "subsection": "Memory Hierarchy Levels"
            }
          ]
        },
        {
          "subsection": "Hierarchy Characteristics",
          "cardCount": 1,
          "cards": [
            {
              "id": "4-The-Memory-Hierarchy-Concept-Hierarchy-Characteristics-Going-Down-the-Hierarchy",
              "title": "Going Down the Hierarchy",
              "front": "Going Down the Hierarchy",
              "back": "1. **Decreasing cost per bit**\n2. Registers: Very expensive\n3. Cache: Expensive\n4. Main memory: Moderate cost\n5. Disk: Cheap\n6. **Increasing capacity**\n7. Registers: ~32-64 words\n8. Cache: KB to MB\n9. Main memory: GB\n10. Disk: TB\n11. **Increasing access time**\n12. Registers: 1 cycle (nanoseconds)\n13. Cache: 1-10 cycles (nanoseconds)\n14. Main memory: 50-100 cycles (nanoseconds)\n15. Disk: Millions of cycles (milliseconds)\n16. **Decreasing frequency of access**\n17. Most accesses: Registers\n18. Many accesses: Cache\n19. Some accesses: Main memory\n20. Few accesses: Disk",
              "type": "list",
              "section": "The Memory Hierarchy Concept",
              "subsection": "Hierarchy Characteristics"
            }
          ]
        },
        {
          "subsection": "Hierarchy Example",
          "cardCount": 4,
          "cards": [
            {
              "id": "4-The-Memory-Hierarchy-Concept-Hierarchy-Example-Typical-System",
              "title": "Typical System",
              "front": "Typical System",
              "back": "1. **L1 Cache:** 32 KB, 1 cycle access, $100/GB\n2. **L2 Cache:** 256 KB, 5 cycles access, $50/GB\n3. **L3 Cache:** 8 MB, 20 cycles access, $10/GB\n4. **Main Memory:** 16 GB, 100 cycles access, $1/GB\n5. **Disk:** 1 TB, 10,000,000 cycles access, $0.01/GB",
              "type": "list",
              "section": "The Memory Hierarchy Concept",
              "subsection": "Hierarchy Example"
            },
            {
              "id": "4-The-Memory-Hierarchy-Concept-Hierarchy-Example-Access-Distribution",
              "title": "Access Distribution",
              "front": "Access Distribution",
              "back": "1. 95% from L1 cache\n2. 4.999% from L2 cache\n3. 0.001% from L3 cache\n4. 0.000005% from main memory\n5. Even fewer from disk",
              "type": "list",
              "section": "The Memory Hierarchy Concept",
              "subsection": "Hierarchy Example"
            },
            {
              "id": "4-The-Memory-Hierarchy-Concept-Hierarchy-Example-Average-Access-Time-Calculation",
              "title": "Average Access Time Calculation",
              "front": "Average Access Time Calculation",
              "back": "AMAT = 0.95 × 1 + 0.04999 × 5 + 0.00001 × 20 + 0.00000005 × 100 = 0.95 + 0.25 + 0.0002 + 0.000005 ≈ 1.2 cycles",
              "type": "definition",
              "section": "The Memory Hierarchy Concept",
              "subsection": "Hierarchy Example"
            },
            {
              "id": "4-The-Memory-Hierarchy-Concept-Hierarchy-Example-Key-Insight",
              "title": "Key Insight",
              "front": "Key Insight",
              "back": "Average access time is much closer to L1 access time (1 cycle) than main memory (100 cycles)!",
              "type": "definition",
              "section": "The Memory Hierarchy Concept",
              "subsection": "Hierarchy Example"
            }
          ]
        }
      ]
    },
    {
      "section": "Locality of Reference",
      "subsections": [
        {
          "subsection": "What is Locality?",
          "cardCount": 2,
          "cards": [
            {
              "id": "4-Locality-of-Reference-What-is-Locality--Definition",
              "title": "Definition",
              "front": "Definition",
              "back": "During program execution, memory references tend to **cluster**.",
              "type": "definition",
              "section": "Locality of Reference",
              "subsection": "What is Locality?"
            },
            {
              "id": "4-Locality-of-Reference-What-is-Locality--Observation",
              "title": "Observation",
              "front": "Observation",
              "back": "Programs access a **small proportion** of their address space at any given time.",
              "type": "definition",
              "section": "Locality of Reference",
              "subsection": "What is Locality?"
            }
          ]
        },
        {
          "subsection": "Two Types of Locality",
          "cardCount": 10,
          "cards": [
            {
              "id": "4-Locality-of-Reference-Two-Types-of-Locality-1--Temporal-Locality",
              "title": "1. Temporal Locality",
              "front": "1. Temporal Locality",
              "back": "",
              "type": "concept",
              "section": "Locality of Reference",
              "subsection": "Two Types of Locality"
            },
            {
              "id": "4-Locality-of-Reference-Two-Types-of-Locality-Definition",
              "title": "Definition",
              "front": "Definition",
              "back": "Items accessed **recently** are likely to be accessed again **soon**.",
              "type": "definition",
              "section": "Locality of Reference",
              "subsection": "Two Types of Locality"
            },
            {
              "id": "4-Locality-of-Reference-Two-Types-of-Locality-Principle",
              "title": "Principle",
              "front": "Principle",
              "back": "If an item is referenced, it will tend to be referenced again soon.",
              "type": "definition",
              "section": "Locality of Reference",
              "subsection": "Two Types of Locality"
            },
            {
              "id": "4-Locality-of-Reference-Two-Types-of-Locality-Examples",
              "title": "Examples",
              "front": "Examples",
              "back": "1. **Loop instructions:** Same instructions executed repeatedly\n2. **Reused variables:** Variables accessed multiple times\n3. **Function calls:** Same functions called repeatedly",
              "type": "list",
              "section": "Locality of Reference",
              "subsection": "Two Types of Locality"
            },
            {
              "id": "4-Locality-of-Reference-Two-Types-of-Locality-Example",
              "title": "Example",
              "front": "Example",
              "back": "for (i = 0; i < 1000; i++) { sum = sum + array[i]; // 'sum' accessed every iteration",
              "type": "definition",
              "section": "Locality of Reference",
              "subsection": "Two Types of Locality"
            },
            {
              "id": "4-Locality-of-Reference-Two-Types-of-Locality-2--Spatial-Locality",
              "title": "2. Spatial Locality",
              "front": "2. Spatial Locality",
              "back": "",
              "type": "concept",
              "section": "Locality of Reference",
              "subsection": "Two Types of Locality"
            },
            {
              "id": "4-Locality-of-Reference-Two-Types-of-Locality-Definition",
              "title": "Definition",
              "front": "Definition",
              "back": "Items **near** those accessed recently are likely to be accessed soon.",
              "type": "definition",
              "section": "Locality of Reference",
              "subsection": "Two Types of Locality"
            },
            {
              "id": "4-Locality-of-Reference-Two-Types-of-Locality-Principle",
              "title": "Principle",
              "front": "Principle",
              "back": "If an item is referenced, items whose addresses are close by will tend to be referenced soon.",
              "type": "definition",
              "section": "Locality of Reference",
              "subsection": "Two Types of Locality"
            },
            {
              "id": "4-Locality-of-Reference-Two-Types-of-Locality-Examples",
              "title": "Examples",
              "front": "Examples",
              "back": "1. **Sequential instruction access:** Instructions stored sequentially\n2. **Array data:** Array elements stored contiguously\n3. **Stack operations:** Stack grows/shrinks sequentially",
              "type": "list",
              "section": "Locality of Reference",
              "subsection": "Two Types of Locality"
            },
            {
              "id": "4-Locality-of-Reference-Two-Types-of-Locality-Example",
              "title": "Example",
              "front": "Example",
              "back": "for (i = 0; i < 1000; i++) { sum = sum + array[i]; // array[i], array[i+1], array[i+2] accessed sequentially",
              "type": "definition",
              "section": "Locality of Reference",
              "subsection": "Two Types of Locality"
            }
          ]
        },
        {
          "subsection": "Exploiting Locality",
          "cardCount": 2,
          "cards": [
            {
              "id": "4-Locality-of-Reference-Exploiting-Locality-Memory-Hierarchy-Strategy",
              "title": "Memory Hierarchy Strategy",
              "front": "Memory Hierarchy Strategy",
              "back": "1. **Store everything on disk** (cheap, large, slow)\n2. **Copy recently accessed items to main memory:**\n3. Exploits temporal locality (recent items likely needed again)\n4. Main memory faster than disk\n5. **Copy more recently accessed items to cache:**\n6. Exploits temporal locality further\n7. Cache faster than main memory\n8. **Copy nearby items when accessing:**\n9. Exploits spatial locality\n10. When accessing one word, bring in entire block\n11. Adjacent words likely to be accessed soon",
              "type": "list",
              "section": "Locality of Reference",
              "subsection": "Exploiting Locality"
            },
            {
              "id": "4-Locality-of-Reference-Exploiting-Locality-Result",
              "title": "Result",
              "front": "Result",
              "back": "Most accesses satisfied by fast memory (cache), few require slow memory (disk).",
              "type": "definition",
              "section": "Locality of Reference",
              "subsection": "Exploiting Locality"
            }
          ]
        }
      ]
    },
    {
      "section": "Cache Memory Fundamentals",
      "subsections": [
        {
          "subsection": "What is Cache?",
          "cardCount": 3,
          "cards": [
            {
              "id": "4-Cache-Memory-Fundamentals-What-is-Cache--Definition",
              "title": "Definition",
              "front": "Definition",
              "back": "A small amount of fast memory located between the processor and main memory.",
              "type": "definition",
              "section": "Cache Memory Fundamentals",
              "subsection": "What is Cache?"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-What-is-Cache--Purpose",
              "title": "Purpose",
              "front": "Purpose",
              "back": "1. Store recently accessed data and instructions\n2. Reduce average memory access time\n3. Bridge the speed gap between CPU and main memory",
              "type": "list",
              "section": "Cache Memory Fundamentals",
              "subsection": "What is Cache?"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-What-is-Cache--Characteristics",
              "title": "Characteristics",
              "front": "Characteristics",
              "back": "1. **Small:** Typically KB to MB\n2. **Fast:** 1-10 cycles access time\n3. **Expensive:** High cost per bit\n4. **On-chip:** Often located on CPU chip",
              "type": "list",
              "section": "Cache Memory Fundamentals",
              "subsection": "What is Cache?"
            }
          ]
        },
        {
          "subsection": "Cache Operation Overview",
          "cardCount": 2,
          "cards": [
            {
              "id": "4-Cache-Memory-Fundamentals-Cache-Operation-Overview-Read-Operation-",
              "title": "Read Operation:",
              "front": "Read Operation:",
              "back": "1. **CPU requests** contents of memory location\n2. **Check cache** for data\n3. **If present (Hit):**\n4. Get data from cache (fast)\n5. Deliver to CPU\n6. **If not present (Miss):**\n7. Read required **block** from main memory\n8. Load block into cache\n9. Deliver requested word to CPU",
              "type": "list",
              "section": "Cache Memory Fundamentals",
              "subsection": "Cache Operation Overview"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Cache-Operation-Overview-Write-Operation-",
              "title": "Write Operation:",
              "front": "Write Operation:",
              "back": "1. **CPU writes** data to memory location\n2. **Check cache** for location\n3. **If present (Hit):**\n4. Update cache\n5. May update main memory (depending on write policy)\n6. **If not present (Miss):**\n7. Load block into cache\n8. Update cache\n9. May update main memory",
              "type": "list",
              "section": "Cache Memory Fundamentals",
              "subsection": "Cache Operation Overview"
            }
          ]
        },
        {
          "subsection": "Key Definitions",
          "cardCount": 15,
          "cards": [
            {
              "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Block--Line-",
              "title": "Block (Line)",
              "front": "Block (Line)",
              "back": "",
              "type": "concept",
              "section": "Cache Memory Fundamentals",
              "subsection": "Key Definitions"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Definition",
              "title": "Definition",
              "front": "Definition",
              "back": "Unit of data transfer between cache and main memory.",
              "type": "definition",
              "section": "Cache Memory Fundamentals",
              "subsection": "Key Definitions"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Characteristics",
              "title": "Characteristics",
              "front": "Characteristics",
              "back": "1. May be multiple words\n2. Typically 16-128 bytes\n3. When one word is accessed, entire block is brought into cache\n4. Exploits spatial locality\n5. Reduces number of memory accesses\n6. More efficient than word-by-word transfer",
              "type": "list",
              "section": "Cache Memory Fundamentals",
              "subsection": "Key Definitions"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Hit",
              "title": "Hit",
              "front": "Hit",
              "back": "",
              "type": "concept",
              "section": "Cache Memory Fundamentals",
              "subsection": "Key Definitions"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Definition",
              "title": "Definition",
              "front": "Definition",
              "back": "Access satisfied by upper level (cache).",
              "type": "definition",
              "section": "Cache Memory Fundamentals",
              "subsection": "Key Definitions"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Hit-Ratio",
              "title": "Hit Ratio",
              "front": "Hit Ratio",
              "back": "`hits / total_accesses`",
              "type": "definition",
              "section": "Cache Memory Fundamentals",
              "subsection": "Key Definitions"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Example",
              "title": "Example",
              "front": "Example",
              "back": "1. 1000 memory accesses\n2. 950 satisfied by cache\n3. Hit ratio = 950/1000 = 0.95 = 95%",
              "type": "list",
              "section": "Cache Memory Fundamentals",
              "subsection": "Key Definitions"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Miss",
              "title": "Miss",
              "front": "Miss",
              "back": "",
              "type": "concept",
              "section": "Cache Memory Fundamentals",
              "subsection": "Key Definitions"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Definition",
              "title": "Definition",
              "front": "Definition",
              "back": "Block not present in cache, must be copied from lower level (main memory).",
              "type": "definition",
              "section": "Cache Memory Fundamentals",
              "subsection": "Key Definitions"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Miss-Ratio",
              "title": "Miss Ratio",
              "front": "Miss Ratio",
              "back": "`misses / total_accesses = 1 - hit_ratio`",
              "type": "definition",
              "section": "Cache Memory Fundamentals",
              "subsection": "Key Definitions"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Example",
              "title": "Example",
              "front": "Example",
              "back": "1. Hit ratio = 95%\n2. Miss ratio = 5%",
              "type": "list",
              "section": "Cache Memory Fundamentals",
              "subsection": "Key Definitions"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Miss-Penalty",
              "title": "Miss Penalty",
              "front": "Miss Penalty",
              "back": "",
              "type": "concept",
              "section": "Cache Memory Fundamentals",
              "subsection": "Key Definitions"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Definition",
              "title": "Definition",
              "front": "Definition",
              "back": "Time taken to handle a miss.",
              "type": "definition",
              "section": "Cache Memory Fundamentals",
              "subsection": "Key Definitions"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Components",
              "title": "Components",
              "front": "Components",
              "back": "1. Time to access main memory\n2. Time to transfer block to cache\n3. Time to deliver data to CPU",
              "type": "list",
              "section": "Cache Memory Fundamentals",
              "subsection": "Key Definitions"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Typical",
              "title": "Typical",
              "front": "Typical",
              "back": "10-100+ cycles",
              "type": "definition",
              "section": "Cache Memory Fundamentals",
              "subsection": "Key Definitions"
            }
          ]
        },
        {
          "subsection": "Cache Organization",
          "cardCount": 3,
          "cards": [
            {
              "id": "4-Cache-Memory-Fundamentals-Cache-Organization-Structure",
              "title": "Structure",
              "front": "Structure",
              "back": "Cache Line: ┌──────┬──────┬──────────┐ │ Valid│ Tag │ Data │ │ Bit │ │ (Block) │ └──────┴──────┴──────────┘",
              "type": "definition",
              "section": "Cache Memory Fundamentals",
              "subsection": "Cache Organization"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Cache-Organization-Components",
              "title": "Components",
              "front": "Components",
              "back": "1. **Valid Bit:** Indicates if line contains valid data\n2. **Tag:** Identifies which memory block is stored\n3. **Data:** The actual data block",
              "type": "list",
              "section": "Cache Memory Fundamentals",
              "subsection": "Cache Organization"
            },
            {
              "id": "4-Cache-Memory-Fundamentals-Cache-Organization-Example",
              "title": "Example",
              "front": "Example",
              "back": "Line 0: [V=1] [Tag=0x1234] [Data: word0, word1, word2, word3] Line 1: [V=0] [Tag=----] [Data: ----] Line 2: [V=1] [Tag=0x5678] [Data: word0, word1, word2, word3]",
              "type": "definition",
              "section": "Cache Memory Fundamentals",
              "subsection": "Cache Organization"
            }
          ]
        }
      ]
    },
    {
      "section": "Cache Mapping Techniques",
      "subsections": [
        {
          "subsection": "The Mapping Problem",
          "cardCount": 3,
          "cards": [
            {
              "id": "4-Cache-Mapping-Techniques-The-Mapping-Problem-Problem",
              "title": "Problem",
              "front": "Problem",
              "back": "There are fewer cache lines than main memory blocks.",
              "type": "definition",
              "section": "Cache Mapping Techniques",
              "subsection": "The Mapping Problem"
            },
            {
              "id": "4-Cache-Mapping-Techniques-The-Mapping-Problem-Question",
              "title": "Question",
              "front": "Question",
              "back": "How do we map main memory blocks to cache lines?",
              "type": "definition",
              "section": "Cache Mapping Techniques",
              "subsection": "The Mapping Problem"
            },
            {
              "id": "4-Cache-Mapping-Techniques-The-Mapping-Problem-Example",
              "title": "Example",
              "front": "Example",
              "back": "1. Main memory: 1 million blocks\n2. Cache: 1,000 lines\n3. Each block must map to one or more possible cache lines",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "The Mapping Problem"
            }
          ]
        },
        {
          "subsection": "Three Mapping Techniques",
          "cardCount": 30,
          "cards": [
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-1--Direct-Mapping",
              "title": "1. Direct Mapping",
              "front": "1. Direct Mapping",
              "back": "",
              "type": "concept",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Concept",
              "title": "Concept",
              "front": "Concept",
              "back": "Each block of main memory maps to **exactly one** cache line.",
              "type": "definition",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Mapping-Formula",
              "title": "Mapping Formula",
              "front": "Mapping Formula",
              "back": "1. `i` = cache line number\n2. `j` = main memory block number\n3. `m` = number of lines in cache",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Address-Structure",
              "title": "Address Structure",
              "front": "Address Structure",
              "back": "┌──────────┬──────┬──────┐ │ Tag │ Line │ Word │ │ (s-r bits)│(r bits)│(w bits)│ └──────────┴──────┴──────┘",
              "type": "definition",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Fields",
              "title": "Fields",
              "front": "Fields",
              "back": "1. **Tag:** High-order bits identifying which block\n2. **Line:** Cache line number (low-order bits of block address)\n3. **Word:** Word offset within block",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Example",
              "title": "Example",
              "front": "Example",
              "back": "1. Cache: 8 lines (3 bits for line number)\n2. Block size: 1 word (0 bits for word offset)\n3. Address: 22 (binary: 10110)",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Breaking-down-address-22",
              "title": "Breaking down address 22",
              "front": "Breaking down address 22",
              "back": "Binary: 1 0 1 1 0 │ │ └─► Line = 110 (binary) = 6 └─┴──────► Tag = 10 (binary) = 2",
              "type": "definition",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Operation",
              "title": "Operation",
              "front": "Operation",
              "back": "1. Extract line number from address\n2. Check if valid bit is set\n3. Compare tag in cache with tag from address\n4. If match: **Hit** (data in cache)\n5. If no match: **Miss** (load block from memory)",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Advantages",
              "title": "Advantages",
              "front": "Advantages",
              "back": "1. **Simple:** Easy to implement\n2. **Fast:** Direct lookup (no search needed)\n3. **Inexpensive:** Minimal hardware",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Disadvantages",
              "title": "Disadvantages",
              "front": "Disadvantages",
              "back": "1. **Fixed location:** Any given block can only be in one specific line\n2. **Thrashing:** If program accesses blocks that map to same line repeatedly, constant misses\n3. **Low flexibility:** No choice in placement",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Thrashing-Example",
              "title": "Thrashing Example",
              "front": "Thrashing Example",
              "back": "Blocks 0, 8, 16, 24 all map to line 0 (0 mod 8 = 0, 8 mod 8 = 0, etc.) Accessing: 0, 8, 0, 8, 0, 8... Result: Constant misses (each access evicts previous block)",
              "type": "definition",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-2--Fully-Associative-Mapping",
              "title": "2. Fully Associative Mapping",
              "front": "2. Fully Associative Mapping",
              "back": "",
              "type": "concept",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Concept",
              "title": "Concept",
              "front": "Concept",
              "back": "A main memory block can load into **any line** of cache.",
              "type": "definition",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Address-Structure",
              "title": "Address Structure",
              "front": "Address Structure",
              "back": "┌──────────┬──────┐ │ Tag │ Word │ │ (s bits) │(w bits)│ └──────────┴──────┘",
              "type": "definition",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Fields",
              "title": "Fields",
              "front": "Fields",
              "back": "1. **Tag:** Full block address (no line field needed)\n2. **Word:** Word offset within block",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Operation",
              "title": "Operation",
              "front": "Operation",
              "back": "1. Extract tag from address\n2. **Search all cache lines** for matching tag\n3. If found: **Hit**\n4. If not found: **Miss** (load into any available line)",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Advantages",
              "title": "Advantages",
              "front": "Advantages",
              "back": "1. **Maximum flexibility:** Block can be placed anywhere\n2. **No thrashing:** No conflicts between blocks\n3. **Best hit ratio:** Optimal placement possible",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Disadvantages",
              "title": "Disadvantages",
              "front": "Disadvantages",
              "back": "1. **Expensive:** Requires comparators for all lines\n2. **Slow:** Must search all lines (parallel search needed for speed)\n3. **Complex:** More hardware complexity",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Hardware-Requirements",
              "title": "Hardware Requirements",
              "front": "Hardware Requirements",
              "back": "1. N comparators (one per cache line)\n2. Parallel tag comparison\n3. More expensive as cache size increases",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-3--Set-Associative-Mapping",
              "title": "3. Set-Associative Mapping",
              "front": "3. Set-Associative Mapping",
              "back": "",
              "type": "concept",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Concept",
              "title": "Concept",
              "front": "Concept",
              "back": "Compromise between direct and fully associative.",
              "type": "definition",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Organization",
              "title": "Organization",
              "front": "Organization",
              "back": "1. Cache divided into **sets**\n2. Each set contains **k lines** (k-way set associative)\n3. Block maps to **one specific set**, but can be placed in **any line within that set**",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Mapping",
              "title": "Mapping",
              "front": "Mapping",
              "back": "1. Set number: `(Block number) mod (Number of sets)`\n2. Within set: Any of k lines",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Address-Structure",
              "title": "Address Structure",
              "front": "Address Structure",
              "back": "┌──────────┬──────┬──────┐ │ Tag │ Set │ Word │ │(s-d bits)│(d bits)│(w bits)│ └──────────┴──────┴──────┘",
              "type": "definition",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Fields",
              "title": "Fields",
              "front": "Fields",
              "back": "1. **Tag:** Identifies block within set\n2. **Set:** Set number (determines which set)\n3. **Word:** Word offset within block",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Example---2-Way-Set-Associative",
              "title": "Example - 2-Way Set Associative",
              "front": "Example - 2-Way Set Associative",
              "back": "1. Cache: 8 lines total\n2. Sets: 4 sets (2 lines per set)\n3. Block 12: 12 mod 4 = 0 → Set 0, can be in either line of Set 0",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Operation",
              "title": "Operation",
              "front": "Operation",
              "back": "1. Extract set number from address\n2. Search **only lines in that set** for matching tag\n3. If found: **Hit**\n4. If not found: **Miss** (load into any available line in set)",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Advantages",
              "title": "Advantages",
              "front": "Advantages",
              "back": "1. **Good flexibility:** Multiple choices per block\n2. **Reasonable cost:** Only k comparators needed (not all lines)\n3. **Better than direct:** Reduces thrashing\n4. **Better than fully associative:** Lower cost, faster",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Disadvantages",
              "title": "Disadvantages",
              "front": "Disadvantages",
              "back": "1. **More complex than direct:** Requires set selection and search\n2. **More expensive than direct:** Needs k comparators\n3. **Less flexible than fully associative:** Limited to k choices",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Common-Configurations",
              "title": "Common Configurations",
              "front": "Common Configurations",
              "back": "1. **2-way:** 2 lines per set (common, good balance)\n2. **4-way:** 4 lines per set (very common)\n3. **8-way:** 8 lines per set (high-end processors)",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Three Mapping Techniques"
            }
          ]
        },
        {
          "subsection": "Comparison of Mapping Techniques",
          "cardCount": 1,
          "cards": [
            {
              "id": "4-Cache-Mapping-Techniques-Comparison-of-Mapping-Techniques-Modern-Practice",
              "title": "Modern Practice",
              "front": "Modern Practice",
              "back": "Most processors use **set-associative** (typically 2-8 way) for good balance.",
              "type": "definition",
              "section": "Cache Mapping Techniques",
              "subsection": "Comparison of Mapping Techniques"
            }
          ]
        },
        {
          "subsection": "Associativity Spectrum",
          "cardCount": 2,
          "cards": [
            {
              "id": "4-Cache-Mapping-Techniques-Associativity-Spectrum-For-a-cache-with-8-entries",
              "title": "For a cache with 8 entries",
              "front": "For a cache with 8 entries",
              "back": "1. **Direct (1-way):** 8 sets, 1 line per set\n2. **2-way:** 4 sets, 2 lines per set\n3. **4-way:** 2 sets, 4 lines per set\n4. **8-way (Fully Associative):** 1 set, 8 lines per set",
              "type": "list",
              "section": "Cache Mapping Techniques",
              "subsection": "Associativity Spectrum"
            },
            {
              "id": "4-Cache-Mapping-Techniques-Associativity-Spectrum-Key-Insight",
              "title": "Key Insight",
              "front": "Key Insight",
              "back": "Direct mapping and fully associative are special cases of set-associative!",
              "type": "definition",
              "section": "Cache Mapping Techniques",
              "subsection": "Associativity Spectrum"
            }
          ]
        }
      ]
    },
    {
      "section": "Cache Replacement Policies",
      "subsections": [
        {
          "subsection": "When Replacement is Needed",
          "cardCount": 3,
          "cards": [
            {
              "id": "4-Cache-Replacement-Policies-When-Replacement-is-Needed-Situation",
              "title": "Situation",
              "front": "Situation",
              "back": "Cache is full, new block must be loaded.",
              "type": "definition",
              "section": "Cache Replacement Policies",
              "subsection": "When Replacement is Needed"
            },
            {
              "id": "4-Cache-Replacement-Policies-When-Replacement-is-Needed-Direct-Mapping",
              "title": "Direct Mapping",
              "front": "Direct Mapping",
              "back": "1. **No choice:** Only one possible line\n2. Replacement is automatic",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "When Replacement is Needed"
            },
            {
              "id": "4-Cache-Replacement-Policies-When-Replacement-is-Needed-Associative-Set-Associative",
              "title": "Associative/Set-Associative",
              "front": "Associative/Set-Associative",
              "back": "1. **Choice available:** Which line to replace?\n2. Need **replacement algorithm**",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "When Replacement is Needed"
            }
          ]
        },
        {
          "subsection": "Replacement Algorithms",
          "cardCount": 24,
          "cards": [
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-1--Least-Recently-Used--LRU-",
              "title": "1. Least Recently Used (LRU)",
              "front": "1. Least Recently Used (LRU)",
              "back": "",
              "type": "concept",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Principle",
              "title": "Principle",
              "front": "Principle",
              "back": "Replace the block that has been in cache **longest without reference**.",
              "type": "definition",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Implementation",
              "title": "Implementation",
              "front": "Implementation",
              "back": "1. Track access order for each set\n2. Replace least recently accessed block",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Advantages",
              "title": "Advantages",
              "front": "Advantages",
              "back": "1. **Most effective:** Exploits temporal locality\n2. **Good hit ratio:** Keeps recently used blocks",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Disadvantages",
              "title": "Disadvantages",
              "front": "Disadvantages",
              "back": "1. **Complexity:** Requires tracking access history\n2. **Hardware cost:** Counters or state machines needed",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Example",
              "title": "Example",
              "front": "Example",
              "back": "Set with blocks: A, B, C Access order: A, B, A, C, B Next miss: Replace C (least recently used)",
              "type": "definition",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-2--First-In-First-Out--FIFO-",
              "title": "2. First-In-First-Out (FIFO)",
              "front": "2. First-In-First-Out (FIFO)",
              "back": "",
              "type": "concept",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Principle",
              "title": "Principle",
              "front": "Principle",
              "back": "Replace the block that has been in cache **longest** (regardless of recent use).",
              "type": "definition",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Implementation",
              "title": "Implementation",
              "front": "Implementation",
              "back": "1. Round-robin or circular buffer\n2. Replace oldest block",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Advantages",
              "title": "Advantages",
              "front": "Advantages",
              "back": "1. **Simple:** Easy to implement\n2. **Low cost:** Minimal hardware",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Disadvantages",
              "title": "Disadvantages",
              "front": "Disadvantages",
              "back": "1. **Less effective:** Doesn't consider recent usage\n2. **May evict frequently used blocks**",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Example",
              "title": "Example",
              "front": "Example",
              "back": "Blocks loaded: A, B, C (in that order) Next miss: Replace A (first in)",
              "type": "definition",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-3--Least-Frequently-Used--LFU-",
              "title": "3. Least Frequently Used (LFU)",
              "front": "3. Least Frequently Used (LFU)",
              "back": "",
              "type": "concept",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Principle",
              "title": "Principle",
              "front": "Principle",
              "back": "Replace the block with **fewest references**.",
              "type": "definition",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Implementation",
              "title": "Implementation",
              "front": "Implementation",
              "back": "1. Counter for each block\n2. Increment on access\n3. Replace block with lowest count",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Advantages",
              "title": "Advantages",
              "front": "Advantages",
              "back": "1. **Considers usage frequency:** Keeps frequently used blocks",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Disadvantages",
              "title": "Disadvantages",
              "front": "Disadvantages",
              "back": "1. **Complexity:** Counters needed\n2. **May keep old blocks:** Blocks accessed many times long ago",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Example",
              "title": "Example",
              "front": "Example",
              "back": "Block A: 10 accesses Block B: 5 accesses Block C: 2 accesses Next miss: Replace C (least frequently used)",
              "type": "definition",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-4--Random",
              "title": "4. Random",
              "front": "4. Random",
              "back": "",
              "type": "concept",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Principle",
              "title": "Principle",
              "front": "Principle",
              "back": "Replace a **randomly selected** block.",
              "type": "definition",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Implementation",
              "title": "Implementation",
              "front": "Implementation",
              "back": "1. Random number generator\n2. Select random line in set",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Advantages",
              "title": "Advantages",
              "front": "Advantages",
              "back": "1. **Very simple:** Minimal hardware\n2. **No tracking needed**",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Disadvantages",
              "title": "Disadvantages",
              "front": "Disadvantages",
              "back": "1. **Poor performance:** No locality consideration\n2. **Unpredictable:** May evict important blocks",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            },
            {
              "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Use",
              "title": "Use",
              "front": "Use",
              "back": "Rarely used, mainly for comparison",
              "type": "definition",
              "section": "Cache Replacement Policies",
              "subsection": "Replacement Algorithms"
            }
          ]
        },
        {
          "subsection": "Algorithm Comparison",
          "cardCount": 3,
          "cards": [
            {
              "id": "4-Cache-Replacement-Policies-Algorithm-Comparison-Effectiveness--Best-to-Worst-",
              "title": "Effectiveness (Best to Worst)",
              "front": "Effectiveness (Best to Worst)",
              "back": "1. LRU (most effective)\n2. LFU\n3. FIFO\n4. Random (least effective)",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "Algorithm Comparison"
            },
            {
              "id": "4-Cache-Replacement-Policies-Algorithm-Comparison-Complexity--Simplest-to-Most-Complex-",
              "title": "Complexity (Simplest to Most Complex)",
              "front": "Complexity (Simplest to Most Complex)",
              "back": "1. Random (simplest)\n2. FIFO\n3. LRU\n4. LFU (most complex)",
              "type": "list",
              "section": "Cache Replacement Policies",
              "subsection": "Algorithm Comparison"
            },
            {
              "id": "4-Cache-Replacement-Policies-Algorithm-Comparison-Modern-Practice",
              "title": "Modern Practice",
              "front": "Modern Practice",
              "back": "**LRU** is most popular due to good effectiveness and reasonable implementation cost.",
              "type": "definition",
              "section": "Cache Replacement Policies",
              "subsection": "Algorithm Comparison"
            }
          ]
        }
      ]
    },
    {
      "section": "Write Policies",
      "subsections": [
        {
          "subsection": "The Write Problem",
          "cardCount": 2,
          "cards": [
            {
              "id": "4-Write-Policies-The-Write-Problem-Issue",
              "title": "Issue",
              "front": "Issue",
              "back": "When CPU writes to cache, main memory must eventually be updated.",
              "type": "definition",
              "section": "Write Policies",
              "subsection": "The Write Problem"
            },
            {
              "id": "4-Write-Policies-The-Write-Problem-Questions",
              "title": "Questions",
              "front": "Questions",
              "back": "1. When should main memory be updated?\n2. What if cache block is replaced before being written to memory?\n3. What if multiple devices access main memory?",
              "type": "list",
              "section": "Write Policies",
              "subsection": "The Write Problem"
            }
          ]
        },
        {
          "subsection": "Two Write Policies",
          "cardCount": 13,
          "cards": [
            {
              "id": "4-Write-Policies-Two-Write-Policies-1--Write-Through",
              "title": "1. Write Through",
              "front": "1. Write Through",
              "back": "",
              "type": "concept",
              "section": "Write Policies",
              "subsection": "Two Write Policies"
            },
            {
              "id": "4-Write-Policies-Two-Write-Policies-Principle",
              "title": "Principle",
              "front": "Principle",
              "back": "Every write to cache **also writes to main memory** immediately.",
              "type": "definition",
              "section": "Write Policies",
              "subsection": "Two Write Policies"
            },
            {
              "id": "4-Write-Policies-Two-Write-Policies-Operation",
              "title": "Operation",
              "front": "Operation",
              "back": "CPU Write → Update Cache → Update Main Memory (simultaneously)",
              "type": "definition",
              "section": "Write Policies",
              "subsection": "Two Write Policies"
            },
            {
              "id": "4-Write-Policies-Two-Write-Policies-Advantages",
              "title": "Advantages",
              "front": "Advantages",
              "back": "1. **Simple:** Straightforward implementation\n2. **Consistency:** Cache and memory always consistent\n3. **I/O compatibility:** I/O devices can read directly from memory",
              "type": "list",
              "section": "Write Policies",
              "subsection": "Two Write Policies"
            },
            {
              "id": "4-Write-Policies-Two-Write-Policies-Disadvantages",
              "title": "Disadvantages",
              "front": "Disadvantages",
              "back": "1. **High memory traffic:** Every write goes to memory\n2. **Slow writes:** Memory access is slow\n3. **Bottleneck:** Memory bus becomes bottleneck",
              "type": "list",
              "section": "Write Policies",
              "subsection": "Two Write Policies"
            },
            {
              "id": "4-Write-Policies-Two-Write-Policies-Performance-Impact",
              "title": "Performance Impact",
              "front": "Performance Impact",
              "back": "1. Hold data waiting to be written\n2. CPU continues immediately\n3. Only stalls if buffer is full\n4. Reduces performance penalty",
              "type": "list",
              "section": "Write Policies",
              "subsection": "Two Write Policies"
            },
            {
              "id": "4-Write-Policies-Two-Write-Policies-2--Write-Back",
              "title": "2. Write Back",
              "front": "2. Write Back",
              "back": "",
              "type": "concept",
              "section": "Write Policies",
              "subsection": "Two Write Policies"
            },
            {
              "id": "4-Write-Policies-Two-Write-Policies-Principle",
              "title": "Principle",
              "front": "Principle",
              "back": "Write only to cache initially. Write to memory only when block is replaced.",
              "type": "definition",
              "section": "Write Policies",
              "subsection": "Two Write Policies"
            },
            {
              "id": "4-Write-Policies-Two-Write-Policies-Operation",
              "title": "Operation",
              "front": "Operation",
              "back": "CPU Write → Update Cache (only) When block replaced → Write to memory (if dirty)",
              "type": "definition",
              "section": "Write Policies",
              "subsection": "Two Write Policies"
            },
            {
              "id": "4-Write-Policies-Two-Write-Policies-Dirty-Bit",
              "title": "Dirty Bit",
              "front": "Dirty Bit",
              "back": "1. Indicates if block has been modified\n2. Set when block is written\n3. Checked when block is replaced\n4. If dirty: Write to memory before replacement",
              "type": "list",
              "section": "Write Policies",
              "subsection": "Two Write Policies"
            },
            {
              "id": "4-Write-Policies-Two-Write-Policies-Advantages",
              "title": "Advantages",
              "front": "Advantages",
              "back": "1. **Minimizes memory writes:** Only dirty blocks written\n2. **Faster writes:** No memory access during write\n3. **Better performance:** Lower memory traffic",
              "type": "list",
              "section": "Write Policies",
              "subsection": "Two Write Policies"
            },
            {
              "id": "4-Write-Policies-Two-Write-Policies-Disadvantages",
              "title": "Disadvantages",
              "front": "Disadvantages",
              "back": "1. **Complexity:** Need dirty bit tracking\n2. **Inconsistency:** Cache and memory may differ\n3. **I/O issues:** I/O must go through cache or use cache coherency",
              "type": "list",
              "section": "Write Policies",
              "subsection": "Two Write Policies"
            },
            {
              "id": "4-Write-Policies-Two-Write-Policies-Performance-Impact",
              "title": "Performance Impact",
              "front": "Performance Impact",
              "back": "Base CPI = 1 10% of instructions are stores Only 20% of replaced blocks are dirty Memory write takes 100 cycles Miss rate = 2% Effective CPI = 1 + 0.02 × 0.2 × 100 = 1.4 (Much better than write through!)",
              "type": "definition",
              "section": "Write Policies",
              "subsection": "Two Write Policies"
            }
          ]
        },
        {
          "subsection": "Write Allocation",
          "cardCount": 4,
          "cards": [
            {
              "id": "4-Write-Policies-Write-Allocation-Question",
              "title": "Question",
              "front": "Question",
              "back": "On write miss, should we load block into cache?",
              "type": "definition",
              "section": "Write Policies",
              "subsection": "Write Allocation"
            },
            {
              "id": "4-Write-Policies-Write-Allocation-Write-Allocate--Fetch-on-Write-Miss-",
              "title": "Write Allocate (Fetch on Write Miss)",
              "front": "Write Allocate (Fetch on Write Miss)",
              "back": "1. Load block into cache\n2. Update cache\n3. Use with write back",
              "type": "list",
              "section": "Write Policies",
              "subsection": "Write Allocation"
            },
            {
              "id": "4-Write-Policies-Write-Allocation-No-Write-Allocate--Write-Around-",
              "title": "No Write Allocate (Write Around)",
              "front": "No Write Allocate (Write Around)",
              "back": "1. Write directly to memory\n2. Don't load into cache\n3. Use with write through",
              "type": "list",
              "section": "Write Policies",
              "subsection": "Write Allocation"
            },
            {
              "id": "4-Write-Policies-Write-Allocation-Modern-Practice",
              "title": "Modern Practice",
              "front": "Modern Practice",
              "back": "1. **Write back + Write allocate:** Most common\n2. **Write through + No write allocate:** Less common",
              "type": "list",
              "section": "Write Policies",
              "subsection": "Write Allocation"
            }
          ]
        },
        {
          "subsection": "Cache Coherency",
          "cardCount": 3,
          "cards": [
            {
              "id": "4-Write-Policies-Cache-Coherency-Problem",
              "title": "Problem",
              "front": "Problem",
              "back": "Multiple devices may access same memory.",
              "type": "definition",
              "section": "Write Policies",
              "subsection": "Cache Coherency"
            },
            {
              "id": "4-Write-Policies-Cache-Coherency-Scenarios",
              "title": "Scenarios",
              "front": "Scenarios",
              "back": "1. **I/O and CPU:** I/O writes to memory, cache has stale data\n2. **Multiple CPUs:** Each has own cache, one CPU writes, others have stale data",
              "type": "list",
              "section": "Write Policies",
              "subsection": "Cache Coherency"
            },
            {
              "id": "4-Write-Policies-Cache-Coherency-Solutions",
              "title": "Solutions",
              "front": "Solutions",
              "back": "1. **Snooping:** Caches monitor bus for writes\n2. **Invalidation:** Mark cache lines as invalid when written by others\n3. **Update:** Update cache lines when written by others\n4. **Cache coherency protocols:** MESI (Modified, Exclusive, Shared, Invalid)",
              "type": "list",
              "section": "Write Policies",
              "subsection": "Cache Coherency"
            }
          ]
        }
      ]
    },
    {
      "section": "Cache Performance Analysis",
      "subsections": [
        {
          "subsection": "Performance Metrics",
          "cardCount": 15,
          "cards": [
            {
              "id": "4-Cache-Performance-Analysis-Performance-Metrics-Average-Memory-Access-Time--AMAT-",
              "title": "Average Memory Access Time (AMAT)",
              "front": "Average Memory Access Time (AMAT)",
              "back": "",
              "type": "concept",
              "section": "Cache Performance Analysis",
              "subsection": "Performance Metrics"
            },
            {
              "id": "4-Cache-Performance-Analysis-Performance-Metrics-Formula",
              "title": "Formula",
              "front": "Formula",
              "back": "AMAT = Hit Time + (Miss Rate × Miss Penalty)",
              "type": "definition",
              "section": "Cache Performance Analysis",
              "subsection": "Performance Metrics"
            },
            {
              "id": "4-Cache-Performance-Analysis-Performance-Metrics-Components",
              "title": "Components",
              "front": "Components",
              "back": "1. **Hit Time:** Time to access cache (typically 1-10 cycles)\n2. **Miss Rate:** Fraction of accesses that miss (0.0 to 1.0)\n3. **Miss Penalty:** Time to handle miss (typically 10-100+ cycles)",
              "type": "list",
              "section": "Cache Performance Analysis",
              "subsection": "Performance Metrics"
            },
            {
              "id": "4-Cache-Performance-Analysis-Performance-Metrics-Example",
              "title": "Example",
              "front": "Example",
              "back": "Hit time = 1 cycle Miss rate = 5% = 0.05 Miss penalty = 100 cycles AMAT = 1 + 0.05 × 100 = 1 + 5 = 6 cycles",
              "type": "definition",
              "section": "Cache Performance Analysis",
              "subsection": "Performance Metrics"
            },
            {
              "id": "4-Cache-Performance-Analysis-Performance-Metrics-Key-Insight",
              "title": "Key Insight",
              "front": "Key Insight",
              "back": "Even with 5% miss rate, average access time is 6x hit time!",
              "type": "definition",
              "section": "Cache Performance Analysis",
              "subsection": "Performance Metrics"
            },
            {
              "id": "4-Cache-Performance-Analysis-Performance-Metrics-CPU-Time",
              "title": "CPU Time",
              "front": "CPU Time",
              "back": "",
              "type": "concept",
              "section": "Cache Performance Analysis",
              "subsection": "Performance Metrics"
            },
            {
              "id": "4-Cache-Performance-Analysis-Performance-Metrics-Formula",
              "title": "Formula",
              "front": "Formula",
              "back": "CPU Time = (CPU execution cycles + Memory stall cycles) × Clock cycle time",
              "type": "definition",
              "section": "Cache Performance Analysis",
              "subsection": "Performance Metrics"
            },
            {
              "id": "4-Cache-Performance-Analysis-Performance-Metrics-Memory-Stall-Cycles",
              "title": "Memory Stall Cycles",
              "front": "Memory Stall Cycles",
              "back": "Memory stall cycles = (Instruction miss cycles) + (Data miss cycles)",
              "type": "definition",
              "section": "Cache Performance Analysis",
              "subsection": "Performance Metrics"
            },
            {
              "id": "4-Cache-Performance-Analysis-Performance-Metrics-Instruction-Miss-Cycles",
              "title": "Instruction Miss Cycles",
              "front": "Instruction Miss Cycles",
              "back": "Instruction miss cycles = (Instructions) × (I-cache miss rate) × (Miss penalty)",
              "type": "definition",
              "section": "Cache Performance Analysis",
              "subsection": "Performance Metrics"
            },
            {
              "id": "4-Cache-Performance-Analysis-Performance-Metrics-Data-Miss-Cycles",
              "title": "Data Miss Cycles",
              "front": "Data Miss Cycles",
              "back": "Data miss cycles = (Instructions) × (Load/store fraction) × (D-cache miss rate) × (Miss penalty)",
              "type": "definition",
              "section": "Cache Performance Analysis",
              "subsection": "Performance Metrics"
            },
            {
              "id": "4-Cache-Performance-Analysis-Performance-Metrics-Effective-CPI",
              "title": "Effective CPI",
              "front": "Effective CPI",
              "back": "",
              "type": "concept",
              "section": "Cache Performance Analysis",
              "subsection": "Performance Metrics"
            },
            {
              "id": "4-Cache-Performance-Analysis-Performance-Metrics-Formula",
              "title": "Formula",
              "front": "Formula",
              "back": "CPI_actual = CPI_base + (Memory stall cycles per instruction)",
              "type": "definition",
              "section": "Cache Performance Analysis",
              "subsection": "Performance Metrics"
            },
            {
              "id": "4-Cache-Performance-Analysis-Performance-Metrics-Memory-Stall-Cycles-per-Instruction",
              "title": "Memory Stall Cycles per Instruction",
              "front": "Memory Stall Cycles per Instruction",
              "back": "Stall cycles = (I-cache miss rate × Miss penalty) + (Load/store fraction × D-cache miss rate × Miss penalty)",
              "type": "definition",
              "section": "Cache Performance Analysis",
              "subsection": "Performance Metrics"
            },
            {
              "id": "4-Cache-Performance-Analysis-Performance-Metrics-Example",
              "title": "Example",
              "front": "Example",
              "back": "Base CPI = 3 I-cache miss rate = 4% = 0.04 D-cache miss rate = 8% = 0.08 Load/store fraction = 60% = 0.6 Miss penalty = 125 cycles CPI_actual = 3 + (0.04 × 125) + (0.6 × 0.08 × 125) = 3 + 5 + 6 = 14 cycles",
              "type": "definition",
              "section": "Cache Performance Analysis",
              "subsection": "Performance Metrics"
            },
            {
              "id": "4-Cache-Performance-Analysis-Performance-Metrics-Performance-Impact",
              "title": "Performance Impact",
              "front": "Performance Impact",
              "back": "Cache misses increase CPI from 3 to 14 (4.7x slower)!",
              "type": "definition",
              "section": "Cache Performance Analysis",
              "subsection": "Performance Metrics"
            }
          ]
        },
        {
          "subsection": "Improving Cache Performance",
          "cardCount": 2,
          "cards": [
            {
              "id": "4-Cache-Performance-Analysis-Improving-Cache-Performance-Strategies",
              "title": "Strategies",
              "front": "Strategies",
              "back": "1. **Reduce Miss Rate:**\n2. Larger cache\n3. Higher associativity\n4. Better replacement algorithm\n5. Larger block size (up to a point)\n6. **Reduce Miss Penalty:**\n7. Faster main memory\n8. Multi-level caches\n9. Write buffers\n10. **Reduce Hit Time:**\n11. Smaller cache\n12. Lower associativity\n13. On-chip cache",
              "type": "list",
              "section": "Cache Performance Analysis",
              "subsection": "Improving Cache Performance"
            },
            {
              "id": "4-Cache-Performance-Analysis-Improving-Cache-Performance-Trade-offs",
              "title": "Trade-offs",
              "front": "Trade-offs",
              "back": "1. Larger cache → Lower miss rate, but higher hit time\n2. Higher associativity → Lower miss rate, but higher hit time\n3. Larger blocks → Better spatial locality, but fewer blocks fit",
              "type": "list",
              "section": "Cache Performance Analysis",
              "subsection": "Improving Cache Performance"
            }
          ]
        }
      ]
    },
    {
      "section": "Multi-Level Caches",
      "subsections": [
        {
          "subsection": "Why Multiple Levels?",
          "cardCount": 2,
          "cards": [
            {
              "id": "4-Multi-Level-Caches-Why-Multiple-Levels--Problem",
              "title": "Problem",
              "front": "Problem",
              "back": "1. **Size:** Large enough for good hit rate\n2. **Speed:** Small enough for fast access",
              "type": "list",
              "section": "Multi-Level Caches",
              "subsection": "Why Multiple Levels?"
            },
            {
              "id": "4-Multi-Level-Caches-Why-Multiple-Levels--Solution",
              "title": "Solution",
              "front": "Solution",
              "back": "Use **multiple cache levels** with different characteristics.",
              "type": "definition",
              "section": "Multi-Level Caches",
              "subsection": "Why Multiple Levels?"
            }
          ]
        },
        {
          "subsection": "Two-Level Cache Organization",
          "cardCount": 4,
          "cards": [
            {
              "id": "4-Multi-Level-Caches-Two-Level-Cache-Organization-L1-Cache--Level-1-",
              "title": "L1 Cache (Level 1)",
              "front": "L1 Cache (Level 1)",
              "back": "1. **Location:** On CPU chip\n2. **Size:** Small (8-64 KB)\n3. **Speed:** Very fast (1-2 cycles)\n4. **Characteristics:**\n5. Split instruction and data caches\n6. Very close to processor\n7. Fastest access",
              "type": "list",
              "section": "Multi-Level Caches",
              "subsection": "Two-Level Cache Organization"
            },
            {
              "id": "4-Multi-Level-Caches-Two-Level-Cache-Organization-L2-Cache--Level-2-",
              "title": "L2 Cache (Level 2)",
              "front": "L2 Cache (Level 2)",
              "back": "1. **Location:** On CPU chip or off-chip\n2. **Size:** Medium (256 KB - 8 MB)\n3. **Speed:** Fast (5-20 cycles)\n4. **Characteristics:**\n5. Services misses from L1\n6. Larger than L1\n7. Slower than L1, but faster than main memory",
              "type": "list",
              "section": "Multi-Level Caches",
              "subsection": "Two-Level Cache Organization"
            },
            {
              "id": "4-Multi-Level-Caches-Two-Level-Cache-Organization-L3-Cache--Level-3-",
              "title": "L3 Cache (Level 3)",
              "front": "L3 Cache (Level 3)",
              "back": "1. **Location:** On CPU chip or off-chip\n2. **Size:** Large (8-64 MB)\n3. **Speed:** Moderate (20-50 cycles)\n4. **Characteristics:**\n5. Services misses from L2\n6. Shared among multiple cores\n7. Larger than L2",
              "type": "list",
              "section": "Multi-Level Caches",
              "subsection": "Two-Level Cache Organization"
            },
            {
              "id": "4-Multi-Level-Caches-Two-Level-Cache-Organization-Main-Memory",
              "title": "Main Memory",
              "front": "Main Memory",
              "back": "1. **Location:** Off-chip\n2. **Size:** Very large (GB)\n3. **Speed:** Slow (50-100+ cycles)\n4. **Characteristics:**\n5. Services misses from L3\n6. Largest capacity\n7. Slowest access",
              "type": "list",
              "section": "Multi-Level Caches",
              "subsection": "Two-Level Cache Organization"
            }
          ]
        },
        {
          "subsection": "Multi-Level Cache Operation",
          "cardCount": 5,
          "cards": [
            {
              "id": "4-Multi-Level-Caches-Multi-Level-Cache-Operation-Access-Flow",
              "title": "Access Flow",
              "front": "Access Flow",
              "back": "CPU Request Check L1 Cache Check L2 Cache Check L3 Cache Access Main Memory",
              "type": "definition",
              "section": "Multi-Level Caches",
              "subsection": "Multi-Level Cache Operation"
            },
            {
              "id": "4-Multi-Level-Caches-Multi-Level-Cache-Operation-Hit-at-L1",
              "title": "Hit at L1",
              "front": "Hit at L1",
              "back": "Fastest (1-2 cycles)",
              "type": "definition",
              "section": "Multi-Level Caches",
              "subsection": "Multi-Level Cache Operation"
            },
            {
              "id": "4-Multi-Level-Caches-Multi-Level-Cache-Operation-Hit-at-L2",
              "title": "Hit at L2",
              "front": "Hit at L2",
              "back": "Fast (5-20 cycles)",
              "type": "definition",
              "section": "Multi-Level Caches",
              "subsection": "Multi-Level Cache Operation"
            },
            {
              "id": "4-Multi-Level-Caches-Multi-Level-Cache-Operation-Hit-at-L3",
              "title": "Hit at L3",
              "front": "Hit at L3",
              "back": "Moderate (20-50 cycles)",
              "type": "definition",
              "section": "Multi-Level Caches",
              "subsection": "Multi-Level Cache Operation"
            },
            {
              "id": "4-Multi-Level-Caches-Multi-Level-Cache-Operation-Miss--Main-Memory-",
              "title": "Miss (Main Memory)",
              "front": "Miss (Main Memory)",
              "back": "Slow (50-100+ cycles)",
              "type": "definition",
              "section": "Multi-Level Caches",
              "subsection": "Multi-Level Cache Operation"
            }
          ]
        },
        {
          "subsection": "Multi-Level Performance",
          "cardCount": 5,
          "cards": [
            {
              "id": "4-Multi-Level-Caches-Multi-Level-Performance-Example-Calculation",
              "title": "Example Calculation",
              "front": "Example Calculation",
              "back": "",
              "type": "definition",
              "section": "Multi-Level Caches",
              "subsection": "Multi-Level Performance"
            },
            {
              "id": "4-Multi-Level-Caches-Multi-Level-Performance-Given",
              "title": "Given",
              "front": "Given",
              "back": "1. Base CPI = 1\n2. Clock rate = 4 GHz (0.25 ns per cycle)\n3. L1 miss rate = 2%\n4. L2 access time = 5 ns\n5. L2 global miss rate = 0.5%\n6. Main memory access time = 100 ns",
              "type": "list",
              "section": "Multi-Level Caches",
              "subsection": "Multi-Level Performance"
            },
            {
              "id": "4-Multi-Level-Caches-Multi-Level-Performance-Without-L2",
              "title": "Without L2",
              "front": "Without L2",
              "back": "Miss penalty = 100 ns / 0.25 ns = 400 cycles CPI = 1 + 0.02 × 400 = 9",
              "type": "definition",
              "section": "Multi-Level Caches",
              "subsection": "Multi-Level Performance"
            },
            {
              "id": "4-Multi-Level-Caches-Multi-Level-Performance-With-L2",
              "title": "With L2",
              "front": "With L2",
              "back": "L1 miss, L2 hit penalty = 5 ns / 0.25 ns = 20 cycles L1 miss, L2 miss penalty = 100 ns / 0.25 ns = 400 cycles CPI = 1 + 0.02 × 20 + 0.005 × 400 = 1 + 0.4 + 2",
              "type": "definition",
              "section": "Multi-Level Caches",
              "subsection": "Multi-Level Performance"
            },
            {
              "id": "4-Multi-Level-Caches-Multi-Level-Performance-Performance-Improvement",
              "title": "Performance Improvement",
              "front": "Performance Improvement",
              "back": "9/3.4 = 2.6x faster!",
              "type": "definition",
              "section": "Multi-Level Caches",
              "subsection": "Multi-Level Performance"
            }
          ]
        },
        {
          "subsection": "Unified vs. Split Caches",
          "cardCount": 3,
          "cards": [
            {
              "id": "4-Multi-Level-Caches-Unified-vs--Split-Caches-Unified-Cache",
              "title": "Unified Cache",
              "front": "Unified Cache",
              "back": "1. Single cache for both instructions and data\n2. **Advantages:**\n3. Higher hit rate (balances load automatically)\n4. Simpler design\n5. **Disadvantages:**\n6. Contention between instruction fetch and data access\n7. Problematic for pipelining",
              "type": "list",
              "section": "Multi-Level Caches",
              "subsection": "Unified vs. Split Caches"
            },
            {
              "id": "4-Multi-Level-Caches-Unified-vs--Split-Caches-Split-Cache",
              "title": "Split Cache",
              "front": "Split Cache",
              "back": "1. Separate instruction cache (I-cache) and data cache (D-cache)\n2. **Advantages:**\n3. No contention\n4. Better for pipelining\n5. Can optimize each separately\n6. **Disadvantages:**\n7. Lower hit rate (fixed allocation)\n8. More complex",
              "type": "list",
              "section": "Multi-Level Caches",
              "subsection": "Unified vs. Split Caches"
            },
            {
              "id": "4-Multi-Level-Caches-Unified-vs--Split-Caches-Modern-Practice",
              "title": "Modern Practice",
              "front": "Modern Practice",
              "back": "1. **L1:** Split (I-cache and D-cache)\n2. **L2/L3:** Unified (shared)",
              "type": "list",
              "section": "Multi-Level Caches",
              "subsection": "Unified vs. Split Caches"
            }
          ]
        }
      ]
    },
    {
      "section": "Internal Memory: DRAM and SRAM",
      "subsections": [
        {
          "subsection": "Semiconductor Memory Overview",
          "cardCount": 1,
          "cards": [
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Semiconductor-Memory-Overview-Two-Main-Types",
              "title": "Two Main Types",
              "front": "Two Main Types",
              "back": "1. **RAM (Random Access Memory):**\n2. Read/Write\n3. Volatile\n4. Temporary storage\n5. Static (SRAM) or Dynamic (DRAM)\n6. **ROM (Read-Only Memory):**\n7. Read-only (mostly)\n8. Nonvolatile\n9. Permanent storage\n10. Various types (ROM, PROM, EPROM, EEPROM, Flash)",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Semiconductor Memory Overview"
            }
          ]
        },
        {
          "subsection": "Dynamic RAM (DRAM)",
          "cardCount": 10,
          "cards": [
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Structure",
              "title": "Structure",
              "front": "Structure",
              "back": "",
              "type": "concept",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Dynamic RAM (DRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Basic-Cell",
              "title": "Basic Cell",
              "front": "Basic Cell",
              "back": "1. **Capacitor:** Stores charge (1) or no charge (0)\n2. **Transistor:** Switch to access capacitor",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Dynamic RAM (DRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Operation",
              "title": "Operation",
              "front": "Operation",
              "back": "1. **Write:** Apply voltage to bit line, activate address line, charge/discharge capacitor\n2. **Read:** Activate address line, sense charge on capacitor, restore charge",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Dynamic RAM (DRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Characteristics",
              "title": "Characteristics",
              "front": "Characteristics",
              "back": "",
              "type": "concept",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Dynamic RAM (DRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Storage-Mechanism",
              "title": "Storage Mechanism",
              "front": "Storage Mechanism",
              "back": "1. Bits stored as **charge in capacitors**\n2. Presence/absence of charge = binary 1/0\n3. **Essentially analogue:** Level of charge determines value",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Dynamic RAM (DRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Key-Properties",
              "title": "Key Properties",
              "front": "Key Properties",
              "back": "1. **Charges leak:** Capacitors lose charge over time\n2. **Needs refreshing:** Periodic refresh to maintain data\n3. **Dynamic:** Stored charge leaks away even with power",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Dynamic RAM (DRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Refresh-Requirements",
              "title": "Refresh Requirements",
              "front": "Refresh Requirements",
              "back": "1. Must refresh every few milliseconds (typically 64 ms)\n2. Refresh circuit included on chip\n3. Refresh process:\n4. Disable chip\n5. Count through rows\n6. Read and write back each row\n7. Takes time, slows performance",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Dynamic RAM (DRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Advantages",
              "title": "Advantages",
              "front": "Advantages",
              "back": "1. **Simpler construction:** Fewer transistors per cell\n2. **Smaller per bit:** Higher density\n3. **Less expensive:** Lower cost\n4. **High capacity:** Good for main memory",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Dynamic RAM (DRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Disadvantages",
              "title": "Disadvantages",
              "front": "Disadvantages",
              "back": "1. **Needs refresh circuits:** Additional complexity\n2. **Slower:** Refresh overhead\n3. **More complex timing:** Refresh cycles",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Dynamic RAM (DRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Use",
              "title": "Use",
              "front": "Use",
              "back": "**Main memory** (large capacity needed, cost-sensitive)",
              "type": "definition",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Dynamic RAM (DRAM)"
            }
          ]
        },
        {
          "subsection": "Static RAM (SRAM)",
          "cardCount": 9,
          "cards": [
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Structure",
              "title": "Structure",
              "front": "Structure",
              "back": "",
              "type": "concept",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Static RAM (SRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Basic-Cell",
              "title": "Basic Cell",
              "front": "Basic Cell",
              "back": "1. **Flip-flop circuit:** Two cross-coupled inverters\n2. **Transistor arrangement:** Provides stable logic state\n3. **No capacitor:** State maintained by circuit",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Static RAM (SRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Operation",
              "title": "Operation",
              "front": "Operation",
              "back": "1. **State 1:** C1 high, C2 low (T1, T4 off; T2, T3 on)\n2. **State 0:** C2 high, C1 low (T2, T3 off; T1, T4 on)\n3. **Write:** Apply value to bit lines\n4. **Read:** Sense value on bit line",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Static RAM (SRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Characteristics",
              "title": "Characteristics",
              "front": "Characteristics",
              "back": "",
              "type": "concept",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Static RAM (SRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Storage-Mechanism",
              "title": "Storage Mechanism",
              "front": "Storage Mechanism",
              "back": "1. Bits stored as **on/off switches** (flip-flops)\n2. Digital: Clear 1 or 0 state\n3. **No charges to leak:** State maintained by circuit",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Static RAM (SRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Key-Properties",
              "title": "Key Properties",
              "front": "Key Properties",
              "back": "1. **No refresh needed:** Maintains state as long as powered\n2. **Faster:** No refresh overhead\n3. **More complex:** More transistors per cell",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Static RAM (SRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Advantages",
              "title": "Advantages",
              "front": "Advantages",
              "back": "1. **No refresh needed:** Simpler operation\n2. **Faster:** Lower access time\n3. **Simpler timing:** No refresh cycles",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Static RAM (SRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Disadvantages",
              "title": "Disadvantages",
              "front": "Disadvantages",
              "back": "1. **More complex construction:** More transistors\n2. **Larger per bit:** Lower density\n3. **More expensive:** Higher cost",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Static RAM (SRAM)"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Use",
              "title": "Use",
              "front": "Use",
              "back": "**Cache memory** (speed critical, smaller capacity)",
              "type": "definition",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "Static RAM (SRAM)"
            }
          ]
        },
        {
          "subsection": "SRAM vs. DRAM Comparison",
          "cardCount": 1,
          "cards": [
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-SRAM-vs--DRAM-Comparison-Key-Insight",
              "title": "Key Insight",
              "front": "Key Insight",
              "back": "SRAM is faster but more expensive. DRAM is cheaper but slower. Use SRAM for small, fast cache; DRAM for large, cheap main memory.",
              "type": "definition",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "SRAM vs. DRAM Comparison"
            }
          ]
        },
        {
          "subsection": "DRAM Organization",
          "cardCount": 3,
          "cards": [
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-DRAM-Organization-Internal-Structure",
              "title": "Internal Structure",
              "front": "Internal Structure",
              "back": "1. Organized as **2D array** of cells\n2. **Row and column addressing:**\n3. Reduces number of address pins\n4. Multiplex row and column addresses\n5. Example: 2048 × 2048 × 4 bits = 16 Mbit chip\n6. Only 11 address pins needed (2^11 = 2048)",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "DRAM Organization"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-DRAM-Organization-Module-Organization",
              "title": "Module Organization",
              "front": "Module Organization",
              "back": "1. Multiple chips combined to form memory module\n2. Example: 256 KB module = 8 chips × 32 Kbit each\n3. Example: 1 MB module = 8 chips × 1 Mbit each",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "DRAM Organization"
            },
            {
              "id": "4-Internal-Memory--DRAM-and-SRAM-DRAM-Organization-Interleaved-Memory",
              "title": "Interleaved Memory",
              "front": "Interleaved Memory",
              "back": "1. Multiple memory banks\n2. Each bank can service requests independently\n3. Consecutive words stored in different banks\n4. **K banks** can service **K requests** simultaneously\n5. Increases memory bandwidth",
              "type": "list",
              "section": "Internal Memory: DRAM and SRAM",
              "subsection": "DRAM Organization"
            }
          ]
        }
      ]
    },
    {
      "section": "Error Detection and Correction",
      "subsections": [
        {
          "subsection": "Why Error Correction?",
          "cardCount": 2,
          "cards": [
            {
              "id": "4-Error-Detection-and-Correction-Why-Error-Correction--Problem",
              "title": "Problem",
              "front": "Problem",
              "back": "Memory can have errors.",
              "type": "definition",
              "section": "Error Detection and Correction",
              "subsection": "Why Error Correction?"
            },
            {
              "id": "4-Error-Detection-and-Correction-Why-Error-Correction--Types-of-Errors",
              "title": "Types of Errors",
              "front": "Types of Errors",
              "back": "1. **Hard Failure:**\n2. Permanent defect\n3. Manufacturing defect\n4. Physical damage\n5. **Soft Error:**\n6. Random, non-destructive\n7. Caused by:\n8. Alpha particles\n9. Cosmic rays\n10. Electrical noise\n11. No permanent damage\n12. Data corruption",
              "type": "list",
              "section": "Error Detection and Correction",
              "subsection": "Why Error Correction?"
            }
          ]
        },
        {
          "subsection": "Hamming Error Correcting Code",
          "cardCount": 8,
          "cards": [
            {
              "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Purpose",
              "title": "Purpose",
              "front": "Purpose",
              "back": "Detect and correct single-bit errors.",
              "type": "definition",
              "section": "Error Detection and Correction",
              "subsection": "Hamming Error Correcting Code"
            },
            {
              "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Principle",
              "title": "Principle",
              "front": "Principle",
              "back": "1. Add **check bits** (redundancy) to data\n2. Check bits encode information about data bits\n3. Can detect and correct errors",
              "type": "list",
              "section": "Error Detection and Correction",
              "subsection": "Hamming Error Correcting Code"
            },
            {
              "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Hamming-Distance",
              "title": "Hamming Distance",
              "front": "Hamming Distance",
              "back": "1. Minimum number of bit positions in which two code words differ\n2. For single-bit error correction: Minimum distance = 3",
              "type": "list",
              "section": "Error Detection and Correction",
              "subsection": "Hamming Error Correcting Code"
            },
            {
              "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Implementation",
              "title": "Implementation",
              "front": "Implementation",
              "back": "1. Check bits placed at positions that are powers of 2 (1, 2, 4, 8, ...)\n2. Each check bit covers specific data bits\n3. Check bits calculated using XOR operations",
              "type": "list",
              "section": "Error Detection and Correction",
              "subsection": "Hamming Error Correcting Code"
            },
            {
              "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Example---8-bit-data",
              "title": "Example - 8-bit data",
              "front": "Example - 8-bit data",
              "back": "1. Need 4 check bits (positions 1, 2, 4, 8)\n2. Total: 12 bits (8 data + 4 check)",
              "type": "list",
              "section": "Error Detection and Correction",
              "subsection": "Hamming Error Correcting Code"
            },
            {
              "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Error-Detection",
              "title": "Error Detection",
              "front": "Error Detection",
              "back": "1. Recalculate check bits from data\n2. Compare with stored check bits\n3. If different: Error detected\n4. Pattern indicates which bit is wrong",
              "type": "list",
              "section": "Error Detection and Correction",
              "subsection": "Hamming Error Correcting Code"
            },
            {
              "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Error-Correction",
              "title": "Error Correction",
              "front": "Error Correction",
              "back": "1. Error pattern identifies bit position\n2. Flip that bit to correct error",
              "type": "list",
              "section": "Error Detection and Correction",
              "subsection": "Hamming Error Correcting Code"
            },
            {
              "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Overhead",
              "title": "Overhead",
              "front": "Overhead",
              "back": "1. More bits needed (redundancy)\n2. Example: 8 data bits → 12 total bits (50% overhead)\n3. Larger data words: Lower overhead percentage",
              "type": "list",
              "section": "Error Detection and Correction",
              "subsection": "Hamming Error Correcting Code"
            }
          ]
        }
      ]
    },
    {
      "section": "Advanced DRAM Technologies",
      "subsections": [
        {
          "subsection": "Traditional DRAM Limitations",
          "cardCount": 1,
          "cards": [
            {
              "id": "4-Advanced-DRAM-Technologies-Traditional-DRAM-Limitations-Problems",
              "title": "Problems",
              "front": "Problems",
              "back": "1. **Internal architecture:** Slow access patterns\n2. **Interface:** Asynchronous, processor must wait\n3. **Bottleneck:** Memory interface limits performance",
              "type": "list",
              "section": "Advanced DRAM Technologies",
              "subsection": "Traditional DRAM Limitations"
            }
          ]
        },
        {
          "subsection": "Synchronous DRAM (SDRAM)",
          "cardCount": 4,
          "cards": [
            {
              "id": "4-Advanced-DRAM-Technologies-Synchronous-DRAM--SDRAM--Key-Innovation",
              "title": "Key Innovation",
              "front": "Key Innovation",
              "back": "Access synchronized with external clock.",
              "type": "definition",
              "section": "Advanced DRAM Technologies",
              "subsection": "Synchronous DRAM (SDRAM)"
            },
            {
              "id": "4-Advanced-DRAM-Technologies-Synchronous-DRAM--SDRAM--Operation",
              "title": "Operation",
              "front": "Operation",
              "back": "1. Address presented to RAM\n2. RAM finds data\n3. **CPU knows when data will be ready** (synchronized with clock)\n4. CPU doesn't have to wait (can do other work)",
              "type": "list",
              "section": "Advanced DRAM Technologies",
              "subsection": "Synchronous DRAM (SDRAM)"
            },
            {
              "id": "4-Advanced-DRAM-Technologies-Synchronous-DRAM--SDRAM--Burst-Mode",
              "title": "Burst Mode",
              "front": "Burst Mode",
              "back": "1. Set up stream of data\n2. Fire out data in block\n3. More efficient than single-word transfers",
              "type": "list",
              "section": "Advanced DRAM Technologies",
              "subsection": "Synchronous DRAM (SDRAM)"
            },
            {
              "id": "4-Advanced-DRAM-Technologies-Synchronous-DRAM--SDRAM--Advantages",
              "title": "Advantages",
              "front": "Advantages",
              "back": "1. Predictable timing\n2. CPU can pipeline other operations\n3. Better bandwidth utilization",
              "type": "list",
              "section": "Advanced DRAM Technologies",
              "subsection": "Synchronous DRAM (SDRAM)"
            }
          ]
        },
        {
          "subsection": "Double Data Rate SDRAM (DDR SDRAM)",
          "cardCount": 5,
          "cards": [
            {
              "id": "4-Advanced-DRAM-Technologies-Double-Data-Rate-SDRAM--DDR-SDRAM--Key-Innovation",
              "title": "Key Innovation",
              "front": "Key Innovation",
              "back": "Sends data **twice per clock cycle**.",
              "type": "definition",
              "section": "Advanced DRAM Technologies",
              "subsection": "Double Data Rate SDRAM (DDR SDRAM)"
            },
            {
              "id": "4-Advanced-DRAM-Technologies-Double-Data-Rate-SDRAM--DDR-SDRAM--How",
              "title": "How",
              "front": "How",
              "back": "1. Data transfer on **both rising and falling edge** of clock\n2. Doubles data rate compared to SDRAM",
              "type": "list",
              "section": "Advanced DRAM Technologies",
              "subsection": "Double Data Rate SDRAM (DDR SDRAM)"
            },
            {
              "id": "4-Advanced-DRAM-Technologies-Double-Data-Rate-SDRAM--DDR-SDRAM--Achieves-Higher-Rates-Through",
              "title": "Achieves Higher Rates Through",
              "front": "Achieves Higher Rates Through",
              "back": "1. **Double clocking:** Data on both edges\n2. **Higher bus clock rate:** Faster interface\n3. **Buffering scheme:** Prefetch and buffer data",
              "type": "list",
              "section": "Advanced DRAM Technologies",
              "subsection": "Double Data Rate SDRAM (DDR SDRAM)"
            },
            {
              "id": "4-Advanced-DRAM-Technologies-Double-Data-Rate-SDRAM--DDR-SDRAM--Generations",
              "title": "Generations",
              "front": "Generations",
              "back": "1. **DDR:** 2x SDRAM\n2. **DDR2:** 2x DDR (4x SDRAM)\n3. **DDR3:** 2x DDR2 (8x SDRAM)\n4. **DDR4:** 2x DDR3 (16x SDRAM)\n5. **DDR5:** 2x DDR4 (32x SDRAM)",
              "type": "list",
              "section": "Advanced DRAM Technologies",
              "subsection": "Double Data Rate SDRAM (DDR SDRAM)"
            },
            {
              "id": "4-Advanced-DRAM-Technologies-Double-Data-Rate-SDRAM--DDR-SDRAM--Each-Generation",
              "title": "Each Generation",
              "front": "Each Generation",
              "back": "1. Higher data rates\n2. Lower voltage (power efficiency)\n3. Better signal integrity\n4. More features",
              "type": "list",
              "section": "Advanced DRAM Technologies",
              "subsection": "Double Data Rate SDRAM (DDR SDRAM)"
            }
          ]
        },
        {
          "subsection": "Other Advanced Technologies",
          "cardCount": 2,
          "cards": [
            {
              "id": "4-Advanced-DRAM-Technologies-Other-Advanced-Technologies-DDR-Variations",
              "title": "DDR Variations",
              "front": "DDR Variations",
              "back": "1. **GDDR:** Graphics DDR (for GPUs)\n2. **LPDDR:** Low Power DDR (for mobile devices)",
              "type": "list",
              "section": "Advanced DRAM Technologies",
              "subsection": "Other Advanced Technologies"
            },
            {
              "id": "4-Advanced-DRAM-Technologies-Other-Advanced-Technologies-Future-Technologies",
              "title": "Future Technologies",
              "front": "Future Technologies",
              "back": "1. **HBM (High Bandwidth Memory):** 3D stacked memory\n2. **HMC (Hybrid Memory Cube):** Advanced 3D memory",
              "type": "list",
              "section": "Advanced DRAM Technologies",
              "subsection": "Other Advanced Technologies"
            }
          ]
        }
      ]
    },
    {
      "section": "Key Concepts Summary",
      "subsections": [
        {
          "subsection": "Memory Hierarchy Principles",
          "cardCount": 1,
          "cards": [
            {
              "id": "4-Key-Concepts-Summary-Memory-Hierarchy-Principles-Memory-Hierarchy-Principles",
              "title": "Memory Hierarchy Principles",
              "front": "Memory Hierarchy Principles",
              "back": "1. **Trade-offs:** Speed, capacity, cost cannot all be optimized simultaneously\n2. **Hierarchy Solution:** Use multiple levels with different characteristics\n3. **Locality:** Temporal and spatial locality enable hierarchy to work\n4. **Performance:** Average access time much closer to fast memory than slow memory",
              "type": "list",
              "section": "Key Concepts Summary",
              "subsection": "Memory Hierarchy Principles"
            }
          ]
        },
        {
          "subsection": "Cache Fundamentals",
          "cardCount": 1,
          "cards": [
            {
              "id": "4-Key-Concepts-Summary-Cache-Fundamentals-Cache-Fundamentals",
              "title": "Cache Fundamentals",
              "front": "Cache Fundamentals",
              "back": "1. **Purpose:** Bridge speed gap between CPU and main memory\n2. **Operation:** Store recently accessed blocks\n3. **Metrics:** Hit rate, miss rate, miss penalty\n4. **Performance:** AMAT = Hit time + (Miss rate × Miss penalty)",
              "type": "list",
              "section": "Key Concepts Summary",
              "subsection": "Cache Fundamentals"
            }
          ]
        },
        {
          "subsection": "Cache Design",
          "cardCount": 1,
          "cards": [
            {
              "id": "4-Key-Concepts-Summary-Cache-Design-Cache-Design",
              "title": "Cache Design",
              "front": "Cache Design",
              "back": "1. **Mapping:** Direct, associative, set-associative (trade-offs)\n2. **Replacement:** LRU most effective, but more complex\n3. **Write Policy:** Write back better performance, write through simpler\n4. **Size:** Larger = better hit rate, but slower hit time",
              "type": "list",
              "section": "Key Concepts Summary",
              "subsection": "Cache Design"
            }
          ]
        },
        {
          "subsection": "Memory Technologies",
          "cardCount": 1,
          "cards": [
            {
              "id": "4-Key-Concepts-Summary-Memory-Technologies-Memory-Technologies",
              "title": "Memory Technologies",
              "front": "Memory Technologies",
              "back": "1. **DRAM:** Cheap, dense, needs refresh, used for main memory\n2. **SRAM:** Fast, expensive, no refresh, used for cache\n3. **Error Correction:** Hamming codes detect and correct errors\n4. **Advanced DRAM:** SDRAM, DDR improve performance",
              "type": "list",
              "section": "Key Concepts Summary",
              "subsection": "Memory Technologies"
            }
          ]
        }
      ]
    },
    {
      "section": "Practice Problems and Examples",
      "subsections": [
        {
          "subsection": "Problem 1: Cache Address Breakdown",
          "cardCount": 3,
          "cards": [
            {
              "id": "4-Practice-Problems-and-Examples-Problem-1--Cache-Address-Breakdown-Question",
              "title": "Question",
              "front": "Question",
              "back": "A direct-mapped cache has 32 lines, block size 8 bytes. Main memory is 16 MB. How is a 24-bit address divided?",
              "type": "definition",
              "section": "Practice Problems and Examples",
              "subsection": "Problem 1: Cache Address Breakdown"
            },
            {
              "id": "4-Practice-Problems-and-Examples-Problem-1--Cache-Address-Breakdown-Solution",
              "title": "Solution",
              "front": "Solution",
              "back": "1. Cache lines: 32 = 2^5 → 5 bits for line\n2. Block size: 8 bytes = 2^3 → 3 bits for word offset\n3. Tag: 24 - 5 - 3 = 16 bits",
              "type": "list",
              "section": "Practice Problems and Examples",
              "subsection": "Problem 1: Cache Address Breakdown"
            },
            {
              "id": "4-Practice-Problems-and-Examples-Problem-1--Cache-Address-Breakdown-Answer",
              "title": "Answer",
              "front": "Answer",
              "back": "1. Tag: 16 bits\n2. Line: 5 bits\n3. Word: 3 bits",
              "type": "list",
              "section": "Practice Problems and Examples",
              "subsection": "Problem 1: Cache Address Breakdown"
            }
          ]
        },
        {
          "subsection": "Problem 2: AMAT Calculation",
          "cardCount": 3,
          "cards": [
            {
              "id": "4-Practice-Problems-and-Examples-Problem-2--AMAT-Calculation-Question",
              "title": "Question",
              "front": "Question",
              "back": "Cache has hit time 2 ns, miss rate 3%, miss penalty 50 ns. What is AMAT?",
              "type": "definition",
              "section": "Practice Problems and Examples",
              "subsection": "Problem 2: AMAT Calculation"
            },
            {
              "id": "4-Practice-Problems-and-Examples-Problem-2--AMAT-Calculation-Solution",
              "title": "Solution",
              "front": "Solution",
              "back": "AMAT = Hit time + (Miss rate × Miss penalty) = 2 + (0.03 × 50)",
              "type": "definition",
              "section": "Practice Problems and Examples",
              "subsection": "Problem 2: AMAT Calculation"
            },
            {
              "id": "4-Practice-Problems-and-Examples-Problem-2--AMAT-Calculation-Answer",
              "title": "Answer",
              "front": "Answer",
              "back": "3.5 ns",
              "type": "definition",
              "section": "Practice Problems and Examples",
              "subsection": "Problem 2: AMAT Calculation"
            }
          ]
        },
        {
          "subsection": "Problem 3: CPI with Cache",
          "cardCount": 3,
          "cards": [
            {
              "id": "4-Practice-Problems-and-Examples-Problem-3--CPI-with-Cache-Question",
              "title": "Question",
              "front": "Question",
              "back": "Base CPI = 2, I-cache miss rate = 2%, D-cache miss rate = 5%, 40% loads/stores, miss penalty = 100 cycles. What is actual CPI?",
              "type": "definition",
              "section": "Practice Problems and Examples",
              "subsection": "Problem 3: CPI with Cache"
            },
            {
              "id": "4-Practice-Problems-and-Examples-Problem-3--CPI-with-Cache-Solution",
              "title": "Solution",
              "front": "Solution",
              "back": "Instruction miss cycles = 0.02 × 100 = 2 Data miss cycles = 0.4 × 0.05 × 100 = 2 CPI_actual = 2 + 2 + 2 = 6",
              "type": "definition",
              "section": "Practice Problems and Examples",
              "subsection": "Problem 3: CPI with Cache"
            },
            {
              "id": "4-Practice-Problems-and-Examples-Problem-3--CPI-with-Cache-Answer",
              "title": "Answer",
              "front": "Answer",
              "back": "CPI = 6",
              "type": "definition",
              "section": "Practice Problems and Examples",
              "subsection": "Problem 3: CPI with Cache"
            }
          ]
        },
        {
          "subsection": "Problem 4: Set-Associative Mapping",
          "cardCount": 3,
          "cards": [
            {
              "id": "4-Practice-Problems-and-Examples-Problem-4--Set-Associative-Mapping-Question",
              "title": "Question",
              "front": "Question",
              "back": "4-way set associative cache, 64 lines total. How many sets? Block 100 maps to which set?",
              "type": "definition",
              "section": "Practice Problems and Examples",
              "subsection": "Problem 4: Set-Associative Mapping"
            },
            {
              "id": "4-Practice-Problems-and-Examples-Problem-4--Set-Associative-Mapping-Solution",
              "title": "Solution",
              "front": "Solution",
              "back": "1. Total lines: 64\n2. Ways per set: 4\n3. Number of sets: 64 / 4 = 16 sets\n4. Set bits: log2(16) = 4 bits\n5. Block 100: 100 mod 16 = 4 → Set 4",
              "type": "list",
              "section": "Practice Problems and Examples",
              "subsection": "Problem 4: Set-Associative Mapping"
            },
            {
              "id": "4-Practice-Problems-and-Examples-Problem-4--Set-Associative-Mapping-Answer",
              "title": "Answer",
              "front": "Answer",
              "back": "16 sets, Block 100 → Set 4",
              "type": "definition",
              "section": "Practice Problems and Examples",
              "subsection": "Problem 4: Set-Associative Mapping"
            }
          ]
        },
        {
          "subsection": "Problem 5: Multi-Level Cache",
          "cardCount": 3,
          "cards": [
            {
              "id": "4-Practice-Problems-and-Examples-Problem-5--Multi-Level-Cache-Question",
              "title": "Question",
              "front": "Question",
              "back": "L1 hit time = 1 cycle, miss rate = 5%, L2 hit time = 10 cycles, global miss rate = 1%, main memory = 100 cycles. What is AMAT?",
              "type": "definition",
              "section": "Practice Problems and Examples",
              "subsection": "Problem 5: Multi-Level Cache"
            },
            {
              "id": "4-Practice-Problems-and-Examples-Problem-5--Multi-Level-Cache-Solution",
              "title": "Solution",
              "front": "Solution",
              "back": "L1 hit: 0.95 × 1 = 0.95 cycles L1 miss, L2 hit: 0.05 × 0.8 × 10 = 0.4 cycles (80% of L1 misses hit in L2) L1 miss, L2 miss: 0.05 × 0.2 × 100 = 1.0 cycles (20% of L1 misses miss in L2) AMAT = 0.95 + 0.4 + 1.0 = 2.35 cycles",
              "type": "definition",
              "section": "Practice Problems and Examples",
              "subsection": "Problem 5: Multi-Level Cache"
            },
            {
              "id": "4-Practice-Problems-and-Examples-Problem-5--Multi-Level-Cache-Answer",
              "title": "Answer",
              "front": "Answer",
              "back": "1. **Understand Trade-offs:**\n2. Every design decision has pros and cons\n3. Larger cache vs. faster cache\n4. Higher associativity vs. lower cost\n5. **Master Calculations:**\n6. AMAT formula\n7. CPI with cache misses\n8. Address breakdown for different mappings\n9. **Visualize Cache Organization:**\n10. Draw cache structures\n11. Understand tag, line, word fields\n12. Trace cache operations\n13. **Compare Techniques:**\n14. Direct vs. associative vs. set-associative\n15. Write through vs. write back\n16. Replacement algorithms\n17. **Understand Locality:**\n18. Why hierarchy works\n19. How to exploit locality\n20. Impact on performance\n21. **Memory hierarchy** solves the speed/capacity/cost trade-off\n22. **Locality of reference** makes hierarchy effective\n23. **Cache memory** bridges the CPU-memory speed gap\n24. **Mapping techniques** balance flexibility and cost\n25. **Multi-level caches** further improve performance\n26. **DRAM and SRAM** serve different roles in the hierarchy\n27. Memory is often the performance bottleneck\n28. Cache design significantly impacts performance\n29. Memory hierarchy is fundamental to computer architecture",
              "type": "list",
              "section": "Practice Problems and Examples",
              "subsection": "Problem 5: Multi-Level Cache"
            }
          ]
        }
      ]
    }
  ],
  "allCards": [
    {
      "id": "4-Detailed-Study-Guide--Detailed-Study-Guide",
      "title": "Detailed Study Guide",
      "front": "Detailed Study Guide",
      "back": "1. [Memory Characteristics and Classification](#memory-characteristics-and-classification)\n2. [The Memory Hierarchy Concept](#the-memory-hierarchy-concept)\n3. [Locality of Reference](#locality-of-reference)\n4. [Cache Memory Fundamentals](#cache-memory-fundamentals)\n5. [Cache Mapping Techniques](#cache-mapping-techniques)\n6. [Cache Replacement Policies](#cache-replacement-policies)\n7. [Write Policies](#write-policies)\n8. [Cache Performance Analysis](#cache-performance-analysis)\n9. [Multi-Level Caches](#multi-level-caches)\n10. [Internal Memory: DRAM and SRAM](#internal-memory-dram-and-sram)\n11. [Error Detection and Correction](#error-detection-and-correction)\n12. [Advanced DRAM Technologies](#advanced-dram-technologies)\n13. [Key Concepts Summary](#key-concepts-summary)\n14. [Practice Problems and Examples](#practice-problems-and-examples)",
      "type": "list",
      "section": "Detailed Study Guide",
      "subsection": ""
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-1--Location",
      "title": "1. Location",
      "front": "1. Location",
      "back": "",
      "type": "concept",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-CPU",
      "title": "CPU",
      "front": "CPU",
      "back": "1. **Registers:** Fastest, smallest, most expensive\n2. Located directly in processor\n3. Used for temporary storage during execution",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Internal",
      "title": "Internal",
      "front": "Internal",
      "back": "1. **Main Memory (RAM):** Primary storage\n2. **Cache Memory:** Fast buffer between CPU and main memory\n3. Accessible directly by processor",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-External",
      "title": "External",
      "front": "External",
      "back": "1. **Secondary Storage:** Disk drives, SSDs, tape\n2. Accessible via I/O controllers\n3. Persistent, non-volatile",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-2--Capacity",
      "title": "2. Capacity",
      "front": "2. Capacity",
      "back": "",
      "type": "concept",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Word-Size",
      "title": "Word Size",
      "front": "Word Size",
      "back": "1. Natural unit of organization\n2. Common sizes: 8, 16, 32, 64 bits\n3. Determines how much data can be processed at once",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Number-of-Words-Bytes",
      "title": "Number of Words/Bytes",
      "front": "Number of Words/Bytes",
      "back": "1. Total storage capacity\n2. External memory typically expressed in bytes (KB, MB, GB, TB)\n3. Internal memory may be expressed in words or bytes",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Relationship",
      "title": "Relationship",
      "front": "Relationship",
      "back": "1. Address length (A bits) → 2^A addressable units\n2. Example: 20-bit address → 2^20 = 1,048,576 locations",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-3--Unit-of-Transfer",
      "title": "3. Unit of Transfer",
      "front": "3. Unit of Transfer",
      "back": "",
      "type": "concept",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Internal-Memory",
      "title": "Internal Memory",
      "front": "Internal Memory",
      "back": "1. Usually governed by bus data width\n2. May equal word length, but often larger\n3. Example: 32-bit processor with 64-bit data bus",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-External-Memory",
      "title": "External Memory",
      "front": "External Memory",
      "back": "1. Usually a **block** (much larger than a word)\n2. Example: Disk sectors (512 bytes, 4 KB, etc.)",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Addressable-Unit",
      "title": "Addressable Unit",
      "front": "Addressable Unit",
      "back": "1. Smallest location that can be uniquely addressed\n2. Typically: byte (8 bits) or word (16/32/64 bits)",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-4--Access-Methods",
      "title": "4. Access Methods",
      "front": "4. Access Methods",
      "back": "",
      "type": "concept",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Sequential-Access",
      "title": "Sequential Access",
      "front": "Sequential Access",
      "back": "1. Memory organized into records\n2. Must start at beginning and read through in order\n3. Access time variable, depends on location\n4. **Example:** Magnetic tape",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Direct-Access",
      "title": "Direct Access",
      "front": "Direct Access",
      "back": "1. Individual blocks have unique addresses\n2. Access by jumping to vicinity plus sequential search\n3. Access time depends on location and previous location\n4. **Example:** Magnetic disk",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Random-Access",
      "title": "Random Access",
      "front": "Random Access",
      "back": "1. Individual addresses identify locations exactly\n2. Access time independent of location or previous access\n3. **Example:** RAM, cache",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Associative-Access",
      "title": "Associative Access",
      "front": "Associative Access",
      "back": "1. Word retrieved based on portion of contents (not address)\n2. Access time independent of location or previous access\n3. **Example:** Cache (when searching by tag)",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-5--Performance",
      "title": "5. Performance",
      "front": "5. Performance",
      "back": "",
      "type": "concept",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Access-Time--Latency-",
      "title": "Access Time (Latency)",
      "front": "Access Time (Latency)",
      "back": "1. Time between presenting address and getting valid data\n2. Critical for performance\n3. Measured in nanoseconds (ns) or clock cycles",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Memory-Cycle-Time",
      "title": "Memory Cycle Time",
      "front": "Memory Cycle Time",
      "back": "1. Time required for memory to \"recover\" before next access\n2. Access time + recovery time\n3. Concerned with system bus, not processor\n4. May be longer than access time",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Transfer-Rate",
      "title": "Transfer Rate",
      "front": "Transfer Rate",
      "back": "1. Rate at which data can be transferred into/out of memory\n2. Measured in bits/second or bytes/second\n3. For random-access memory: 1/(cycle time)\n4. Also called **bandwidth**",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Example",
      "title": "Example",
      "front": "Example",
      "back": "Access time: 50 ns Cycle time: 100 ns Transfer rate: 1/100ns = 10 MB/s (for 1-byte transfers)",
      "type": "definition",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-6--Physical-Types",
      "title": "6. Physical Types",
      "front": "6. Physical Types",
      "back": "",
      "type": "concept",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Semiconductor-Memory",
      "title": "Semiconductor Memory",
      "front": "Semiconductor Memory",
      "back": "1. RAM (Random Access Memory)\n2. ROM (Read-Only Memory)\n3. Flash memory",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Magnetic-Surface-Memory",
      "title": "Magnetic Surface Memory",
      "front": "Magnetic Surface Memory",
      "back": "1. Hard disk drives\n2. Magnetic tape",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Optical",
      "title": "Optical",
      "front": "Optical",
      "back": "1. CD, DVD, Blu-ray",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-7--Physical-Characteristics",
      "title": "7. Physical Characteristics",
      "front": "7. Physical Characteristics",
      "back": "",
      "type": "concept",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Volatile-vs--Nonvolatile",
      "title": "Volatile vs. Nonvolatile",
      "front": "Volatile vs. Nonvolatile",
      "back": "",
      "type": "definition",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Volatile-Memory",
      "title": "Volatile Memory",
      "front": "Volatile Memory",
      "back": "1. Information lost when power is switched off\n2. Requires continuous power to retain data\n3. **Examples:** DRAM, SRAM",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Nonvolatile-Memory",
      "title": "Nonvolatile Memory",
      "front": "Nonvolatile Memory",
      "back": "1. Information remains without deterioration\n2. No electrical power needed to retain information\n3. **Examples:** ROM, Flash, Magnetic disk",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Erasable-vs--Nonerasable",
      "title": "Erasable vs. Nonerasable",
      "front": "Erasable vs. Nonerasable",
      "back": "",
      "type": "definition",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Nonerasable-Memory",
      "title": "Nonerasable Memory",
      "front": "Nonerasable Memory",
      "back": "1. Cannot be altered (except by destroying storage unit)\n2. **Example:** ROM (Read-Only Memory)",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-Memory-Characteristics-and-Classification-Key-Characteristics-of-Memory-Systems-Erasable-Memory",
      "title": "Erasable Memory",
      "front": "Erasable Memory",
      "back": "1. Can be written and rewritten\n2. **Examples:** RAM, EEPROM, Flash",
      "type": "list",
      "section": "Memory Characteristics and Classification",
      "subsection": "Key Characteristics of Memory Systems"
    },
    {
      "id": "4-The-Memory-Hierarchy-Concept-The-Memory-Dilemma-Design-Constraints",
      "title": "Design Constraints",
      "front": "Design Constraints",
      "back": "1. **How much?** (Capacity)\n2. **How fast?** (Access time)\n3. **How expensive?** (Cost per bit)",
      "type": "list",
      "section": "The Memory Hierarchy Concept",
      "subsection": "The Memory Dilemma"
    },
    {
      "id": "4-The-Memory-Hierarchy-Concept-The-Memory-Dilemma-The-Trade-off",
      "title": "The Trade-off",
      "front": "The Trade-off",
      "back": "1. **Faster access time** → **Greater cost per bit**\n2. **Greater capacity** → **Smaller cost per bit**\n3. **Greater capacity** → **Slower access time**",
      "type": "list",
      "section": "The Memory Hierarchy Concept",
      "subsection": "The Memory Dilemma"
    },
    {
      "id": "4-The-Memory-Hierarchy-Concept-The-Memory-Dilemma-The-Problem",
      "title": "The Problem",
      "front": "The Problem",
      "back": "1. We want: Large capacity, fast access, low cost\n2. But: Can't have all three simultaneously!",
      "type": "list",
      "section": "The Memory Hierarchy Concept",
      "subsection": "The Memory Dilemma"
    },
    {
      "id": "4-The-Memory-Hierarchy-Concept-The-Solution--Memory-Hierarchy-Concept",
      "title": "Concept",
      "front": "Concept",
      "back": "Use multiple levels of memory with different speeds and sizes.",
      "type": "definition",
      "section": "The Memory Hierarchy Concept",
      "subsection": "The Solution: Memory Hierarchy"
    },
    {
      "id": "4-The-Memory-Hierarchy-Concept-The-Solution--Memory-Hierarchy-Principle",
      "title": "Principle",
      "front": "Principle",
      "back": "Store frequently accessed data in fast, expensive memory; store bulk data in slow, cheap memory.",
      "type": "definition",
      "section": "The Memory Hierarchy Concept",
      "subsection": "The Solution: Memory Hierarchy"
    },
    {
      "id": "4-The-Memory-Hierarchy-Concept-Memory-Hierarchy-Levels-From-Fastest-Smallest-Most-Expensive-to-Slowest-Largest-Cheapest",
      "title": "From Fastest/Smallest/Most Expensive to Slowest/Largest/Cheapest",
      "front": "From Fastest/Smallest/Most Expensive to Slowest/Largest/Cheapest",
      "back": "Level 1: CPU Registers Level 2: Cache Memory (L1, L2, L3) Level 3: Main Memory (DRAM) Level 4: Secondary Storage (Disk, SSD) Level 5: Tertiary Storage (Tape, Optical)",
      "type": "definition",
      "section": "The Memory Hierarchy Concept",
      "subsection": "Memory Hierarchy Levels"
    },
    {
      "id": "4-The-Memory-Hierarchy-Concept-Hierarchy-Characteristics-Going-Down-the-Hierarchy",
      "title": "Going Down the Hierarchy",
      "front": "Going Down the Hierarchy",
      "back": "1. **Decreasing cost per bit**\n2. Registers: Very expensive\n3. Cache: Expensive\n4. Main memory: Moderate cost\n5. Disk: Cheap\n6. **Increasing capacity**\n7. Registers: ~32-64 words\n8. Cache: KB to MB\n9. Main memory: GB\n10. Disk: TB\n11. **Increasing access time**\n12. Registers: 1 cycle (nanoseconds)\n13. Cache: 1-10 cycles (nanoseconds)\n14. Main memory: 50-100 cycles (nanoseconds)\n15. Disk: Millions of cycles (milliseconds)\n16. **Decreasing frequency of access**\n17. Most accesses: Registers\n18. Many accesses: Cache\n19. Some accesses: Main memory\n20. Few accesses: Disk",
      "type": "list",
      "section": "The Memory Hierarchy Concept",
      "subsection": "Hierarchy Characteristics"
    },
    {
      "id": "4-The-Memory-Hierarchy-Concept-Hierarchy-Example-Typical-System",
      "title": "Typical System",
      "front": "Typical System",
      "back": "1. **L1 Cache:** 32 KB, 1 cycle access, $100/GB\n2. **L2 Cache:** 256 KB, 5 cycles access, $50/GB\n3. **L3 Cache:** 8 MB, 20 cycles access, $10/GB\n4. **Main Memory:** 16 GB, 100 cycles access, $1/GB\n5. **Disk:** 1 TB, 10,000,000 cycles access, $0.01/GB",
      "type": "list",
      "section": "The Memory Hierarchy Concept",
      "subsection": "Hierarchy Example"
    },
    {
      "id": "4-The-Memory-Hierarchy-Concept-Hierarchy-Example-Access-Distribution",
      "title": "Access Distribution",
      "front": "Access Distribution",
      "back": "1. 95% from L1 cache\n2. 4.999% from L2 cache\n3. 0.001% from L3 cache\n4. 0.000005% from main memory\n5. Even fewer from disk",
      "type": "list",
      "section": "The Memory Hierarchy Concept",
      "subsection": "Hierarchy Example"
    },
    {
      "id": "4-The-Memory-Hierarchy-Concept-Hierarchy-Example-Average-Access-Time-Calculation",
      "title": "Average Access Time Calculation",
      "front": "Average Access Time Calculation",
      "back": "AMAT = 0.95 × 1 + 0.04999 × 5 + 0.00001 × 20 + 0.00000005 × 100 = 0.95 + 0.25 + 0.0002 + 0.000005 ≈ 1.2 cycles",
      "type": "definition",
      "section": "The Memory Hierarchy Concept",
      "subsection": "Hierarchy Example"
    },
    {
      "id": "4-The-Memory-Hierarchy-Concept-Hierarchy-Example-Key-Insight",
      "title": "Key Insight",
      "front": "Key Insight",
      "back": "Average access time is much closer to L1 access time (1 cycle) than main memory (100 cycles)!",
      "type": "definition",
      "section": "The Memory Hierarchy Concept",
      "subsection": "Hierarchy Example"
    },
    {
      "id": "4-Locality-of-Reference-What-is-Locality--Definition",
      "title": "Definition",
      "front": "Definition",
      "back": "During program execution, memory references tend to **cluster**.",
      "type": "definition",
      "section": "Locality of Reference",
      "subsection": "What is Locality?"
    },
    {
      "id": "4-Locality-of-Reference-What-is-Locality--Observation",
      "title": "Observation",
      "front": "Observation",
      "back": "Programs access a **small proportion** of their address space at any given time.",
      "type": "definition",
      "section": "Locality of Reference",
      "subsection": "What is Locality?"
    },
    {
      "id": "4-Locality-of-Reference-Two-Types-of-Locality-1--Temporal-Locality",
      "title": "1. Temporal Locality",
      "front": "1. Temporal Locality",
      "back": "",
      "type": "concept",
      "section": "Locality of Reference",
      "subsection": "Two Types of Locality"
    },
    {
      "id": "4-Locality-of-Reference-Two-Types-of-Locality-Definition",
      "title": "Definition",
      "front": "Definition",
      "back": "Items accessed **recently** are likely to be accessed again **soon**.",
      "type": "definition",
      "section": "Locality of Reference",
      "subsection": "Two Types of Locality"
    },
    {
      "id": "4-Locality-of-Reference-Two-Types-of-Locality-Principle",
      "title": "Principle",
      "front": "Principle",
      "back": "If an item is referenced, it will tend to be referenced again soon.",
      "type": "definition",
      "section": "Locality of Reference",
      "subsection": "Two Types of Locality"
    },
    {
      "id": "4-Locality-of-Reference-Two-Types-of-Locality-Examples",
      "title": "Examples",
      "front": "Examples",
      "back": "1. **Loop instructions:** Same instructions executed repeatedly\n2. **Reused variables:** Variables accessed multiple times\n3. **Function calls:** Same functions called repeatedly",
      "type": "list",
      "section": "Locality of Reference",
      "subsection": "Two Types of Locality"
    },
    {
      "id": "4-Locality-of-Reference-Two-Types-of-Locality-Example",
      "title": "Example",
      "front": "Example",
      "back": "for (i = 0; i < 1000; i++) { sum = sum + array[i]; // 'sum' accessed every iteration",
      "type": "definition",
      "section": "Locality of Reference",
      "subsection": "Two Types of Locality"
    },
    {
      "id": "4-Locality-of-Reference-Two-Types-of-Locality-2--Spatial-Locality",
      "title": "2. Spatial Locality",
      "front": "2. Spatial Locality",
      "back": "",
      "type": "concept",
      "section": "Locality of Reference",
      "subsection": "Two Types of Locality"
    },
    {
      "id": "4-Locality-of-Reference-Two-Types-of-Locality-Definition",
      "title": "Definition",
      "front": "Definition",
      "back": "Items **near** those accessed recently are likely to be accessed soon.",
      "type": "definition",
      "section": "Locality of Reference",
      "subsection": "Two Types of Locality"
    },
    {
      "id": "4-Locality-of-Reference-Two-Types-of-Locality-Principle",
      "title": "Principle",
      "front": "Principle",
      "back": "If an item is referenced, items whose addresses are close by will tend to be referenced soon.",
      "type": "definition",
      "section": "Locality of Reference",
      "subsection": "Two Types of Locality"
    },
    {
      "id": "4-Locality-of-Reference-Two-Types-of-Locality-Examples",
      "title": "Examples",
      "front": "Examples",
      "back": "1. **Sequential instruction access:** Instructions stored sequentially\n2. **Array data:** Array elements stored contiguously\n3. **Stack operations:** Stack grows/shrinks sequentially",
      "type": "list",
      "section": "Locality of Reference",
      "subsection": "Two Types of Locality"
    },
    {
      "id": "4-Locality-of-Reference-Two-Types-of-Locality-Example",
      "title": "Example",
      "front": "Example",
      "back": "for (i = 0; i < 1000; i++) { sum = sum + array[i]; // array[i], array[i+1], array[i+2] accessed sequentially",
      "type": "definition",
      "section": "Locality of Reference",
      "subsection": "Two Types of Locality"
    },
    {
      "id": "4-Locality-of-Reference-Exploiting-Locality-Memory-Hierarchy-Strategy",
      "title": "Memory Hierarchy Strategy",
      "front": "Memory Hierarchy Strategy",
      "back": "1. **Store everything on disk** (cheap, large, slow)\n2. **Copy recently accessed items to main memory:**\n3. Exploits temporal locality (recent items likely needed again)\n4. Main memory faster than disk\n5. **Copy more recently accessed items to cache:**\n6. Exploits temporal locality further\n7. Cache faster than main memory\n8. **Copy nearby items when accessing:**\n9. Exploits spatial locality\n10. When accessing one word, bring in entire block\n11. Adjacent words likely to be accessed soon",
      "type": "list",
      "section": "Locality of Reference",
      "subsection": "Exploiting Locality"
    },
    {
      "id": "4-Locality-of-Reference-Exploiting-Locality-Result",
      "title": "Result",
      "front": "Result",
      "back": "Most accesses satisfied by fast memory (cache), few require slow memory (disk).",
      "type": "definition",
      "section": "Locality of Reference",
      "subsection": "Exploiting Locality"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-What-is-Cache--Definition",
      "title": "Definition",
      "front": "Definition",
      "back": "A small amount of fast memory located between the processor and main memory.",
      "type": "definition",
      "section": "Cache Memory Fundamentals",
      "subsection": "What is Cache?"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-What-is-Cache--Purpose",
      "title": "Purpose",
      "front": "Purpose",
      "back": "1. Store recently accessed data and instructions\n2. Reduce average memory access time\n3. Bridge the speed gap between CPU and main memory",
      "type": "list",
      "section": "Cache Memory Fundamentals",
      "subsection": "What is Cache?"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-What-is-Cache--Characteristics",
      "title": "Characteristics",
      "front": "Characteristics",
      "back": "1. **Small:** Typically KB to MB\n2. **Fast:** 1-10 cycles access time\n3. **Expensive:** High cost per bit\n4. **On-chip:** Often located on CPU chip",
      "type": "list",
      "section": "Cache Memory Fundamentals",
      "subsection": "What is Cache?"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Cache-Operation-Overview-Read-Operation-",
      "title": "Read Operation:",
      "front": "Read Operation:",
      "back": "1. **CPU requests** contents of memory location\n2. **Check cache** for data\n3. **If present (Hit):**\n4. Get data from cache (fast)\n5. Deliver to CPU\n6. **If not present (Miss):**\n7. Read required **block** from main memory\n8. Load block into cache\n9. Deliver requested word to CPU",
      "type": "list",
      "section": "Cache Memory Fundamentals",
      "subsection": "Cache Operation Overview"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Cache-Operation-Overview-Write-Operation-",
      "title": "Write Operation:",
      "front": "Write Operation:",
      "back": "1. **CPU writes** data to memory location\n2. **Check cache** for location\n3. **If present (Hit):**\n4. Update cache\n5. May update main memory (depending on write policy)\n6. **If not present (Miss):**\n7. Load block into cache\n8. Update cache\n9. May update main memory",
      "type": "list",
      "section": "Cache Memory Fundamentals",
      "subsection": "Cache Operation Overview"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Block--Line-",
      "title": "Block (Line)",
      "front": "Block (Line)",
      "back": "",
      "type": "concept",
      "section": "Cache Memory Fundamentals",
      "subsection": "Key Definitions"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Definition",
      "title": "Definition",
      "front": "Definition",
      "back": "Unit of data transfer between cache and main memory.",
      "type": "definition",
      "section": "Cache Memory Fundamentals",
      "subsection": "Key Definitions"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Characteristics",
      "title": "Characteristics",
      "front": "Characteristics",
      "back": "1. May be multiple words\n2. Typically 16-128 bytes\n3. When one word is accessed, entire block is brought into cache\n4. Exploits spatial locality\n5. Reduces number of memory accesses\n6. More efficient than word-by-word transfer",
      "type": "list",
      "section": "Cache Memory Fundamentals",
      "subsection": "Key Definitions"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Hit",
      "title": "Hit",
      "front": "Hit",
      "back": "",
      "type": "concept",
      "section": "Cache Memory Fundamentals",
      "subsection": "Key Definitions"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Definition",
      "title": "Definition",
      "front": "Definition",
      "back": "Access satisfied by upper level (cache).",
      "type": "definition",
      "section": "Cache Memory Fundamentals",
      "subsection": "Key Definitions"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Hit-Ratio",
      "title": "Hit Ratio",
      "front": "Hit Ratio",
      "back": "`hits / total_accesses`",
      "type": "definition",
      "section": "Cache Memory Fundamentals",
      "subsection": "Key Definitions"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Example",
      "title": "Example",
      "front": "Example",
      "back": "1. 1000 memory accesses\n2. 950 satisfied by cache\n3. Hit ratio = 950/1000 = 0.95 = 95%",
      "type": "list",
      "section": "Cache Memory Fundamentals",
      "subsection": "Key Definitions"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Miss",
      "title": "Miss",
      "front": "Miss",
      "back": "",
      "type": "concept",
      "section": "Cache Memory Fundamentals",
      "subsection": "Key Definitions"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Definition",
      "title": "Definition",
      "front": "Definition",
      "back": "Block not present in cache, must be copied from lower level (main memory).",
      "type": "definition",
      "section": "Cache Memory Fundamentals",
      "subsection": "Key Definitions"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Miss-Ratio",
      "title": "Miss Ratio",
      "front": "Miss Ratio",
      "back": "`misses / total_accesses = 1 - hit_ratio`",
      "type": "definition",
      "section": "Cache Memory Fundamentals",
      "subsection": "Key Definitions"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Example",
      "title": "Example",
      "front": "Example",
      "back": "1. Hit ratio = 95%\n2. Miss ratio = 5%",
      "type": "list",
      "section": "Cache Memory Fundamentals",
      "subsection": "Key Definitions"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Miss-Penalty",
      "title": "Miss Penalty",
      "front": "Miss Penalty",
      "back": "",
      "type": "concept",
      "section": "Cache Memory Fundamentals",
      "subsection": "Key Definitions"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Definition",
      "title": "Definition",
      "front": "Definition",
      "back": "Time taken to handle a miss.",
      "type": "definition",
      "section": "Cache Memory Fundamentals",
      "subsection": "Key Definitions"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Components",
      "title": "Components",
      "front": "Components",
      "back": "1. Time to access main memory\n2. Time to transfer block to cache\n3. Time to deliver data to CPU",
      "type": "list",
      "section": "Cache Memory Fundamentals",
      "subsection": "Key Definitions"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Key-Definitions-Typical",
      "title": "Typical",
      "front": "Typical",
      "back": "10-100+ cycles",
      "type": "definition",
      "section": "Cache Memory Fundamentals",
      "subsection": "Key Definitions"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Cache-Organization-Structure",
      "title": "Structure",
      "front": "Structure",
      "back": "Cache Line: ┌──────┬──────┬──────────┐ │ Valid│ Tag │ Data │ │ Bit │ │ (Block) │ └──────┴──────┴──────────┘",
      "type": "definition",
      "section": "Cache Memory Fundamentals",
      "subsection": "Cache Organization"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Cache-Organization-Components",
      "title": "Components",
      "front": "Components",
      "back": "1. **Valid Bit:** Indicates if line contains valid data\n2. **Tag:** Identifies which memory block is stored\n3. **Data:** The actual data block",
      "type": "list",
      "section": "Cache Memory Fundamentals",
      "subsection": "Cache Organization"
    },
    {
      "id": "4-Cache-Memory-Fundamentals-Cache-Organization-Example",
      "title": "Example",
      "front": "Example",
      "back": "Line 0: [V=1] [Tag=0x1234] [Data: word0, word1, word2, word3] Line 1: [V=0] [Tag=----] [Data: ----] Line 2: [V=1] [Tag=0x5678] [Data: word0, word1, word2, word3]",
      "type": "definition",
      "section": "Cache Memory Fundamentals",
      "subsection": "Cache Organization"
    },
    {
      "id": "4-Cache-Mapping-Techniques-The-Mapping-Problem-Problem",
      "title": "Problem",
      "front": "Problem",
      "back": "There are fewer cache lines than main memory blocks.",
      "type": "definition",
      "section": "Cache Mapping Techniques",
      "subsection": "The Mapping Problem"
    },
    {
      "id": "4-Cache-Mapping-Techniques-The-Mapping-Problem-Question",
      "title": "Question",
      "front": "Question",
      "back": "How do we map main memory blocks to cache lines?",
      "type": "definition",
      "section": "Cache Mapping Techniques",
      "subsection": "The Mapping Problem"
    },
    {
      "id": "4-Cache-Mapping-Techniques-The-Mapping-Problem-Example",
      "title": "Example",
      "front": "Example",
      "back": "1. Main memory: 1 million blocks\n2. Cache: 1,000 lines\n3. Each block must map to one or more possible cache lines",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "The Mapping Problem"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-1--Direct-Mapping",
      "title": "1. Direct Mapping",
      "front": "1. Direct Mapping",
      "back": "",
      "type": "concept",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Concept",
      "title": "Concept",
      "front": "Concept",
      "back": "Each block of main memory maps to **exactly one** cache line.",
      "type": "definition",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Mapping-Formula",
      "title": "Mapping Formula",
      "front": "Mapping Formula",
      "back": "1. `i` = cache line number\n2. `j` = main memory block number\n3. `m` = number of lines in cache",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Address-Structure",
      "title": "Address Structure",
      "front": "Address Structure",
      "back": "┌──────────┬──────┬──────┐ │ Tag │ Line │ Word │ │ (s-r bits)│(r bits)│(w bits)│ └──────────┴──────┴──────┘",
      "type": "definition",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Fields",
      "title": "Fields",
      "front": "Fields",
      "back": "1. **Tag:** High-order bits identifying which block\n2. **Line:** Cache line number (low-order bits of block address)\n3. **Word:** Word offset within block",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Example",
      "title": "Example",
      "front": "Example",
      "back": "1. Cache: 8 lines (3 bits for line number)\n2. Block size: 1 word (0 bits for word offset)\n3. Address: 22 (binary: 10110)",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Breaking-down-address-22",
      "title": "Breaking down address 22",
      "front": "Breaking down address 22",
      "back": "Binary: 1 0 1 1 0 │ │ └─► Line = 110 (binary) = 6 └─┴──────► Tag = 10 (binary) = 2",
      "type": "definition",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Operation",
      "title": "Operation",
      "front": "Operation",
      "back": "1. Extract line number from address\n2. Check if valid bit is set\n3. Compare tag in cache with tag from address\n4. If match: **Hit** (data in cache)\n5. If no match: **Miss** (load block from memory)",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Advantages",
      "title": "Advantages",
      "front": "Advantages",
      "back": "1. **Simple:** Easy to implement\n2. **Fast:** Direct lookup (no search needed)\n3. **Inexpensive:** Minimal hardware",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Disadvantages",
      "title": "Disadvantages",
      "front": "Disadvantages",
      "back": "1. **Fixed location:** Any given block can only be in one specific line\n2. **Thrashing:** If program accesses blocks that map to same line repeatedly, constant misses\n3. **Low flexibility:** No choice in placement",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Thrashing-Example",
      "title": "Thrashing Example",
      "front": "Thrashing Example",
      "back": "Blocks 0, 8, 16, 24 all map to line 0 (0 mod 8 = 0, 8 mod 8 = 0, etc.) Accessing: 0, 8, 0, 8, 0, 8... Result: Constant misses (each access evicts previous block)",
      "type": "definition",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-2--Fully-Associative-Mapping",
      "title": "2. Fully Associative Mapping",
      "front": "2. Fully Associative Mapping",
      "back": "",
      "type": "concept",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Concept",
      "title": "Concept",
      "front": "Concept",
      "back": "A main memory block can load into **any line** of cache.",
      "type": "definition",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Address-Structure",
      "title": "Address Structure",
      "front": "Address Structure",
      "back": "┌──────────┬──────┐ │ Tag │ Word │ │ (s bits) │(w bits)│ └──────────┴──────┘",
      "type": "definition",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Fields",
      "title": "Fields",
      "front": "Fields",
      "back": "1. **Tag:** Full block address (no line field needed)\n2. **Word:** Word offset within block",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Operation",
      "title": "Operation",
      "front": "Operation",
      "back": "1. Extract tag from address\n2. **Search all cache lines** for matching tag\n3. If found: **Hit**\n4. If not found: **Miss** (load into any available line)",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Advantages",
      "title": "Advantages",
      "front": "Advantages",
      "back": "1. **Maximum flexibility:** Block can be placed anywhere\n2. **No thrashing:** No conflicts between blocks\n3. **Best hit ratio:** Optimal placement possible",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Disadvantages",
      "title": "Disadvantages",
      "front": "Disadvantages",
      "back": "1. **Expensive:** Requires comparators for all lines\n2. **Slow:** Must search all lines (parallel search needed for speed)\n3. **Complex:** More hardware complexity",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Hardware-Requirements",
      "title": "Hardware Requirements",
      "front": "Hardware Requirements",
      "back": "1. N comparators (one per cache line)\n2. Parallel tag comparison\n3. More expensive as cache size increases",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-3--Set-Associative-Mapping",
      "title": "3. Set-Associative Mapping",
      "front": "3. Set-Associative Mapping",
      "back": "",
      "type": "concept",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Concept",
      "title": "Concept",
      "front": "Concept",
      "back": "Compromise between direct and fully associative.",
      "type": "definition",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Organization",
      "title": "Organization",
      "front": "Organization",
      "back": "1. Cache divided into **sets**\n2. Each set contains **k lines** (k-way set associative)\n3. Block maps to **one specific set**, but can be placed in **any line within that set**",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Mapping",
      "title": "Mapping",
      "front": "Mapping",
      "back": "1. Set number: `(Block number) mod (Number of sets)`\n2. Within set: Any of k lines",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Address-Structure",
      "title": "Address Structure",
      "front": "Address Structure",
      "back": "┌──────────┬──────┬──────┐ │ Tag │ Set │ Word │ │(s-d bits)│(d bits)│(w bits)│ └──────────┴──────┴──────┘",
      "type": "definition",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Fields",
      "title": "Fields",
      "front": "Fields",
      "back": "1. **Tag:** Identifies block within set\n2. **Set:** Set number (determines which set)\n3. **Word:** Word offset within block",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Example---2-Way-Set-Associative",
      "title": "Example - 2-Way Set Associative",
      "front": "Example - 2-Way Set Associative",
      "back": "1. Cache: 8 lines total\n2. Sets: 4 sets (2 lines per set)\n3. Block 12: 12 mod 4 = 0 → Set 0, can be in either line of Set 0",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Operation",
      "title": "Operation",
      "front": "Operation",
      "back": "1. Extract set number from address\n2. Search **only lines in that set** for matching tag\n3. If found: **Hit**\n4. If not found: **Miss** (load into any available line in set)",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Advantages",
      "title": "Advantages",
      "front": "Advantages",
      "back": "1. **Good flexibility:** Multiple choices per block\n2. **Reasonable cost:** Only k comparators needed (not all lines)\n3. **Better than direct:** Reduces thrashing\n4. **Better than fully associative:** Lower cost, faster",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Disadvantages",
      "title": "Disadvantages",
      "front": "Disadvantages",
      "back": "1. **More complex than direct:** Requires set selection and search\n2. **More expensive than direct:** Needs k comparators\n3. **Less flexible than fully associative:** Limited to k choices",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Three-Mapping-Techniques-Common-Configurations",
      "title": "Common Configurations",
      "front": "Common Configurations",
      "back": "1. **2-way:** 2 lines per set (common, good balance)\n2. **4-way:** 4 lines per set (very common)\n3. **8-way:** 8 lines per set (high-end processors)",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Three Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Comparison-of-Mapping-Techniques-Modern-Practice",
      "title": "Modern Practice",
      "front": "Modern Practice",
      "back": "Most processors use **set-associative** (typically 2-8 way) for good balance.",
      "type": "definition",
      "section": "Cache Mapping Techniques",
      "subsection": "Comparison of Mapping Techniques"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Associativity-Spectrum-For-a-cache-with-8-entries",
      "title": "For a cache with 8 entries",
      "front": "For a cache with 8 entries",
      "back": "1. **Direct (1-way):** 8 sets, 1 line per set\n2. **2-way:** 4 sets, 2 lines per set\n3. **4-way:** 2 sets, 4 lines per set\n4. **8-way (Fully Associative):** 1 set, 8 lines per set",
      "type": "list",
      "section": "Cache Mapping Techniques",
      "subsection": "Associativity Spectrum"
    },
    {
      "id": "4-Cache-Mapping-Techniques-Associativity-Spectrum-Key-Insight",
      "title": "Key Insight",
      "front": "Key Insight",
      "back": "Direct mapping and fully associative are special cases of set-associative!",
      "type": "definition",
      "section": "Cache Mapping Techniques",
      "subsection": "Associativity Spectrum"
    },
    {
      "id": "4-Cache-Replacement-Policies-When-Replacement-is-Needed-Situation",
      "title": "Situation",
      "front": "Situation",
      "back": "Cache is full, new block must be loaded.",
      "type": "definition",
      "section": "Cache Replacement Policies",
      "subsection": "When Replacement is Needed"
    },
    {
      "id": "4-Cache-Replacement-Policies-When-Replacement-is-Needed-Direct-Mapping",
      "title": "Direct Mapping",
      "front": "Direct Mapping",
      "back": "1. **No choice:** Only one possible line\n2. Replacement is automatic",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "When Replacement is Needed"
    },
    {
      "id": "4-Cache-Replacement-Policies-When-Replacement-is-Needed-Associative-Set-Associative",
      "title": "Associative/Set-Associative",
      "front": "Associative/Set-Associative",
      "back": "1. **Choice available:** Which line to replace?\n2. Need **replacement algorithm**",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "When Replacement is Needed"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-1--Least-Recently-Used--LRU-",
      "title": "1. Least Recently Used (LRU)",
      "front": "1. Least Recently Used (LRU)",
      "back": "",
      "type": "concept",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Principle",
      "title": "Principle",
      "front": "Principle",
      "back": "Replace the block that has been in cache **longest without reference**.",
      "type": "definition",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Implementation",
      "title": "Implementation",
      "front": "Implementation",
      "back": "1. Track access order for each set\n2. Replace least recently accessed block",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Advantages",
      "title": "Advantages",
      "front": "Advantages",
      "back": "1. **Most effective:** Exploits temporal locality\n2. **Good hit ratio:** Keeps recently used blocks",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Disadvantages",
      "title": "Disadvantages",
      "front": "Disadvantages",
      "back": "1. **Complexity:** Requires tracking access history\n2. **Hardware cost:** Counters or state machines needed",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Example",
      "title": "Example",
      "front": "Example",
      "back": "Set with blocks: A, B, C Access order: A, B, A, C, B Next miss: Replace C (least recently used)",
      "type": "definition",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-2--First-In-First-Out--FIFO-",
      "title": "2. First-In-First-Out (FIFO)",
      "front": "2. First-In-First-Out (FIFO)",
      "back": "",
      "type": "concept",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Principle",
      "title": "Principle",
      "front": "Principle",
      "back": "Replace the block that has been in cache **longest** (regardless of recent use).",
      "type": "definition",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Implementation",
      "title": "Implementation",
      "front": "Implementation",
      "back": "1. Round-robin or circular buffer\n2. Replace oldest block",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Advantages",
      "title": "Advantages",
      "front": "Advantages",
      "back": "1. **Simple:** Easy to implement\n2. **Low cost:** Minimal hardware",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Disadvantages",
      "title": "Disadvantages",
      "front": "Disadvantages",
      "back": "1. **Less effective:** Doesn't consider recent usage\n2. **May evict frequently used blocks**",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Example",
      "title": "Example",
      "front": "Example",
      "back": "Blocks loaded: A, B, C (in that order) Next miss: Replace A (first in)",
      "type": "definition",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-3--Least-Frequently-Used--LFU-",
      "title": "3. Least Frequently Used (LFU)",
      "front": "3. Least Frequently Used (LFU)",
      "back": "",
      "type": "concept",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Principle",
      "title": "Principle",
      "front": "Principle",
      "back": "Replace the block with **fewest references**.",
      "type": "definition",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Implementation",
      "title": "Implementation",
      "front": "Implementation",
      "back": "1. Counter for each block\n2. Increment on access\n3. Replace block with lowest count",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Advantages",
      "title": "Advantages",
      "front": "Advantages",
      "back": "1. **Considers usage frequency:** Keeps frequently used blocks",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Disadvantages",
      "title": "Disadvantages",
      "front": "Disadvantages",
      "back": "1. **Complexity:** Counters needed\n2. **May keep old blocks:** Blocks accessed many times long ago",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Example",
      "title": "Example",
      "front": "Example",
      "back": "Block A: 10 accesses Block B: 5 accesses Block C: 2 accesses Next miss: Replace C (least frequently used)",
      "type": "definition",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-4--Random",
      "title": "4. Random",
      "front": "4. Random",
      "back": "",
      "type": "concept",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Principle",
      "title": "Principle",
      "front": "Principle",
      "back": "Replace a **randomly selected** block.",
      "type": "definition",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Implementation",
      "title": "Implementation",
      "front": "Implementation",
      "back": "1. Random number generator\n2. Select random line in set",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Advantages",
      "title": "Advantages",
      "front": "Advantages",
      "back": "1. **Very simple:** Minimal hardware\n2. **No tracking needed**",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Disadvantages",
      "title": "Disadvantages",
      "front": "Disadvantages",
      "back": "1. **Poor performance:** No locality consideration\n2. **Unpredictable:** May evict important blocks",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Replacement-Algorithms-Use",
      "title": "Use",
      "front": "Use",
      "back": "Rarely used, mainly for comparison",
      "type": "definition",
      "section": "Cache Replacement Policies",
      "subsection": "Replacement Algorithms"
    },
    {
      "id": "4-Cache-Replacement-Policies-Algorithm-Comparison-Effectiveness--Best-to-Worst-",
      "title": "Effectiveness (Best to Worst)",
      "front": "Effectiveness (Best to Worst)",
      "back": "1. LRU (most effective)\n2. LFU\n3. FIFO\n4. Random (least effective)",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "Algorithm Comparison"
    },
    {
      "id": "4-Cache-Replacement-Policies-Algorithm-Comparison-Complexity--Simplest-to-Most-Complex-",
      "title": "Complexity (Simplest to Most Complex)",
      "front": "Complexity (Simplest to Most Complex)",
      "back": "1. Random (simplest)\n2. FIFO\n3. LRU\n4. LFU (most complex)",
      "type": "list",
      "section": "Cache Replacement Policies",
      "subsection": "Algorithm Comparison"
    },
    {
      "id": "4-Cache-Replacement-Policies-Algorithm-Comparison-Modern-Practice",
      "title": "Modern Practice",
      "front": "Modern Practice",
      "back": "**LRU** is most popular due to good effectiveness and reasonable implementation cost.",
      "type": "definition",
      "section": "Cache Replacement Policies",
      "subsection": "Algorithm Comparison"
    },
    {
      "id": "4-Write-Policies-The-Write-Problem-Issue",
      "title": "Issue",
      "front": "Issue",
      "back": "When CPU writes to cache, main memory must eventually be updated.",
      "type": "definition",
      "section": "Write Policies",
      "subsection": "The Write Problem"
    },
    {
      "id": "4-Write-Policies-The-Write-Problem-Questions",
      "title": "Questions",
      "front": "Questions",
      "back": "1. When should main memory be updated?\n2. What if cache block is replaced before being written to memory?\n3. What if multiple devices access main memory?",
      "type": "list",
      "section": "Write Policies",
      "subsection": "The Write Problem"
    },
    {
      "id": "4-Write-Policies-Two-Write-Policies-1--Write-Through",
      "title": "1. Write Through",
      "front": "1. Write Through",
      "back": "",
      "type": "concept",
      "section": "Write Policies",
      "subsection": "Two Write Policies"
    },
    {
      "id": "4-Write-Policies-Two-Write-Policies-Principle",
      "title": "Principle",
      "front": "Principle",
      "back": "Every write to cache **also writes to main memory** immediately.",
      "type": "definition",
      "section": "Write Policies",
      "subsection": "Two Write Policies"
    },
    {
      "id": "4-Write-Policies-Two-Write-Policies-Operation",
      "title": "Operation",
      "front": "Operation",
      "back": "CPU Write → Update Cache → Update Main Memory (simultaneously)",
      "type": "definition",
      "section": "Write Policies",
      "subsection": "Two Write Policies"
    },
    {
      "id": "4-Write-Policies-Two-Write-Policies-Advantages",
      "title": "Advantages",
      "front": "Advantages",
      "back": "1. **Simple:** Straightforward implementation\n2. **Consistency:** Cache and memory always consistent\n3. **I/O compatibility:** I/O devices can read directly from memory",
      "type": "list",
      "section": "Write Policies",
      "subsection": "Two Write Policies"
    },
    {
      "id": "4-Write-Policies-Two-Write-Policies-Disadvantages",
      "title": "Disadvantages",
      "front": "Disadvantages",
      "back": "1. **High memory traffic:** Every write goes to memory\n2. **Slow writes:** Memory access is slow\n3. **Bottleneck:** Memory bus becomes bottleneck",
      "type": "list",
      "section": "Write Policies",
      "subsection": "Two Write Policies"
    },
    {
      "id": "4-Write-Policies-Two-Write-Policies-Performance-Impact",
      "title": "Performance Impact",
      "front": "Performance Impact",
      "back": "1. Hold data waiting to be written\n2. CPU continues immediately\n3. Only stalls if buffer is full\n4. Reduces performance penalty",
      "type": "list",
      "section": "Write Policies",
      "subsection": "Two Write Policies"
    },
    {
      "id": "4-Write-Policies-Two-Write-Policies-2--Write-Back",
      "title": "2. Write Back",
      "front": "2. Write Back",
      "back": "",
      "type": "concept",
      "section": "Write Policies",
      "subsection": "Two Write Policies"
    },
    {
      "id": "4-Write-Policies-Two-Write-Policies-Principle",
      "title": "Principle",
      "front": "Principle",
      "back": "Write only to cache initially. Write to memory only when block is replaced.",
      "type": "definition",
      "section": "Write Policies",
      "subsection": "Two Write Policies"
    },
    {
      "id": "4-Write-Policies-Two-Write-Policies-Operation",
      "title": "Operation",
      "front": "Operation",
      "back": "CPU Write → Update Cache (only) When block replaced → Write to memory (if dirty)",
      "type": "definition",
      "section": "Write Policies",
      "subsection": "Two Write Policies"
    },
    {
      "id": "4-Write-Policies-Two-Write-Policies-Dirty-Bit",
      "title": "Dirty Bit",
      "front": "Dirty Bit",
      "back": "1. Indicates if block has been modified\n2. Set when block is written\n3. Checked when block is replaced\n4. If dirty: Write to memory before replacement",
      "type": "list",
      "section": "Write Policies",
      "subsection": "Two Write Policies"
    },
    {
      "id": "4-Write-Policies-Two-Write-Policies-Advantages",
      "title": "Advantages",
      "front": "Advantages",
      "back": "1. **Minimizes memory writes:** Only dirty blocks written\n2. **Faster writes:** No memory access during write\n3. **Better performance:** Lower memory traffic",
      "type": "list",
      "section": "Write Policies",
      "subsection": "Two Write Policies"
    },
    {
      "id": "4-Write-Policies-Two-Write-Policies-Disadvantages",
      "title": "Disadvantages",
      "front": "Disadvantages",
      "back": "1. **Complexity:** Need dirty bit tracking\n2. **Inconsistency:** Cache and memory may differ\n3. **I/O issues:** I/O must go through cache or use cache coherency",
      "type": "list",
      "section": "Write Policies",
      "subsection": "Two Write Policies"
    },
    {
      "id": "4-Write-Policies-Two-Write-Policies-Performance-Impact",
      "title": "Performance Impact",
      "front": "Performance Impact",
      "back": "Base CPI = 1 10% of instructions are stores Only 20% of replaced blocks are dirty Memory write takes 100 cycles Miss rate = 2% Effective CPI = 1 + 0.02 × 0.2 × 100 = 1.4 (Much better than write through!)",
      "type": "definition",
      "section": "Write Policies",
      "subsection": "Two Write Policies"
    },
    {
      "id": "4-Write-Policies-Write-Allocation-Question",
      "title": "Question",
      "front": "Question",
      "back": "On write miss, should we load block into cache?",
      "type": "definition",
      "section": "Write Policies",
      "subsection": "Write Allocation"
    },
    {
      "id": "4-Write-Policies-Write-Allocation-Write-Allocate--Fetch-on-Write-Miss-",
      "title": "Write Allocate (Fetch on Write Miss)",
      "front": "Write Allocate (Fetch on Write Miss)",
      "back": "1. Load block into cache\n2. Update cache\n3. Use with write back",
      "type": "list",
      "section": "Write Policies",
      "subsection": "Write Allocation"
    },
    {
      "id": "4-Write-Policies-Write-Allocation-No-Write-Allocate--Write-Around-",
      "title": "No Write Allocate (Write Around)",
      "front": "No Write Allocate (Write Around)",
      "back": "1. Write directly to memory\n2. Don't load into cache\n3. Use with write through",
      "type": "list",
      "section": "Write Policies",
      "subsection": "Write Allocation"
    },
    {
      "id": "4-Write-Policies-Write-Allocation-Modern-Practice",
      "title": "Modern Practice",
      "front": "Modern Practice",
      "back": "1. **Write back + Write allocate:** Most common\n2. **Write through + No write allocate:** Less common",
      "type": "list",
      "section": "Write Policies",
      "subsection": "Write Allocation"
    },
    {
      "id": "4-Write-Policies-Cache-Coherency-Problem",
      "title": "Problem",
      "front": "Problem",
      "back": "Multiple devices may access same memory.",
      "type": "definition",
      "section": "Write Policies",
      "subsection": "Cache Coherency"
    },
    {
      "id": "4-Write-Policies-Cache-Coherency-Scenarios",
      "title": "Scenarios",
      "front": "Scenarios",
      "back": "1. **I/O and CPU:** I/O writes to memory, cache has stale data\n2. **Multiple CPUs:** Each has own cache, one CPU writes, others have stale data",
      "type": "list",
      "section": "Write Policies",
      "subsection": "Cache Coherency"
    },
    {
      "id": "4-Write-Policies-Cache-Coherency-Solutions",
      "title": "Solutions",
      "front": "Solutions",
      "back": "1. **Snooping:** Caches monitor bus for writes\n2. **Invalidation:** Mark cache lines as invalid when written by others\n3. **Update:** Update cache lines when written by others\n4. **Cache coherency protocols:** MESI (Modified, Exclusive, Shared, Invalid)",
      "type": "list",
      "section": "Write Policies",
      "subsection": "Cache Coherency"
    },
    {
      "id": "4-Cache-Performance-Analysis-Performance-Metrics-Average-Memory-Access-Time--AMAT-",
      "title": "Average Memory Access Time (AMAT)",
      "front": "Average Memory Access Time (AMAT)",
      "back": "",
      "type": "concept",
      "section": "Cache Performance Analysis",
      "subsection": "Performance Metrics"
    },
    {
      "id": "4-Cache-Performance-Analysis-Performance-Metrics-Formula",
      "title": "Formula",
      "front": "Formula",
      "back": "AMAT = Hit Time + (Miss Rate × Miss Penalty)",
      "type": "definition",
      "section": "Cache Performance Analysis",
      "subsection": "Performance Metrics"
    },
    {
      "id": "4-Cache-Performance-Analysis-Performance-Metrics-Components",
      "title": "Components",
      "front": "Components",
      "back": "1. **Hit Time:** Time to access cache (typically 1-10 cycles)\n2. **Miss Rate:** Fraction of accesses that miss (0.0 to 1.0)\n3. **Miss Penalty:** Time to handle miss (typically 10-100+ cycles)",
      "type": "list",
      "section": "Cache Performance Analysis",
      "subsection": "Performance Metrics"
    },
    {
      "id": "4-Cache-Performance-Analysis-Performance-Metrics-Example",
      "title": "Example",
      "front": "Example",
      "back": "Hit time = 1 cycle Miss rate = 5% = 0.05 Miss penalty = 100 cycles AMAT = 1 + 0.05 × 100 = 1 + 5 = 6 cycles",
      "type": "definition",
      "section": "Cache Performance Analysis",
      "subsection": "Performance Metrics"
    },
    {
      "id": "4-Cache-Performance-Analysis-Performance-Metrics-Key-Insight",
      "title": "Key Insight",
      "front": "Key Insight",
      "back": "Even with 5% miss rate, average access time is 6x hit time!",
      "type": "definition",
      "section": "Cache Performance Analysis",
      "subsection": "Performance Metrics"
    },
    {
      "id": "4-Cache-Performance-Analysis-Performance-Metrics-CPU-Time",
      "title": "CPU Time",
      "front": "CPU Time",
      "back": "",
      "type": "concept",
      "section": "Cache Performance Analysis",
      "subsection": "Performance Metrics"
    },
    {
      "id": "4-Cache-Performance-Analysis-Performance-Metrics-Formula",
      "title": "Formula",
      "front": "Formula",
      "back": "CPU Time = (CPU execution cycles + Memory stall cycles) × Clock cycle time",
      "type": "definition",
      "section": "Cache Performance Analysis",
      "subsection": "Performance Metrics"
    },
    {
      "id": "4-Cache-Performance-Analysis-Performance-Metrics-Memory-Stall-Cycles",
      "title": "Memory Stall Cycles",
      "front": "Memory Stall Cycles",
      "back": "Memory stall cycles = (Instruction miss cycles) + (Data miss cycles)",
      "type": "definition",
      "section": "Cache Performance Analysis",
      "subsection": "Performance Metrics"
    },
    {
      "id": "4-Cache-Performance-Analysis-Performance-Metrics-Instruction-Miss-Cycles",
      "title": "Instruction Miss Cycles",
      "front": "Instruction Miss Cycles",
      "back": "Instruction miss cycles = (Instructions) × (I-cache miss rate) × (Miss penalty)",
      "type": "definition",
      "section": "Cache Performance Analysis",
      "subsection": "Performance Metrics"
    },
    {
      "id": "4-Cache-Performance-Analysis-Performance-Metrics-Data-Miss-Cycles",
      "title": "Data Miss Cycles",
      "front": "Data Miss Cycles",
      "back": "Data miss cycles = (Instructions) × (Load/store fraction) × (D-cache miss rate) × (Miss penalty)",
      "type": "definition",
      "section": "Cache Performance Analysis",
      "subsection": "Performance Metrics"
    },
    {
      "id": "4-Cache-Performance-Analysis-Performance-Metrics-Effective-CPI",
      "title": "Effective CPI",
      "front": "Effective CPI",
      "back": "",
      "type": "concept",
      "section": "Cache Performance Analysis",
      "subsection": "Performance Metrics"
    },
    {
      "id": "4-Cache-Performance-Analysis-Performance-Metrics-Formula",
      "title": "Formula",
      "front": "Formula",
      "back": "CPI_actual = CPI_base + (Memory stall cycles per instruction)",
      "type": "definition",
      "section": "Cache Performance Analysis",
      "subsection": "Performance Metrics"
    },
    {
      "id": "4-Cache-Performance-Analysis-Performance-Metrics-Memory-Stall-Cycles-per-Instruction",
      "title": "Memory Stall Cycles per Instruction",
      "front": "Memory Stall Cycles per Instruction",
      "back": "Stall cycles = (I-cache miss rate × Miss penalty) + (Load/store fraction × D-cache miss rate × Miss penalty)",
      "type": "definition",
      "section": "Cache Performance Analysis",
      "subsection": "Performance Metrics"
    },
    {
      "id": "4-Cache-Performance-Analysis-Performance-Metrics-Example",
      "title": "Example",
      "front": "Example",
      "back": "Base CPI = 3 I-cache miss rate = 4% = 0.04 D-cache miss rate = 8% = 0.08 Load/store fraction = 60% = 0.6 Miss penalty = 125 cycles CPI_actual = 3 + (0.04 × 125) + (0.6 × 0.08 × 125) = 3 + 5 + 6 = 14 cycles",
      "type": "definition",
      "section": "Cache Performance Analysis",
      "subsection": "Performance Metrics"
    },
    {
      "id": "4-Cache-Performance-Analysis-Performance-Metrics-Performance-Impact",
      "title": "Performance Impact",
      "front": "Performance Impact",
      "back": "Cache misses increase CPI from 3 to 14 (4.7x slower)!",
      "type": "definition",
      "section": "Cache Performance Analysis",
      "subsection": "Performance Metrics"
    },
    {
      "id": "4-Cache-Performance-Analysis-Improving-Cache-Performance-Strategies",
      "title": "Strategies",
      "front": "Strategies",
      "back": "1. **Reduce Miss Rate:**\n2. Larger cache\n3. Higher associativity\n4. Better replacement algorithm\n5. Larger block size (up to a point)\n6. **Reduce Miss Penalty:**\n7. Faster main memory\n8. Multi-level caches\n9. Write buffers\n10. **Reduce Hit Time:**\n11. Smaller cache\n12. Lower associativity\n13. On-chip cache",
      "type": "list",
      "section": "Cache Performance Analysis",
      "subsection": "Improving Cache Performance"
    },
    {
      "id": "4-Cache-Performance-Analysis-Improving-Cache-Performance-Trade-offs",
      "title": "Trade-offs",
      "front": "Trade-offs",
      "back": "1. Larger cache → Lower miss rate, but higher hit time\n2. Higher associativity → Lower miss rate, but higher hit time\n3. Larger blocks → Better spatial locality, but fewer blocks fit",
      "type": "list",
      "section": "Cache Performance Analysis",
      "subsection": "Improving Cache Performance"
    },
    {
      "id": "4-Multi-Level-Caches-Why-Multiple-Levels--Problem",
      "title": "Problem",
      "front": "Problem",
      "back": "1. **Size:** Large enough for good hit rate\n2. **Speed:** Small enough for fast access",
      "type": "list",
      "section": "Multi-Level Caches",
      "subsection": "Why Multiple Levels?"
    },
    {
      "id": "4-Multi-Level-Caches-Why-Multiple-Levels--Solution",
      "title": "Solution",
      "front": "Solution",
      "back": "Use **multiple cache levels** with different characteristics.",
      "type": "definition",
      "section": "Multi-Level Caches",
      "subsection": "Why Multiple Levels?"
    },
    {
      "id": "4-Multi-Level-Caches-Two-Level-Cache-Organization-L1-Cache--Level-1-",
      "title": "L1 Cache (Level 1)",
      "front": "L1 Cache (Level 1)",
      "back": "1. **Location:** On CPU chip\n2. **Size:** Small (8-64 KB)\n3. **Speed:** Very fast (1-2 cycles)\n4. **Characteristics:**\n5. Split instruction and data caches\n6. Very close to processor\n7. Fastest access",
      "type": "list",
      "section": "Multi-Level Caches",
      "subsection": "Two-Level Cache Organization"
    },
    {
      "id": "4-Multi-Level-Caches-Two-Level-Cache-Organization-L2-Cache--Level-2-",
      "title": "L2 Cache (Level 2)",
      "front": "L2 Cache (Level 2)",
      "back": "1. **Location:** On CPU chip or off-chip\n2. **Size:** Medium (256 KB - 8 MB)\n3. **Speed:** Fast (5-20 cycles)\n4. **Characteristics:**\n5. Services misses from L1\n6. Larger than L1\n7. Slower than L1, but faster than main memory",
      "type": "list",
      "section": "Multi-Level Caches",
      "subsection": "Two-Level Cache Organization"
    },
    {
      "id": "4-Multi-Level-Caches-Two-Level-Cache-Organization-L3-Cache--Level-3-",
      "title": "L3 Cache (Level 3)",
      "front": "L3 Cache (Level 3)",
      "back": "1. **Location:** On CPU chip or off-chip\n2. **Size:** Large (8-64 MB)\n3. **Speed:** Moderate (20-50 cycles)\n4. **Characteristics:**\n5. Services misses from L2\n6. Shared among multiple cores\n7. Larger than L2",
      "type": "list",
      "section": "Multi-Level Caches",
      "subsection": "Two-Level Cache Organization"
    },
    {
      "id": "4-Multi-Level-Caches-Two-Level-Cache-Organization-Main-Memory",
      "title": "Main Memory",
      "front": "Main Memory",
      "back": "1. **Location:** Off-chip\n2. **Size:** Very large (GB)\n3. **Speed:** Slow (50-100+ cycles)\n4. **Characteristics:**\n5. Services misses from L3\n6. Largest capacity\n7. Slowest access",
      "type": "list",
      "section": "Multi-Level Caches",
      "subsection": "Two-Level Cache Organization"
    },
    {
      "id": "4-Multi-Level-Caches-Multi-Level-Cache-Operation-Access-Flow",
      "title": "Access Flow",
      "front": "Access Flow",
      "back": "CPU Request Check L1 Cache Check L2 Cache Check L3 Cache Access Main Memory",
      "type": "definition",
      "section": "Multi-Level Caches",
      "subsection": "Multi-Level Cache Operation"
    },
    {
      "id": "4-Multi-Level-Caches-Multi-Level-Cache-Operation-Hit-at-L1",
      "title": "Hit at L1",
      "front": "Hit at L1",
      "back": "Fastest (1-2 cycles)",
      "type": "definition",
      "section": "Multi-Level Caches",
      "subsection": "Multi-Level Cache Operation"
    },
    {
      "id": "4-Multi-Level-Caches-Multi-Level-Cache-Operation-Hit-at-L2",
      "title": "Hit at L2",
      "front": "Hit at L2",
      "back": "Fast (5-20 cycles)",
      "type": "definition",
      "section": "Multi-Level Caches",
      "subsection": "Multi-Level Cache Operation"
    },
    {
      "id": "4-Multi-Level-Caches-Multi-Level-Cache-Operation-Hit-at-L3",
      "title": "Hit at L3",
      "front": "Hit at L3",
      "back": "Moderate (20-50 cycles)",
      "type": "definition",
      "section": "Multi-Level Caches",
      "subsection": "Multi-Level Cache Operation"
    },
    {
      "id": "4-Multi-Level-Caches-Multi-Level-Cache-Operation-Miss--Main-Memory-",
      "title": "Miss (Main Memory)",
      "front": "Miss (Main Memory)",
      "back": "Slow (50-100+ cycles)",
      "type": "definition",
      "section": "Multi-Level Caches",
      "subsection": "Multi-Level Cache Operation"
    },
    {
      "id": "4-Multi-Level-Caches-Multi-Level-Performance-Example-Calculation",
      "title": "Example Calculation",
      "front": "Example Calculation",
      "back": "",
      "type": "definition",
      "section": "Multi-Level Caches",
      "subsection": "Multi-Level Performance"
    },
    {
      "id": "4-Multi-Level-Caches-Multi-Level-Performance-Given",
      "title": "Given",
      "front": "Given",
      "back": "1. Base CPI = 1\n2. Clock rate = 4 GHz (0.25 ns per cycle)\n3. L1 miss rate = 2%\n4. L2 access time = 5 ns\n5. L2 global miss rate = 0.5%\n6. Main memory access time = 100 ns",
      "type": "list",
      "section": "Multi-Level Caches",
      "subsection": "Multi-Level Performance"
    },
    {
      "id": "4-Multi-Level-Caches-Multi-Level-Performance-Without-L2",
      "title": "Without L2",
      "front": "Without L2",
      "back": "Miss penalty = 100 ns / 0.25 ns = 400 cycles CPI = 1 + 0.02 × 400 = 9",
      "type": "definition",
      "section": "Multi-Level Caches",
      "subsection": "Multi-Level Performance"
    },
    {
      "id": "4-Multi-Level-Caches-Multi-Level-Performance-With-L2",
      "title": "With L2",
      "front": "With L2",
      "back": "L1 miss, L2 hit penalty = 5 ns / 0.25 ns = 20 cycles L1 miss, L2 miss penalty = 100 ns / 0.25 ns = 400 cycles CPI = 1 + 0.02 × 20 + 0.005 × 400 = 1 + 0.4 + 2",
      "type": "definition",
      "section": "Multi-Level Caches",
      "subsection": "Multi-Level Performance"
    },
    {
      "id": "4-Multi-Level-Caches-Multi-Level-Performance-Performance-Improvement",
      "title": "Performance Improvement",
      "front": "Performance Improvement",
      "back": "9/3.4 = 2.6x faster!",
      "type": "definition",
      "section": "Multi-Level Caches",
      "subsection": "Multi-Level Performance"
    },
    {
      "id": "4-Multi-Level-Caches-Unified-vs--Split-Caches-Unified-Cache",
      "title": "Unified Cache",
      "front": "Unified Cache",
      "back": "1. Single cache for both instructions and data\n2. **Advantages:**\n3. Higher hit rate (balances load automatically)\n4. Simpler design\n5. **Disadvantages:**\n6. Contention between instruction fetch and data access\n7. Problematic for pipelining",
      "type": "list",
      "section": "Multi-Level Caches",
      "subsection": "Unified vs. Split Caches"
    },
    {
      "id": "4-Multi-Level-Caches-Unified-vs--Split-Caches-Split-Cache",
      "title": "Split Cache",
      "front": "Split Cache",
      "back": "1. Separate instruction cache (I-cache) and data cache (D-cache)\n2. **Advantages:**\n3. No contention\n4. Better for pipelining\n5. Can optimize each separately\n6. **Disadvantages:**\n7. Lower hit rate (fixed allocation)\n8. More complex",
      "type": "list",
      "section": "Multi-Level Caches",
      "subsection": "Unified vs. Split Caches"
    },
    {
      "id": "4-Multi-Level-Caches-Unified-vs--Split-Caches-Modern-Practice",
      "title": "Modern Practice",
      "front": "Modern Practice",
      "back": "1. **L1:** Split (I-cache and D-cache)\n2. **L2/L3:** Unified (shared)",
      "type": "list",
      "section": "Multi-Level Caches",
      "subsection": "Unified vs. Split Caches"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Semiconductor-Memory-Overview-Two-Main-Types",
      "title": "Two Main Types",
      "front": "Two Main Types",
      "back": "1. **RAM (Random Access Memory):**\n2. Read/Write\n3. Volatile\n4. Temporary storage\n5. Static (SRAM) or Dynamic (DRAM)\n6. **ROM (Read-Only Memory):**\n7. Read-only (mostly)\n8. Nonvolatile\n9. Permanent storage\n10. Various types (ROM, PROM, EPROM, EEPROM, Flash)",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Semiconductor Memory Overview"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Structure",
      "title": "Structure",
      "front": "Structure",
      "back": "",
      "type": "concept",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Dynamic RAM (DRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Basic-Cell",
      "title": "Basic Cell",
      "front": "Basic Cell",
      "back": "1. **Capacitor:** Stores charge (1) or no charge (0)\n2. **Transistor:** Switch to access capacitor",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Dynamic RAM (DRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Operation",
      "title": "Operation",
      "front": "Operation",
      "back": "1. **Write:** Apply voltage to bit line, activate address line, charge/discharge capacitor\n2. **Read:** Activate address line, sense charge on capacitor, restore charge",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Dynamic RAM (DRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Characteristics",
      "title": "Characteristics",
      "front": "Characteristics",
      "back": "",
      "type": "concept",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Dynamic RAM (DRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Storage-Mechanism",
      "title": "Storage Mechanism",
      "front": "Storage Mechanism",
      "back": "1. Bits stored as **charge in capacitors**\n2. Presence/absence of charge = binary 1/0\n3. **Essentially analogue:** Level of charge determines value",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Dynamic RAM (DRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Key-Properties",
      "title": "Key Properties",
      "front": "Key Properties",
      "back": "1. **Charges leak:** Capacitors lose charge over time\n2. **Needs refreshing:** Periodic refresh to maintain data\n3. **Dynamic:** Stored charge leaks away even with power",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Dynamic RAM (DRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Refresh-Requirements",
      "title": "Refresh Requirements",
      "front": "Refresh Requirements",
      "back": "1. Must refresh every few milliseconds (typically 64 ms)\n2. Refresh circuit included on chip\n3. Refresh process:\n4. Disable chip\n5. Count through rows\n6. Read and write back each row\n7. Takes time, slows performance",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Dynamic RAM (DRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Advantages",
      "title": "Advantages",
      "front": "Advantages",
      "back": "1. **Simpler construction:** Fewer transistors per cell\n2. **Smaller per bit:** Higher density\n3. **Less expensive:** Lower cost\n4. **High capacity:** Good for main memory",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Dynamic RAM (DRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Disadvantages",
      "title": "Disadvantages",
      "front": "Disadvantages",
      "back": "1. **Needs refresh circuits:** Additional complexity\n2. **Slower:** Refresh overhead\n3. **More complex timing:** Refresh cycles",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Dynamic RAM (DRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Dynamic-RAM--DRAM--Use",
      "title": "Use",
      "front": "Use",
      "back": "**Main memory** (large capacity needed, cost-sensitive)",
      "type": "definition",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Dynamic RAM (DRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Structure",
      "title": "Structure",
      "front": "Structure",
      "back": "",
      "type": "concept",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Static RAM (SRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Basic-Cell",
      "title": "Basic Cell",
      "front": "Basic Cell",
      "back": "1. **Flip-flop circuit:** Two cross-coupled inverters\n2. **Transistor arrangement:** Provides stable logic state\n3. **No capacitor:** State maintained by circuit",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Static RAM (SRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Operation",
      "title": "Operation",
      "front": "Operation",
      "back": "1. **State 1:** C1 high, C2 low (T1, T4 off; T2, T3 on)\n2. **State 0:** C2 high, C1 low (T2, T3 off; T1, T4 on)\n3. **Write:** Apply value to bit lines\n4. **Read:** Sense value on bit line",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Static RAM (SRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Characteristics",
      "title": "Characteristics",
      "front": "Characteristics",
      "back": "",
      "type": "concept",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Static RAM (SRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Storage-Mechanism",
      "title": "Storage Mechanism",
      "front": "Storage Mechanism",
      "back": "1. Bits stored as **on/off switches** (flip-flops)\n2. Digital: Clear 1 or 0 state\n3. **No charges to leak:** State maintained by circuit",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Static RAM (SRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Key-Properties",
      "title": "Key Properties",
      "front": "Key Properties",
      "back": "1. **No refresh needed:** Maintains state as long as powered\n2. **Faster:** No refresh overhead\n3. **More complex:** More transistors per cell",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Static RAM (SRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Advantages",
      "title": "Advantages",
      "front": "Advantages",
      "back": "1. **No refresh needed:** Simpler operation\n2. **Faster:** Lower access time\n3. **Simpler timing:** No refresh cycles",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Static RAM (SRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Disadvantages",
      "title": "Disadvantages",
      "front": "Disadvantages",
      "back": "1. **More complex construction:** More transistors\n2. **Larger per bit:** Lower density\n3. **More expensive:** Higher cost",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Static RAM (SRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-Static-RAM--SRAM--Use",
      "title": "Use",
      "front": "Use",
      "back": "**Cache memory** (speed critical, smaller capacity)",
      "type": "definition",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "Static RAM (SRAM)"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-SRAM-vs--DRAM-Comparison-Key-Insight",
      "title": "Key Insight",
      "front": "Key Insight",
      "back": "SRAM is faster but more expensive. DRAM is cheaper but slower. Use SRAM for small, fast cache; DRAM for large, cheap main memory.",
      "type": "definition",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "SRAM vs. DRAM Comparison"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-DRAM-Organization-Internal-Structure",
      "title": "Internal Structure",
      "front": "Internal Structure",
      "back": "1. Organized as **2D array** of cells\n2. **Row and column addressing:**\n3. Reduces number of address pins\n4. Multiplex row and column addresses\n5. Example: 2048 × 2048 × 4 bits = 16 Mbit chip\n6. Only 11 address pins needed (2^11 = 2048)",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "DRAM Organization"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-DRAM-Organization-Module-Organization",
      "title": "Module Organization",
      "front": "Module Organization",
      "back": "1. Multiple chips combined to form memory module\n2. Example: 256 KB module = 8 chips × 32 Kbit each\n3. Example: 1 MB module = 8 chips × 1 Mbit each",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "DRAM Organization"
    },
    {
      "id": "4-Internal-Memory--DRAM-and-SRAM-DRAM-Organization-Interleaved-Memory",
      "title": "Interleaved Memory",
      "front": "Interleaved Memory",
      "back": "1. Multiple memory banks\n2. Each bank can service requests independently\n3. Consecutive words stored in different banks\n4. **K banks** can service **K requests** simultaneously\n5. Increases memory bandwidth",
      "type": "list",
      "section": "Internal Memory: DRAM and SRAM",
      "subsection": "DRAM Organization"
    },
    {
      "id": "4-Error-Detection-and-Correction-Why-Error-Correction--Problem",
      "title": "Problem",
      "front": "Problem",
      "back": "Memory can have errors.",
      "type": "definition",
      "section": "Error Detection and Correction",
      "subsection": "Why Error Correction?"
    },
    {
      "id": "4-Error-Detection-and-Correction-Why-Error-Correction--Types-of-Errors",
      "title": "Types of Errors",
      "front": "Types of Errors",
      "back": "1. **Hard Failure:**\n2. Permanent defect\n3. Manufacturing defect\n4. Physical damage\n5. **Soft Error:**\n6. Random, non-destructive\n7. Caused by:\n8. Alpha particles\n9. Cosmic rays\n10. Electrical noise\n11. No permanent damage\n12. Data corruption",
      "type": "list",
      "section": "Error Detection and Correction",
      "subsection": "Why Error Correction?"
    },
    {
      "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Purpose",
      "title": "Purpose",
      "front": "Purpose",
      "back": "Detect and correct single-bit errors.",
      "type": "definition",
      "section": "Error Detection and Correction",
      "subsection": "Hamming Error Correcting Code"
    },
    {
      "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Principle",
      "title": "Principle",
      "front": "Principle",
      "back": "1. Add **check bits** (redundancy) to data\n2. Check bits encode information about data bits\n3. Can detect and correct errors",
      "type": "list",
      "section": "Error Detection and Correction",
      "subsection": "Hamming Error Correcting Code"
    },
    {
      "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Hamming-Distance",
      "title": "Hamming Distance",
      "front": "Hamming Distance",
      "back": "1. Minimum number of bit positions in which two code words differ\n2. For single-bit error correction: Minimum distance = 3",
      "type": "list",
      "section": "Error Detection and Correction",
      "subsection": "Hamming Error Correcting Code"
    },
    {
      "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Implementation",
      "title": "Implementation",
      "front": "Implementation",
      "back": "1. Check bits placed at positions that are powers of 2 (1, 2, 4, 8, ...)\n2. Each check bit covers specific data bits\n3. Check bits calculated using XOR operations",
      "type": "list",
      "section": "Error Detection and Correction",
      "subsection": "Hamming Error Correcting Code"
    },
    {
      "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Example---8-bit-data",
      "title": "Example - 8-bit data",
      "front": "Example - 8-bit data",
      "back": "1. Need 4 check bits (positions 1, 2, 4, 8)\n2. Total: 12 bits (8 data + 4 check)",
      "type": "list",
      "section": "Error Detection and Correction",
      "subsection": "Hamming Error Correcting Code"
    },
    {
      "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Error-Detection",
      "title": "Error Detection",
      "front": "Error Detection",
      "back": "1. Recalculate check bits from data\n2. Compare with stored check bits\n3. If different: Error detected\n4. Pattern indicates which bit is wrong",
      "type": "list",
      "section": "Error Detection and Correction",
      "subsection": "Hamming Error Correcting Code"
    },
    {
      "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Error-Correction",
      "title": "Error Correction",
      "front": "Error Correction",
      "back": "1. Error pattern identifies bit position\n2. Flip that bit to correct error",
      "type": "list",
      "section": "Error Detection and Correction",
      "subsection": "Hamming Error Correcting Code"
    },
    {
      "id": "4-Error-Detection-and-Correction-Hamming-Error-Correcting-Code-Overhead",
      "title": "Overhead",
      "front": "Overhead",
      "back": "1. More bits needed (redundancy)\n2. Example: 8 data bits → 12 total bits (50% overhead)\n3. Larger data words: Lower overhead percentage",
      "type": "list",
      "section": "Error Detection and Correction",
      "subsection": "Hamming Error Correcting Code"
    },
    {
      "id": "4-Advanced-DRAM-Technologies-Traditional-DRAM-Limitations-Problems",
      "title": "Problems",
      "front": "Problems",
      "back": "1. **Internal architecture:** Slow access patterns\n2. **Interface:** Asynchronous, processor must wait\n3. **Bottleneck:** Memory interface limits performance",
      "type": "list",
      "section": "Advanced DRAM Technologies",
      "subsection": "Traditional DRAM Limitations"
    },
    {
      "id": "4-Advanced-DRAM-Technologies-Synchronous-DRAM--SDRAM--Key-Innovation",
      "title": "Key Innovation",
      "front": "Key Innovation",
      "back": "Access synchronized with external clock.",
      "type": "definition",
      "section": "Advanced DRAM Technologies",
      "subsection": "Synchronous DRAM (SDRAM)"
    },
    {
      "id": "4-Advanced-DRAM-Technologies-Synchronous-DRAM--SDRAM--Operation",
      "title": "Operation",
      "front": "Operation",
      "back": "1. Address presented to RAM\n2. RAM finds data\n3. **CPU knows when data will be ready** (synchronized with clock)\n4. CPU doesn't have to wait (can do other work)",
      "type": "list",
      "section": "Advanced DRAM Technologies",
      "subsection": "Synchronous DRAM (SDRAM)"
    },
    {
      "id": "4-Advanced-DRAM-Technologies-Synchronous-DRAM--SDRAM--Burst-Mode",
      "title": "Burst Mode",
      "front": "Burst Mode",
      "back": "1. Set up stream of data\n2. Fire out data in block\n3. More efficient than single-word transfers",
      "type": "list",
      "section": "Advanced DRAM Technologies",
      "subsection": "Synchronous DRAM (SDRAM)"
    },
    {
      "id": "4-Advanced-DRAM-Technologies-Synchronous-DRAM--SDRAM--Advantages",
      "title": "Advantages",
      "front": "Advantages",
      "back": "1. Predictable timing\n2. CPU can pipeline other operations\n3. Better bandwidth utilization",
      "type": "list",
      "section": "Advanced DRAM Technologies",
      "subsection": "Synchronous DRAM (SDRAM)"
    },
    {
      "id": "4-Advanced-DRAM-Technologies-Double-Data-Rate-SDRAM--DDR-SDRAM--Key-Innovation",
      "title": "Key Innovation",
      "front": "Key Innovation",
      "back": "Sends data **twice per clock cycle**.",
      "type": "definition",
      "section": "Advanced DRAM Technologies",
      "subsection": "Double Data Rate SDRAM (DDR SDRAM)"
    },
    {
      "id": "4-Advanced-DRAM-Technologies-Double-Data-Rate-SDRAM--DDR-SDRAM--How",
      "title": "How",
      "front": "How",
      "back": "1. Data transfer on **both rising and falling edge** of clock\n2. Doubles data rate compared to SDRAM",
      "type": "list",
      "section": "Advanced DRAM Technologies",
      "subsection": "Double Data Rate SDRAM (DDR SDRAM)"
    },
    {
      "id": "4-Advanced-DRAM-Technologies-Double-Data-Rate-SDRAM--DDR-SDRAM--Achieves-Higher-Rates-Through",
      "title": "Achieves Higher Rates Through",
      "front": "Achieves Higher Rates Through",
      "back": "1. **Double clocking:** Data on both edges\n2. **Higher bus clock rate:** Faster interface\n3. **Buffering scheme:** Prefetch and buffer data",
      "type": "list",
      "section": "Advanced DRAM Technologies",
      "subsection": "Double Data Rate SDRAM (DDR SDRAM)"
    },
    {
      "id": "4-Advanced-DRAM-Technologies-Double-Data-Rate-SDRAM--DDR-SDRAM--Generations",
      "title": "Generations",
      "front": "Generations",
      "back": "1. **DDR:** 2x SDRAM\n2. **DDR2:** 2x DDR (4x SDRAM)\n3. **DDR3:** 2x DDR2 (8x SDRAM)\n4. **DDR4:** 2x DDR3 (16x SDRAM)\n5. **DDR5:** 2x DDR4 (32x SDRAM)",
      "type": "list",
      "section": "Advanced DRAM Technologies",
      "subsection": "Double Data Rate SDRAM (DDR SDRAM)"
    },
    {
      "id": "4-Advanced-DRAM-Technologies-Double-Data-Rate-SDRAM--DDR-SDRAM--Each-Generation",
      "title": "Each Generation",
      "front": "Each Generation",
      "back": "1. Higher data rates\n2. Lower voltage (power efficiency)\n3. Better signal integrity\n4. More features",
      "type": "list",
      "section": "Advanced DRAM Technologies",
      "subsection": "Double Data Rate SDRAM (DDR SDRAM)"
    },
    {
      "id": "4-Advanced-DRAM-Technologies-Other-Advanced-Technologies-DDR-Variations",
      "title": "DDR Variations",
      "front": "DDR Variations",
      "back": "1. **GDDR:** Graphics DDR (for GPUs)\n2. **LPDDR:** Low Power DDR (for mobile devices)",
      "type": "list",
      "section": "Advanced DRAM Technologies",
      "subsection": "Other Advanced Technologies"
    },
    {
      "id": "4-Advanced-DRAM-Technologies-Other-Advanced-Technologies-Future-Technologies",
      "title": "Future Technologies",
      "front": "Future Technologies",
      "back": "1. **HBM (High Bandwidth Memory):** 3D stacked memory\n2. **HMC (Hybrid Memory Cube):** Advanced 3D memory",
      "type": "list",
      "section": "Advanced DRAM Technologies",
      "subsection": "Other Advanced Technologies"
    },
    {
      "id": "4-Key-Concepts-Summary-Memory-Hierarchy-Principles-Memory-Hierarchy-Principles",
      "title": "Memory Hierarchy Principles",
      "front": "Memory Hierarchy Principles",
      "back": "1. **Trade-offs:** Speed, capacity, cost cannot all be optimized simultaneously\n2. **Hierarchy Solution:** Use multiple levels with different characteristics\n3. **Locality:** Temporal and spatial locality enable hierarchy to work\n4. **Performance:** Average access time much closer to fast memory than slow memory",
      "type": "list",
      "section": "Key Concepts Summary",
      "subsection": "Memory Hierarchy Principles"
    },
    {
      "id": "4-Key-Concepts-Summary-Cache-Fundamentals-Cache-Fundamentals",
      "title": "Cache Fundamentals",
      "front": "Cache Fundamentals",
      "back": "1. **Purpose:** Bridge speed gap between CPU and main memory\n2. **Operation:** Store recently accessed blocks\n3. **Metrics:** Hit rate, miss rate, miss penalty\n4. **Performance:** AMAT = Hit time + (Miss rate × Miss penalty)",
      "type": "list",
      "section": "Key Concepts Summary",
      "subsection": "Cache Fundamentals"
    },
    {
      "id": "4-Key-Concepts-Summary-Cache-Design-Cache-Design",
      "title": "Cache Design",
      "front": "Cache Design",
      "back": "1. **Mapping:** Direct, associative, set-associative (trade-offs)\n2. **Replacement:** LRU most effective, but more complex\n3. **Write Policy:** Write back better performance, write through simpler\n4. **Size:** Larger = better hit rate, but slower hit time",
      "type": "list",
      "section": "Key Concepts Summary",
      "subsection": "Cache Design"
    },
    {
      "id": "4-Key-Concepts-Summary-Memory-Technologies-Memory-Technologies",
      "title": "Memory Technologies",
      "front": "Memory Technologies",
      "back": "1. **DRAM:** Cheap, dense, needs refresh, used for main memory\n2. **SRAM:** Fast, expensive, no refresh, used for cache\n3. **Error Correction:** Hamming codes detect and correct errors\n4. **Advanced DRAM:** SDRAM, DDR improve performance",
      "type": "list",
      "section": "Key Concepts Summary",
      "subsection": "Memory Technologies"
    },
    {
      "id": "4-Practice-Problems-and-Examples-Problem-1--Cache-Address-Breakdown-Question",
      "title": "Question",
      "front": "Question",
      "back": "A direct-mapped cache has 32 lines, block size 8 bytes. Main memory is 16 MB. How is a 24-bit address divided?",
      "type": "definition",
      "section": "Practice Problems and Examples",
      "subsection": "Problem 1: Cache Address Breakdown"
    },
    {
      "id": "4-Practice-Problems-and-Examples-Problem-1--Cache-Address-Breakdown-Solution",
      "title": "Solution",
      "front": "Solution",
      "back": "1. Cache lines: 32 = 2^5 → 5 bits for line\n2. Block size: 8 bytes = 2^3 → 3 bits for word offset\n3. Tag: 24 - 5 - 3 = 16 bits",
      "type": "list",
      "section": "Practice Problems and Examples",
      "subsection": "Problem 1: Cache Address Breakdown"
    },
    {
      "id": "4-Practice-Problems-and-Examples-Problem-1--Cache-Address-Breakdown-Answer",
      "title": "Answer",
      "front": "Answer",
      "back": "1. Tag: 16 bits\n2. Line: 5 bits\n3. Word: 3 bits",
      "type": "list",
      "section": "Practice Problems and Examples",
      "subsection": "Problem 1: Cache Address Breakdown"
    },
    {
      "id": "4-Practice-Problems-and-Examples-Problem-2--AMAT-Calculation-Question",
      "title": "Question",
      "front": "Question",
      "back": "Cache has hit time 2 ns, miss rate 3%, miss penalty 50 ns. What is AMAT?",
      "type": "definition",
      "section": "Practice Problems and Examples",
      "subsection": "Problem 2: AMAT Calculation"
    },
    {
      "id": "4-Practice-Problems-and-Examples-Problem-2--AMAT-Calculation-Solution",
      "title": "Solution",
      "front": "Solution",
      "back": "AMAT = Hit time + (Miss rate × Miss penalty) = 2 + (0.03 × 50)",
      "type": "definition",
      "section": "Practice Problems and Examples",
      "subsection": "Problem 2: AMAT Calculation"
    },
    {
      "id": "4-Practice-Problems-and-Examples-Problem-2--AMAT-Calculation-Answer",
      "title": "Answer",
      "front": "Answer",
      "back": "3.5 ns",
      "type": "definition",
      "section": "Practice Problems and Examples",
      "subsection": "Problem 2: AMAT Calculation"
    },
    {
      "id": "4-Practice-Problems-and-Examples-Problem-3--CPI-with-Cache-Question",
      "title": "Question",
      "front": "Question",
      "back": "Base CPI = 2, I-cache miss rate = 2%, D-cache miss rate = 5%, 40% loads/stores, miss penalty = 100 cycles. What is actual CPI?",
      "type": "definition",
      "section": "Practice Problems and Examples",
      "subsection": "Problem 3: CPI with Cache"
    },
    {
      "id": "4-Practice-Problems-and-Examples-Problem-3--CPI-with-Cache-Solution",
      "title": "Solution",
      "front": "Solution",
      "back": "Instruction miss cycles = 0.02 × 100 = 2 Data miss cycles = 0.4 × 0.05 × 100 = 2 CPI_actual = 2 + 2 + 2 = 6",
      "type": "definition",
      "section": "Practice Problems and Examples",
      "subsection": "Problem 3: CPI with Cache"
    },
    {
      "id": "4-Practice-Problems-and-Examples-Problem-3--CPI-with-Cache-Answer",
      "title": "Answer",
      "front": "Answer",
      "back": "CPI = 6",
      "type": "definition",
      "section": "Practice Problems and Examples",
      "subsection": "Problem 3: CPI with Cache"
    },
    {
      "id": "4-Practice-Problems-and-Examples-Problem-4--Set-Associative-Mapping-Question",
      "title": "Question",
      "front": "Question",
      "back": "4-way set associative cache, 64 lines total. How many sets? Block 100 maps to which set?",
      "type": "definition",
      "section": "Practice Problems and Examples",
      "subsection": "Problem 4: Set-Associative Mapping"
    },
    {
      "id": "4-Practice-Problems-and-Examples-Problem-4--Set-Associative-Mapping-Solution",
      "title": "Solution",
      "front": "Solution",
      "back": "1. Total lines: 64\n2. Ways per set: 4\n3. Number of sets: 64 / 4 = 16 sets\n4. Set bits: log2(16) = 4 bits\n5. Block 100: 100 mod 16 = 4 → Set 4",
      "type": "list",
      "section": "Practice Problems and Examples",
      "subsection": "Problem 4: Set-Associative Mapping"
    },
    {
      "id": "4-Practice-Problems-and-Examples-Problem-4--Set-Associative-Mapping-Answer",
      "title": "Answer",
      "front": "Answer",
      "back": "16 sets, Block 100 → Set 4",
      "type": "definition",
      "section": "Practice Problems and Examples",
      "subsection": "Problem 4: Set-Associative Mapping"
    },
    {
      "id": "4-Practice-Problems-and-Examples-Problem-5--Multi-Level-Cache-Question",
      "title": "Question",
      "front": "Question",
      "back": "L1 hit time = 1 cycle, miss rate = 5%, L2 hit time = 10 cycles, global miss rate = 1%, main memory = 100 cycles. What is AMAT?",
      "type": "definition",
      "section": "Practice Problems and Examples",
      "subsection": "Problem 5: Multi-Level Cache"
    },
    {
      "id": "4-Practice-Problems-and-Examples-Problem-5--Multi-Level-Cache-Solution",
      "title": "Solution",
      "front": "Solution",
      "back": "L1 hit: 0.95 × 1 = 0.95 cycles L1 miss, L2 hit: 0.05 × 0.8 × 10 = 0.4 cycles (80% of L1 misses hit in L2) L1 miss, L2 miss: 0.05 × 0.2 × 100 = 1.0 cycles (20% of L1 misses miss in L2) AMAT = 0.95 + 0.4 + 1.0 = 2.35 cycles",
      "type": "definition",
      "section": "Practice Problems and Examples",
      "subsection": "Problem 5: Multi-Level Cache"
    },
    {
      "id": "4-Practice-Problems-and-Examples-Problem-5--Multi-Level-Cache-Answer",
      "title": "Answer",
      "front": "Answer",
      "back": "1. **Understand Trade-offs:**\n2. Every design decision has pros and cons\n3. Larger cache vs. faster cache\n4. Higher associativity vs. lower cost\n5. **Master Calculations:**\n6. AMAT formula\n7. CPI with cache misses\n8. Address breakdown for different mappings\n9. **Visualize Cache Organization:**\n10. Draw cache structures\n11. Understand tag, line, word fields\n12. Trace cache operations\n13. **Compare Techniques:**\n14. Direct vs. associative vs. set-associative\n15. Write through vs. write back\n16. Replacement algorithms\n17. **Understand Locality:**\n18. Why hierarchy works\n19. How to exploit locality\n20. Impact on performance\n21. **Memory hierarchy** solves the speed/capacity/cost trade-off\n22. **Locality of reference** makes hierarchy effective\n23. **Cache memory** bridges the CPU-memory speed gap\n24. **Mapping techniques** balance flexibility and cost\n25. **Multi-level caches** further improve performance\n26. **DRAM and SRAM** serve different roles in the hierarchy\n27. Memory is often the performance bottleneck\n28. Cache design significantly impacts performance\n29. Memory hierarchy is fundamental to computer architecture",
      "type": "list",
      "section": "Practice Problems and Examples",
      "subsection": "Problem 5: Multi-Level Cache"
    }
  ]
}