# Group 1: Foundations of Computer Architecture
## Comprehensive Exercise Solutions - Part 2

**Chapter 3: Top-Level View of Computer Function and Interconnection**

---

## Table of Contents
1. [Short Answer Solutions (Continued)](#short-answer-solutions-continued)
2. [Calculation Problem Solutions](#calculation-problem-solutions)
3. [Diagram and Flowchart Solutions](#diagram-and-flowchart-solutions)
4. [Problem-Solving Exercise Solutions](#problem-solving-exercise-solutions)
5. [Conceptual Question Solutions](#conceptual-question-solutions)

---

## Short Answer Solutions (Continued)

### Section 4: Interrupts

**SA 4.1** List the four classes of interrupts and give one example of each.

**Answer:**

1. **Program Interrupts:**
   - Generated by program execution itself
   - Examples:
     - **Overflow:** Arithmetic operation result exceeds register capacity
     - **Division by Zero:** Attempt to divide by zero
     - **Illegal Instruction:** Invalid opcode encountered
     - **Privilege Violation:** User program attempts privileged operation

2. **Timer Interrupts:**
   - Generated by internal processor timer
   - Examples:
     - **OS Scheduler:** Periodic interrupt for task switching (every 10ms)
     - **Time Slicing:** Fair CPU time allocation in multi-tasking
     - **Real-Time Systems:** Periodic task scheduling

3. **I/O Interrupts:**
   - Generated by I/O controllers
   - Examples:
     - **Device Ready:** Printer finishes printing, requests next page
     - **Device Error:** Disk read error, network timeout
     - **Data Available:** Keyboard input ready, network packet received

4. **Hardware Failure Interrupts:**
   - Generated by hardware malfunctions
   - Examples:
     - **Power Failure:** Power supply issues detected
     - **Memory Parity Error:** Data corruption detected in memory
     - **Hardware Malfunction:** Component failure detected

---

**SA 4.2** Explain how interrupts improve processing efficiency, especially in I/O operations.

**Answer:**

**Without Interrupts (Programmed I/O):**
- CPU issues I/O command
- CPU continuously polls device status (busy-waiting)
- CPU cannot do other work while waiting
- CPU time is wasted checking device status repeatedly
- Example: CPU checks printer 10,000 times while waiting for 10ms

**With Interrupts (Interrupt-Driven I/O):**
- CPU issues I/O command to device
- CPU continues executing other programs/tasks
- Device operates independently
- When I/O completes, device sends interrupt signal
- CPU suspends current work, handles interrupt, then resumes

**Efficiency Improvement:**
- **CPU Utilization:** CPU can do useful work instead of waiting
- **Throughput:** System can handle multiple I/O operations concurrently
- **Responsiveness:** System remains responsive to other events
- **Scalability:** Can handle many I/O devices efficiently

**Example Scenario:**
- Printer takes 10ms to print a page
- Without interrupts: CPU wastes 10ms polling
- With interrupts: CPU does 10ms of useful work, then handles interrupt (50ns)
- Efficiency gain: ~99.9995% improvement

**Key Insight:** Interrupts allow CPU and I/O devices to operate in parallel, dramatically improving system efficiency.

---

**SA 4.3** Describe what happens during the interrupt cycle. What context information must be saved?

**Answer:**

**Interrupt Cycle Steps:**

1. **Interrupt Detection:**
   - After each instruction, CPU checks for interrupt signals
   - If interrupt is pending, proceed to interrupt handling

2. **Current Instruction Completion:**
   - Current instruction is allowed to complete (or is suspended if non-interruptible)
   - Ensures system remains in consistent state

3. **Context Saving:**
   - CPU saves the current processor state to memory (typically stack)
   - **Context Information Saved:**
     - **Program Counter (PC):** Address of next instruction to execute
     - **Processor Status Word (PSW):** Condition codes, flags, processor mode
     - **General-Purpose Registers:** All register contents
     - **Other Processor State:** Interrupt enable flags, mode bits, etc.

4. **Interrupt Handler Invocation:**
   - PC is set to the start address of the interrupt handler routine
   - Interrupts may be disabled (depending on system design)
   - Control transfers to interrupt handler

5. **Interrupt Processing:**
   - Handler executes to process the interrupt
   - May involve I/O operations, error handling, scheduling, etc.

6. **Context Restoration:**
   - Saved context is restored from memory
   - PC, PSW, and registers are restored to their pre-interrupt values

7. **Return to Interrupted Program:**
   - Execution resumes from the instruction after where interrupt occurred
   - Program continues as if interrupt never happened (from program's perspective)

**Why Save Context:**
- Allows interrupted program to resume exactly where it left off
- Ensures no data loss or corruption
- Enables transparent interrupt handling

---

**SA 4.4** Compare and contrast the two strategies for handling multiple interrupts: disabling interrupts vs. priority-based nested interrupts.

**Answer:**

**Strategy 1: Disable Interrupts**

**How it Works:**
- When processing an interrupt, CPU disables further interrupts
- New interrupts are held pending (not lost)
- After current interrupt handler completes, interrupts are re-enabled
- Pending interrupts are then checked and processed

**Advantages:**
- Simple to implement
- No nested interrupt complexity
- Predictable execution
- Lower overhead (no nested context saving)

**Disadvantages:**
- Higher priority interrupts must wait for lower priority ones
- Can cause delays for critical events
- Not suitable for real-time systems
- Lower priority interrupts can block higher priority ones

**Use Cases:**
- Simple systems
- Low interrupt rates
- Non-critical applications

---

**Strategy 2: Priority-Based Nested Interrupts**

**How it Works:**
- Each interrupt has a priority level
- Higher priority interrupts can interrupt lower priority handlers
- When high priority interrupt occurs, lower priority handler is suspended
- High priority handler executes, then returns to suspended handler
- Supports multiple levels of nesting

**Advantages:**
- Critical events handled immediately
- Suitable for real-time systems
- Better responsiveness
- More efficient for mixed priority workloads

**Disadvantages:**
- More complex implementation
- Higher overhead (nested context saving/restoration)
- Requires priority management
- Can lead to stack overflow with deep nesting

**Use Cases:**
- Real-time systems
- Systems with mixed priority events
- Critical applications requiring immediate response

**Comparison Summary:**

| Aspect | Disable Interrupts | Priority-Based Nested |
|--------|-------------------|----------------------|
| Complexity | Low | High |
| Responsiveness | Lower | Higher |
| Overhead | Low | Higher |
| Real-time Support | Poor | Good |
| Implementation | Simple | Complex |

---

**SA 4.5** Why are timer interrupts important for operating systems?

**Answer:**

Timer interrupts are essential for modern operating systems for several reasons:

1. **Pre-emptive Multi-tasking:**
   - Allow OS to periodically regain control from user programs
   - Enable switching between multiple processes
   - Prevent any single process from monopolizing the CPU
   - Ensure fair CPU time allocation

2. **Time Slicing:**
   - Divide CPU time into time slices (quantums)
   - Each process gets a fair share of CPU time
   - Timer interrupt triggers when time slice expires
   - OS scheduler runs and switches to next process

3. **System Responsiveness:**
   - Ensure system remains responsive to user input
   - Prevent long-running programs from freezing the system
   - Allow OS to handle urgent tasks promptly

4. **Scheduling:**
   - Enable implementation of various scheduling algorithms
   - Round-robin, priority-based, etc.
   - Timer provides the mechanism for time-based scheduling

5. **Real-Time Systems:**
   - Support periodic task scheduling
   - Enable time-critical operations
   - Maintain timing guarantees

6. **Resource Management:**
   - Allow OS to monitor and manage system resources
   - Periodic checks for resource allocation
   - Time-based resource limits

**Example:**
- Timer interrupt every 10ms
- OS scheduler runs on each interrupt
- Checks if current process has used its time slice
- If yes, saves context and switches to next process
- If no, allows current process to continue

**Without Timer Interrupts:**
- Only cooperative multitasking possible (unreliable)
- Processes must voluntarily yield CPU
- One misbehaving process can freeze entire system
- No guarantee of fair CPU allocation

---

### Section 5: System Bus

**SA 5.1** Describe the three types of buses that make up the system bus and what each carries.

**Answer:**

The system bus consists of three separate buses:

1. **Data Bus:**
   - **Direction:** Bidirectional (data flows both ways)
   - **Contents:** Actual data and instructions being transferred
   - **Width:** Typically 32, 64, or 128 bits
   - **Function:**
     - Carries data from CPU to memory (write operations)
     - Carries data from memory to CPU (read operations)
     - Carries instructions from memory to CPU (fetch)
     - Carries data between I/O devices and system
   - **Example:** When CPU reads memory location 1000, the data at that location travels on the data bus

2. **Address Bus:**
   - **Direction:** Unidirectional (addresses flow from CPU to memory/I/O)
   - **Contents:** Memory addresses and I/O port addresses
   - **Width:** Determines maximum addressable memory (e.g., 32 bits = 4 GB)
   - **Function:**
     - CPU places address on bus to specify which location to access
     - Memory uses address to select the correct location
     - I/O modules use addresses to identify specific devices/ports
   - **Example:** CPU places address 1000 on address bus to read from that location

3. **Control Bus:**
   - **Direction:** Mixed (various signals in different directions)
   - **Contents:** Control and timing signals
   - **Signals Include:**
     - **Read/Write:** Indicates whether operation is read or write
     - **Memory/IO:** Distinguishes memory access from I/O access
     - **Clock:** Synchronization signal (for synchronous buses)
     - **Ready/Acknowledge:** Handshaking signals
     - **Interrupt:** Interrupt request and acknowledge signals
     - **Bus Request/Grant:** Bus arbitration signals
     - **Reset:** System reset signal
   - **Function:**
     - Coordinates operations between components
     - Synchronizes timing
     - Manages bus access
     - Handles interrupts and arbitration

**Working Together:**
- Address bus specifies WHERE
- Data bus carries WHAT
- Control bus specifies HOW and WHEN

---

**SA 5.2** How does the width of the address bus determine the maximum addressable memory?

**Answer:**

**Fundamental Relationship:**
- Address bus width determines how many unique addresses can be represented
- Maximum number of addresses = 2^(address bus width)
- Maximum memory = Maximum addresses × Bytes per address

**Mathematical Explanation:**
- n-bit address bus can represent 2^n different binary addresses
- Addresses range from 0 to (2^n - 1)
- Each address typically stores 1 byte (but can store more)

**Examples:**

**16-bit address bus:**
- 2^16 = 65,536 addresses
- Maximum memory = 65,536 bytes = 64 KB

**20-bit address bus:**
- 2^20 = 1,048,576 addresses
- Maximum memory = 1,048,576 bytes = 1 MB

**32-bit address bus:**
- 2^32 = 4,294,967,296 addresses
- Maximum memory = 4,294,967,296 bytes = 4 GB

**64-bit address bus:**
- 2^64 = 18,446,744,073,709,551,616 addresses
- Maximum memory = 16 exabytes (theoretical)

**Important Notes:**
- This is a hard architectural limit
- Cannot address more memory than this without increasing address bus width
- Some systems use memory mapping techniques to access more (but still limited by address space)
- Modern systems often use 64-bit addressing for very large memory support

**If Each Address Stores More Than 1 Byte:**
- If each address stores 2 bytes (16 bits): Maximum = 2^n × 2 bytes
- If each address stores 4 bytes (32 bits): Maximum = 2^n × 4 bytes
- But typically, we assume 1 byte per address for simplicity

---

**SA 5.3** Explain the relationship between data bus width and data transfer speed.

**Answer:**

**Direct Relationship:**
- Wider data bus = more bits transferred per clock cycle
- Data transfer speed = Data bus width × Clock frequency

**Mathematical Formula:**
```
Bandwidth (bits/second) = Data Bus Width (bits) × Clock Frequency (Hz)
Bandwidth (bytes/second) = (Data Bus Width / 8) × Clock Frequency
```

**Examples:**

**16-bit bus at 100 MHz:**
- 16 bits × 100,000,000 Hz = 1,600,000,000 bits/s
- = 200,000,000 bytes/s = 200 MB/s

**32-bit bus at 100 MHz:**
- 32 bits × 100,000,000 Hz = 3,200,000,000 bits/s
- = 400,000,000 bytes/s = 400 MB/s

**64-bit bus at 200 MHz:**
- 64 bits × 200,000,000 Hz = 12,800,000,000 bits/s
- = 1,600,000,000 bytes/s = 1.6 GB/s

**Key Points:**
- **Doubling bus width** (with same clock) **doubles bandwidth**
- **Doubling clock frequency** (with same width) **doubles bandwidth**
- Both factors are important for high-speed transfers

**Trade-offs:**
- **Wider Bus:**
  - Pros: Higher bandwidth
  - Cons: More pins, more expensive, more complex routing, more power

- **Higher Frequency:**
  - Pros: Higher bandwidth
  - Cons: Signal integrity challenges, power consumption, timing constraints

**Practical Considerations:**
- Real bandwidth is often less than theoretical (due to overhead, protocol, etc.)
- Bus efficiency factors in: address setup time, control signal overhead, etc.
- Actual transfer rate = Theoretical bandwidth × Efficiency factor

---

**SA 5.4** What types of signals are carried on the control bus? Give three examples.

**Answer:**

The control bus carries various control and timing signals that coordinate system operations. Types include:

**1. Operation Control Signals:**
- **Read (RD):** Indicates a read operation from memory or I/O
- **Write (WR):** Indicates a write operation to memory or I/O
- **Memory/IO (M/IO):** Distinguishes memory access from I/O access

**2. Timing and Synchronization Signals:**
- **Clock (CLK):** Synchronization signal for synchronous buses
- **Ready:** Indicates that addressed device is ready for data transfer
- **Acknowledge (ACK):** Confirms that operation was completed
- **Wait:** Requests CPU to wait (slower device)

**3. Interrupt Signals:**
- **Interrupt Request (IRQ):** Device requests interrupt service
- **Interrupt Acknowledge (INTA):** CPU acknowledges interrupt request
- **Non-Maskable Interrupt (NMI):** High-priority interrupt that cannot be disabled

**4. Bus Control Signals:**
- **Bus Request (BR):** Device requests bus access
- **Bus Grant (BG):** Bus arbiter grants bus access
- **Bus Busy:** Indicates bus is currently in use

**5. Status and Error Signals:**
- **Reset:** System reset signal
- **Error:** Indicates an error condition
- **Parity Error:** Memory parity error detected

**Three Specific Examples:**

1. **Read Signal (RD):**
   - When CPU wants to read from memory
   - Placed on control bus by CPU
   - Memory responds by placing data on data bus
   - Active low signal (0 = read operation)

2. **Interrupt Request (IRQ):**
   - Generated by I/O device when it needs attention
   - Sent to CPU on control bus
   - CPU checks this signal after each instruction
   - Triggers interrupt handling if active

3. **Clock Signal (CLK):**
   - Periodic timing signal
   - Synchronizes all bus operations
   - All devices use this signal for timing
   - Frequency determines bus speed

**Characteristics:**
- Control bus is a collection of individual signal lines
- Some signals are unidirectional (CPU → devices)
- Some signals are bidirectional (handshaking)
- Signal timing is critical for proper operation

---

### Section 6: Bus Architecture

**SA 6.1** Compare synchronous and asynchronous bus timing. What are the advantages and disadvantages of each?

**Answer:**

**Synchronous Bus Timing:**

**How it Works:**
- All operations synchronized to a common clock signal
- Clock signal distributed to all devices on the bus
- Operations occur on specific clock edges (rising/falling)
- All devices must operate at the same speed

**Advantages:**
- **Simplicity:** Easier to design and implement
- **Predictable Timing:** All operations have known, fixed timing
- **Lower Overhead:** No handshaking signals needed
- **Easier Debugging:** Timing is deterministic
- **Cost Effective:** Simpler control logic

**Disadvantages:**
- **Inflexibility:** All devices must operate at same speed
- **Speed Limited by Slowest Device:** Must wait for slowest device
- **Clock Skew Issues:** Clock signal propagation delays can cause problems
- **Not Suitable for Mixed-Speed Devices:** Fast CPU, slow I/O devices

**Example:**
- Clock period: 10 ns
- Memory access: 3 clock cycles = 30 ns
- All devices must complete operations within clock cycles

---

**Asynchronous Bus Timing:**

**How it Works:**
- No common clock signal
- Uses handshaking protocol (Ready/Acknowledge signals)
- Operations complete when devices are ready
- Devices can operate at different speeds

**Advantages:**
- **Flexibility:** Devices can operate at different speeds
- **Efficiency:** No waiting for fixed clock cycles
- **Suitable for Mixed-Speed Devices:** Fast CPU, slow I/O can coexist
- **Self-Timing:** Each operation takes only as long as needed
- **Better for I/O:** I/O devices vary greatly in speed

**Disadvantages:**
- **Complexity:** More complex control logic required
- **Handshaking Overhead:** Ready/Acknowledge signals add overhead
- **Less Predictable:** Timing varies with device speed
- **More Expensive:** More complex circuitry
- **Harder to Debug:** Timing is less deterministic

**Example:**
- CPU places address on bus, asserts Read signal
- Memory responds when ready with Ready signal
- CPU reads data, asserts Acknowledge
- Timing depends on memory speed, not fixed clock

---

**Comparison Summary:**

| Aspect | Synchronous | Asynchronous |
|--------|------------|--------------|
| Clock | Required | Not required |
| Timing | Fixed | Variable |
| Complexity | Simple | Complex |
| Speed Matching | Required | Not required |
| Overhead | Low | Higher (handshaking) |
| Predictability | High | Lower |
| Cost | Lower | Higher |
| Best For | Homogeneous systems | Mixed-speed systems |

**Modern Systems:**
- Many systems use hybrid approaches
- Synchronous for CPU-memory (fast, homogeneous)
- Asynchronous for I/O (mixed speeds)
- Best of both worlds

---

**SA 6.2** What is bus arbitration, and why is it necessary?

**Answer:**

**Definition:**
Bus arbitration is the process of determining which device gets access to the bus when multiple devices request it simultaneously.

**Why It's Necessary:**

1. **Single Bus Constraint:**
   - Only one device can use the bus at a time
   - Bus is a shared resource
   - Multiple devices (CPU, memory, I/O) need bus access

2. **Conflict Resolution:**
   - When multiple devices want bus access at the same time, conflict occurs
   - Arbitration resolves this conflict
   - Determines which device gets access and in what order

3. **System Coordination:**
   - Prevents data corruption from simultaneous access
   - Ensures orderly data transfer
   - Maintains system integrity

**When Arbitration is Needed:**
- Multiple devices on the bus
- Devices can independently request bus access
- Concurrent access requests occur
- System has more than one bus master

**What Happens Without Arbitration:**
- Multiple devices try to drive bus simultaneously
- Bus conflicts and data corruption
- Unpredictable system behavior
- Potential hardware damage

**Arbitration Process:**
1. Device requests bus access (Bus Request signal)
2. Arbiter receives requests from all devices
3. Arbiter determines priority (based on scheme)
4. Arbiter grants bus to one device (Bus Grant signal)
5. Device uses bus for its operation
6. Device releases bus when done
7. Process repeats for next device

**Arbitration Schemes:**
- **Centralized:** Single arbiter manages all requests
- **Distributed:** Devices negotiate among themselves
- **Priority-Based:** Fixed or dynamic priorities
- **Fairness:** Round-robin, time-sliced access

**Example Scenario:**
- CPU wants to read from memory
- Network card wants to write to memory
- Both request bus simultaneously
- Arbiter grants bus to CPU (higher priority)
- CPU completes operation
- Arbiter then grants bus to network card
- Network card completes operation

---

**SA 6.3** Describe how a daisy chain arbitration scheme works.

**Answer:**

**Daisy Chain Arbitration:**

**Physical Setup:**
- Devices connected in a chain (series)
- Bus Request (BR) signal propagates through the chain
- Bus Grant (BG) signal propagates through the chain
- Device closest to arbiter/CPU has highest priority
- Device farthest has lowest priority

**How It Works:**

1. **Request Phase:**
   - Any device wanting bus access asserts Bus Request
   - Request signal propagates down the chain
   - All devices in chain see the request

2. **Grant Phase:**
   - Arbiter/CPU asserts Bus Grant signal
   - Grant signal propagates down the chain
   - First device in chain that wants the bus "captures" the grant
   - That device blocks the grant signal from propagating further
   - Other devices don't receive the grant

3. **Bus Usage:**
   - Device that captured grant uses the bus
   - Other devices wait
   - Device releases bus when done

4. **Next Request:**
   - Process repeats for next device in priority order

**Priority Order:**
- Device 1 (closest): Highest priority
- Device 2: Second priority
- Device 3: Third priority
- Device N (farthest): Lowest priority

**Hardware Implementation:**
- Each device has logic to:
  - Pass grant signal if device doesn't need bus
  - Capture grant signal if device needs bus
  - Block grant from propagating if captured

**Advantages:**
- Simple hardware implementation
- Low cost (minimal logic per device)
- Deterministic priority (fixed by physical position)
- No central arbiter needed (can use CPU as arbiter)

**Disadvantages:**
- Fixed priority (cannot change)
- Unfair to low-priority devices
- Grant propagation delay (slower for devices farther away)
- Single point of failure (broken device breaks chain)

**Example:**
```
CPU/Arbiter → Device A → Device B → Device C
(Highest Priority)              (Lowest Priority)

If A and C both request:
- A gets bus (higher priority)
- C waits until A is done
- Then C gets bus
```

**Use Cases:**
- Simple systems with few devices
- When fixed priority is acceptable
- Cost-sensitive applications
- Systems where priority rarely changes

---

**SA 6.4** Explain how bus bandwidth is calculated and what factors affect it.

**Answer:**

**Bus Bandwidth Calculation:**

**Basic Formula:**
```
Bandwidth = Data Bus Width × Clock Frequency
```

**In Bits per Second:**
```
Bandwidth (bits/s) = Data Bus Width (bits) × Clock Frequency (Hz)
```

**In Bytes per Second:**
```
Bandwidth (bytes/s) = (Data Bus Width / 8) × Clock Frequency (Hz)
```

**In MB/s or GB/s:**
```
Bandwidth (MB/s) = (Data Bus Width / 8) × Clock Frequency (Hz) / 1,000,000
Bandwidth (GB/s) = (Data Bus Width / 8) × Clock Frequency (Hz) / 1,000,000,000
```

**Example Calculations:**

**32-bit bus at 100 MHz:**
- Bandwidth = 32 bits × 100,000,000 Hz = 3,200,000,000 bits/s
- = 400,000,000 bytes/s = 400 MB/s

**64-bit bus at 200 MHz:**
- Bandwidth = 64 bits × 200,000,000 Hz = 12,800,000,000 bits/s
- = 1,600,000,000 bytes/s = 1.6 GB/s

**Factors Affecting Bus Bandwidth:**

1. **Data Bus Width:**
   - **Direct Impact:** Doubling width doubles bandwidth
   - Wider bus = more bits per transfer
   - Trade-off: More pins, more cost, more complexity

2. **Clock Frequency:**
   - **Direct Impact:** Doubling frequency doubles bandwidth
   - Higher frequency = more transfers per second
   - Trade-off: Signal integrity, power consumption, timing constraints

3. **Bus Efficiency:**
   - **Overhead Factors:**
     - Address setup time
     - Control signal overhead
     - Bus arbitration delays
     - Protocol overhead
   - **Actual Bandwidth = Theoretical × Efficiency**
   - Typical efficiency: 60-90% depending on system

4. **Transfer Type:**
   - **Burst Transfers:** More efficient (address once, multiple data)
   - **Single Transfers:** Less efficient (address + data each time)
   - **Read vs. Write:** May have different timings

5. **Bus Protocol:**
   - **Synchronous:** Fixed timing, predictable
   - **Asynchronous:** Variable timing, handshaking overhead
   - **Packet-Based:** Protocol overhead reduces effective bandwidth

6. **Physical Factors:**
   - **Signal Integrity:** Poor signals require slower speeds
   - **Cable Length:** Longer cables limit maximum frequency
   - **Noise:** Electrical noise affects maximum reliable speed

**Improving Bandwidth:**

**Option 1: Increase Bus Width**
- 16-bit → 32-bit: 2× bandwidth
- 32-bit → 64-bit: 2× bandwidth
- Cost: More pins, more complex

**Option 2: Increase Clock Frequency**
- 100 MHz → 200 MHz: 2× bandwidth
- Requires better signal integrity
- Cost: More power, better components

**Option 3: Improve Efficiency**
- Reduce overhead
- Use burst transfers
- Optimize protocol
- Cost: Design complexity

**Real-World Considerations:**
- Theoretical bandwidth is rarely achieved
- Actual bandwidth is lower due to overhead
- System bottlenecks may be elsewhere (CPU, memory)
- Bandwidth must match system needs

**Example with Efficiency:**
- Theoretical: 64 bits × 200 MHz = 1.6 GB/s
- Efficiency: 80%
- Actual: 1.6 GB/s × 0.80 = 1.28 GB/s

---

*[Continued in next part with Calculation Problems...]*

